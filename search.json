[
  {
    "objectID": "posts/secondorder/index.html",
    "href": "posts/secondorder/index.html",
    "title": "Optimizing bivariate functions",
    "section": "",
    "text": "Hessian matrix is a square matrix of second order partial derivatives of a scalar valued function. It can be used to describe the local curvature of the function at a point and it is denoted by H.\n\\[\nH(f) = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \\cdots& \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\frac{\\partial^2 f}{\\partial x_2\\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} \\cdots& \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\ \\vdots &\\quad \\vdots \\quad \\ddots & \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2}  \\cdots& \\frac{\\partial^2 f}{\\partial x_n^2} \\end{bmatrix}\n\\]\nFor example, let’s take a bivariate function(n=2),\n\\[\nf(x,y) = xy^2\n\\]\n\\[\nH(f)= \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\ \\frac{\\partial^2 f}{\\partial y \\partial x} & \\frac{\\partial^2 f}{\\partial y^2} \\end{bmatrix}\n\\]\nHere, \\(\\frac{\\partial f}{\\partial x}=y^2\\), \\(\\frac{\\partial f}{\\partial y}=2xy\\), \\(\\frac{\\partial^2 f}{\\partial x^2}=0,\\frac{\\partial^2 f}{\\partial y^2}=2x,\\frac{\\partial^2 f}{\\partial x \\partial y}=\\frac{\\partial^2 f}{\\partial y \\partial x}=2y\\)\n\\[\nH(f)=\\begin{bmatrix} 0 & 2y \\\\ 2y & 2x \\end{bmatrix}\n\\]\nUsing this matrix, we can find out the nature of the curvature at any point \\((x_1,y_1)\\), by substituting this point in the Hessian.\nIf the Hessian matrix is positive definite(all eigenvalues are positive) at a point, it indicates that the function is locally convex(has a local minimum) around that point. If it is negative definite, the function is locally concave. If the eigenvalues have both positive and negative values, then this point has a mixture of concave and convex behaviour in different directions and such a point is called a saddle point.\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define a sample function\ndef f(x):\n    return x[0]*x[1]**2 \n\n# point where you want to compute Hessian matrix\n# requires_grad=True tells pytorch to keep track of x0 which form a computation graph to compute gradients easily.\nx0 = torch.tensor([2.0, 1.0], requires_grad=True)\n# create_graph=True is used to compute higher order derivatives in the computation graph\ngrads = torch.autograd.grad(f(x0), x0, create_graph=True)[0]\nHessian = torch.zeros((len(x0), len(x0)))\nfor i in range(len(x0)):\n    Hessian[i] = torch.autograd.grad(grads[i], x0, retain_graph=True)[0]\n\nHessian = Hessian.detach().numpy()\nplt.imshow(Hessian, cmap='coolwarm')\nplt.xticks(np.arange(len(x0)))\nplt.yticks(np.arange(len(x0)))\nplt.xlabel('Hessian Row Index')\nplt.ylabel('Hessian Column Index')\nplt.colorbar()\nplt.title('Visualization of the Hessian Matrix')\nplt.show()\n\n\n\n\n\n\nFollowing are the steps to find minimum or maximum of a function:\n\nMake an intial guess.\nAt the initial guess, we find out how steep the slope of the curve is and how quickly the slope is changing. Hence, we calculate the first derivative and the second derivative at this point.\nWe can approximate a quadratic function(parabolic bowl) at that point using the taylor series.\nNewton’s method then moves to the minimum of the parabolic bowl which is the new guess for the optimal point of the original function.\nThis process repeats and with each iteration you edge closer to the optimal value of the original function and finally newton’s method converges.\n\nAt any iteration, the value of \\(x\\) can be updated as,\n\\[\nx_{i+1} = x_i-H^{-1}\\nabla f(x_i) \\quad -(*)\n\\]\nwhere \\(H^{-1}\\) is the inverse of the hessian(which is initially assumed to be the identity matrix) and \\(\\nabla f(x_i)\\) is an array/vector containing the partial derivatives of \\(f\\) with respect to all the variables.\nFollowing is the code for optimizing \\(f(x,y)=-sin(x)-cos(y)\\).\nLet’s first plot \\(f(x,y)\\).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-5, 5, 100)\ny = np.linspace(-5, 5, 100)\nX, Y = np.meshgrid(x, y)\n\ndef f(X,Y):\n    return -np.sin(X) - np.cos(Y)\nZ = f(X,Y)\n\nfig = plt.figure(figsize=(8,6))\nax1= fig.add_subplot(111, projection='3d')\nax1.plot_surface(X, Y, Z, cmap='viridis')\nax1.set_xlabel('X')\nax1.set_ylabel('Y')\nax1.set_zlabel('Z')\nax1.set_title('f(x) = -sin(x) - cos(y)')\nplt.show()\n\n\n\n\n\nimport torch\n\ndef f(x):\n    return -torch.sin(x[0])-torch.cos(x[1])\n\niterations = 10\n\n\ndef newton(guess,f,iterations):\n  guesses=[]\n  guesses.append(guess)\n  for i in range(iterations):\n      f_value = f(guess)\n      gradient = torch.autograd.grad(f_value, guess, create_graph=True)[0]\n      hessian = torch.zeros((len(guess), len(guess)))\n      for j in range(len(guess)):\n          hessian_row = torch.autograd.grad(gradient[j], guess, retain_graph=True)[0]\n          hessian[j] = hessian_row\n      step = -torch.linalg.solve(hessian, gradient)\n      guess = guess + step\n      guesses.append(guess)\n  return guesses\n      \n\nguess = torch.tensor([2.0, 1.0], requires_grad=True)\nguesses=newton(guess,f,iterations)\nfor i in range(len(guesses)):\n  print(f\"Iteration {i}: guess = {guesses[i]}\")\n\nIteration 0: guess = tensor([2., 1.], requires_grad=True)\nIteration 1: guess = tensor([ 1.5423, -0.5574], grad_fn=&lt;AddBackward0&gt;)\nIteration 2: guess = tensor([1.5708, 0.0659], grad_fn=&lt;AddBackward0&gt;)\nIteration 3: guess = tensor([ 1.5708e+00, -9.5725e-05], grad_fn=&lt;AddBackward0&gt;)\nIteration 4: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 5: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 6: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 7: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 8: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 9: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 10: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\n\n\nSo after updating our guess using \\((*)\\) for a sufficient number of iterations, we get our final guess as \\(x=1.5708 \\quad and \\quad y=0.0\\).\nLet’s plot the contour plot of the above function to verify our results.\n\nimport time \n\nx = np.linspace(-6, 6, 100)\ny = np.linspace(-6, 6, 100)\nX, Y = np.meshgrid(x, y)\n\ndef f1(X,Y):\n    return -np.sin(X) - np.cos(Y)\nZ = f1(X,Y)\n\ndef plot_contour(guesses):\n  fig=plt.figure(figsize=(10,6))\n  ax = fig.add_subplot(111)\n  contour = ax.contourf(X,Y,Z)\n  plt.colorbar(contour)\n  ax.set_xlabel(\"X\")\n  ax.set_ylabel(\"Y\")\n  ax.set_title(\"Contour plot of f(x,y)\")\n  marker=\"\"\n  for i in range(2):\n    if i==0:\n      marker=\"o\"\n      color=\"cyan\"\n      ax.scatter(guesses[0][0].detach().numpy(), guesses[0][1].detach().numpy(), color=color, alpha=1,marker=marker)\n    else:\n      marker=\"x\"\n      color=\"red\"\n      ax.scatter(guesses[-1][0].detach().numpy(), guesses[-1][1].detach().numpy(), color=color, alpha=1,marker=marker)\n  plt.show()\n  \nplot_contour(guesses)\n\n\n\n\nhttps://www.flexclip.com/share/367853402cf50de52d9ad00687ce49806e7a8e5.html\nThrough the contour plot we can understand that even though our initial guess was the point \\((2,1)\\) we finally reached the minima of the function. In the above contour plot, the bluish circle is the initial guess and the red cross is the final guess.\nDepending upon different initial guesses, the final guess could land onto different minimas or possibly even a saddle point.\nLet’s say we take another point \\((1,-2)\\).\n\nguess = torch.tensor([1.0, -2.0], requires_grad=True)\niterations=10\nguesses = newton(guess,f,iterations)\nplot_contour(guesses)\n\n\n\n\nIn the case above we got a saddle point, this is one of the drawbacks of the newton method.\nAlthough the newton’s method for optimization converges faster than the gradient descent algorithm and one doesn’t have to also face the difficulty in deciding the learning rate as is faced in gradient descent, the computation of the Hessian and it’s inverse is computationally very expensive(having computational complexity of \\(O(n^3)\\) for functions with n variables.\nIn order to use this method for optimization, the hessian needs to be positive definite which may not always be possible.\nHence to overcome these scenarios, Quasi-newton optimization algorithms can be used like the BFGS, and the LBFGS, where we try to approximate the hessian instead of calculating it.\n\n\n\nThe BFGS algorithm constructs an approximation of the inverse Hessian matrix using a sequence of rank-two updates. This approximation captures information about the curvature of the objective function’s landscape and guides the optimization process. BFGS has good convergence properties and doesn’t require the explicit computation of the Hessian matrix, making it suitable for problems with a large number of variables.\nL-BFGS is a variant of the BFGS algorithm that addresses the memory and computational requirements associated with the Hessian matrix. In high-dimensional optimization problems, storing and manipulating the full Hessian matrix can be expensive. L-BFGS overcomes this limitation by maintaining a limited-memory approximation of the Hessian, using only a small number of vectors.\nL-BFGS uses a recursive formula to update and approximate the inverse Hessian matrix. Instead of storing the full Hessian matrix explicitly, L-BFGS maintains a limited number of vector pairs to approximate the Hessian. This makes L-BFGS well-suited for large-scale optimization problems and enables it to operate efficiently in high-dimensional spaces.\nThe following code implements LBFGS of the function -sin(x)-cos(y)\n\nimport torch\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\n\ndef f(x):\n    return -torch.sin(x[0])-torch.cos(x[1])\n\n# L-BFGS\ndef closure():\n    lbfgs.zero_grad()\n    objective = f(x_lbfgs)\n    objective.backward()\n    return objective\n\nx_lbfgs = torch.ones(2, 1)\nx_lbfgs.requires_grad = True\n\nlbfgs = optim.LBFGS([x_lbfgs],\n                    history_size=10, \n                    max_iter=4, \n                    line_search_fn=\"strong_wolfe\")\n                    \nhistory_lbfgs = []\nfor i in range(100):\n    history_lbfgs.append(f(x_lbfgs).item())\n    lbfgs.step(closure)\n\nLet us also perform gradient descent on this, with learning rate of 10^-5.\n\nx_gd = torch.ones(2, 1)\nx_gd.requires_grad = True\ngd = optim.SGD([x_gd], lr=1e-5)\n\nhistory_gd = []\nfor i in range(100):\n    gd.zero_grad()\n    objective = f(x_gd)\n    objective.backward()\n    gd.step()\n    history_gd.append(objective.item())\n\nNow, to visualize the results, we use a contour plot:\n\nx_range = np.linspace(-5, 5, 400)\ny_range = np.linspace(-5, 5, 400)\nX, Y = np.meshgrid(x_range, y_range)\n\nZ = f(torch.tensor([X, Y])).detach().numpy()\n\nfig=plt.figure(figsize=(10,6))\nplt.contourf(X, Y, Z, levels=20, cmap=\"viridis\")\n\ncoordinates = np.array([2.0, 1.0])\nplt.plot(coordinates[0], coordinates[1], marker=\"o\", color=\"cyan\", label=\"Initial Coordinates\")\n\ncoordinates = x_lbfgs.detach().numpy()\nplt.plot(coordinates[0], coordinates[1], marker=\"o\", color=\"red\", label=\"LBFGS\")\n\ncoordinates = x_gd.detach().numpy()\nplt.plot(coordinates[0], coordinates[1], marker=\"o\", color=\"orange\", label=\"Grad Desc (lr=1e-5)\")\n\nplt.colorbar(label=\"Objective Value\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.title(\"Contour Plot of -sin(X)-cos(Y)\")\nplt.legend()\nplt.show()\n\n\n\n\nWe observe that in a 100 iterations, the gradient descent algorithm does not converge to the minima, but remains somewhere in between. Changing the learning rate might lead to the optimal value. For L-BFGS, the convergence is at the minima.\n\n\n\nThe LBFGS method is appealing for several reasons it is very simple to implement it requires only function and gradient values and no other information on the problem # and it can be faster than the partitioned quasi Newton method on problems where the element functions depend on more than or variables\nIn addition the LBFGS method appears to be preferable to PQN for large problems in which the Hessian matrix is not very sparse or for problems in which the information on the separablity of the ob jective function is difficult to obtain."
  },
  {
    "objectID": "posts/secondorder/index.html#newtons-method-for-optimizing-bivariate-functions-using-hessian.",
    "href": "posts/secondorder/index.html#newtons-method-for-optimizing-bivariate-functions-using-hessian.",
    "title": "Optimizing bivariate functions",
    "section": "",
    "text": "Following are the steps to find minimum or maximum of a function:\n\nMake an intial guess.\nAt the initial guess, we find out how steep the slope of the curve is and how quickly the slope is changing. Hence, we calculate the first derivative and the second derivative at this point.\nWe can approximate a quadratic function(parabolic bowl) at that point using the taylor series.\nNewton’s method then moves to the minimum of the parabolic bowl which is the new guess for the optimal point of the original function.\nThis process repeats and with each iteration you edge closer to the optimal value of the original function and finally newton’s method converges.\n\nAt any iteration, the value of \\(x\\) can be updated as,\n\\[\nx_{i+1} = x_i-H^{-1}\\nabla f(x_i) \\quad -(*)\n\\]\nwhere \\(H^{-1}\\) is the inverse of the hessian(which is initially assumed to be the identity matrix) and \\(\\nabla f(x_i)\\) is an array/vector containing the partial derivatives of \\(f\\) with respect to all the variables.\nFollowing is the code for optimizing \\(f(x,y)=-sin(x)-cos(y)\\).\nLet’s first plot \\(f(x,y)\\).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-5, 5, 100)\ny = np.linspace(-5, 5, 100)\nX, Y = np.meshgrid(x, y)\n\ndef f(X,Y):\n    return -np.sin(X) - np.cos(Y)\nZ = f(X,Y)\n\nfig = plt.figure(figsize=(8,6))\nax1= fig.add_subplot(111, projection='3d')\nax1.plot_surface(X, Y, Z, cmap='viridis')\nax1.set_xlabel('X')\nax1.set_ylabel('Y')\nax1.set_zlabel('Z')\nax1.set_title('f(x) = -sin(x) - cos(y)')\nplt.show()\n\n\n\n\n\nimport torch\n\ndef f(x):\n    return -torch.sin(x[0])-torch.cos(x[1])\n\niterations = 10\n\n\ndef newton(guess,f,iterations):\n  guesses=[]\n  guesses.append(guess)\n  for i in range(iterations):\n      f_value = f(guess)\n      gradient = torch.autograd.grad(f_value, guess, create_graph=True)[0]\n      hessian = torch.zeros((len(guess), len(guess)))\n      for j in range(len(guess)):\n          hessian_row = torch.autograd.grad(gradient[j], guess, retain_graph=True)[0]\n          hessian[j] = hessian_row\n      step = -torch.linalg.solve(hessian, gradient)\n      guess = guess + step\n      guesses.append(guess)\n  return guesses\n      \n\nguess = torch.tensor([2.0, 1.0], requires_grad=True)\nguesses=newton(guess,f,iterations)\nfor i in range(len(guesses)):\n  print(f\"Iteration {i}: guess = {guesses[i]}\")\n\nIteration 0: guess = tensor([2., 1.], requires_grad=True)\nIteration 1: guess = tensor([ 1.5423, -0.5574], grad_fn=&lt;AddBackward0&gt;)\nIteration 2: guess = tensor([1.5708, 0.0659], grad_fn=&lt;AddBackward0&gt;)\nIteration 3: guess = tensor([ 1.5708e+00, -9.5725e-05], grad_fn=&lt;AddBackward0&gt;)\nIteration 4: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 5: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 6: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 7: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 8: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 9: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 10: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\n\n\nSo after updating our guess using \\((*)\\) for a sufficient number of iterations, we get our final guess as \\(x=1.5708 \\quad and \\quad y=0.0\\).\nLet’s plot the contour plot of the above function to verify our results.\n\nimport time \n\nx = np.linspace(-6, 6, 100)\ny = np.linspace(-6, 6, 100)\nX, Y = np.meshgrid(x, y)\n\ndef f1(X,Y):\n    return -np.sin(X) - np.cos(Y)\nZ = f1(X,Y)\n\ndef plot_contour(guesses):\n  fig=plt.figure(figsize=(10,6))\n  ax = fig.add_subplot(111)\n  contour = ax.contourf(X,Y,Z)\n  plt.colorbar(contour)\n  ax.set_xlabel(\"X\")\n  ax.set_ylabel(\"Y\")\n  ax.set_title(\"Contour plot of f(x,y)\")\n  marker=\"\"\n  for i in range(2):\n    if i==0:\n      marker=\"o\"\n      color=\"cyan\"\n      ax.scatter(guesses[0][0].detach().numpy(), guesses[0][1].detach().numpy(), color=color, alpha=1,marker=marker)\n    else:\n      marker=\"x\"\n      color=\"red\"\n      ax.scatter(guesses[-1][0].detach().numpy(), guesses[-1][1].detach().numpy(), color=color, alpha=1,marker=marker)\n  plt.show()\n  \nplot_contour(guesses)\n\n\n\n\nhttps://www.flexclip.com/share/367853402cf50de52d9ad00687ce49806e7a8e5.html\nThrough the contour plot we can understand that even though our initial guess was the point \\((2,1)\\) we finally reached the minima of the function. In the above contour plot, the bluish circle is the initial guess and the red cross is the final guess.\nDepending upon different initial guesses, the final guess could land onto different minimas or possibly even a saddle point.\nLet’s say we take another point \\((1,-2)\\).\n\nguess = torch.tensor([1.0, -2.0], requires_grad=True)\niterations=10\nguesses = newton(guess,f,iterations)\nplot_contour(guesses)\n\n\n\n\nIn the case above we got a saddle point, this is one of the drawbacks of the newton method.\nAlthough the newton’s method for optimization converges faster than the gradient descent algorithm and one doesn’t have to also face the difficulty in deciding the learning rate as is faced in gradient descent, the computation of the Hessian and it’s inverse is computationally very expensive(having computational complexity of \\(O(n^3)\\) for functions with n variables.\nIn order to use this method for optimization, the hessian needs to be positive definite which may not always be possible.\nHence to overcome these scenarios, Quasi-newton optimization algorithms can be used like the BFGS, and the LBFGS, where we try to approximate the hessian instead of calculating it."
  },
  {
    "objectID": "posts/secondorder/index.html#l-bfgs-for-optimizing-functions",
    "href": "posts/secondorder/index.html#l-bfgs-for-optimizing-functions",
    "title": "Optimizing bivariate functions",
    "section": "",
    "text": "The BFGS algorithm constructs an approximation of the inverse Hessian matrix using a sequence of rank-two updates. This approximation captures information about the curvature of the objective function’s landscape and guides the optimization process. BFGS has good convergence properties and doesn’t require the explicit computation of the Hessian matrix, making it suitable for problems with a large number of variables.\nL-BFGS is a variant of the BFGS algorithm that addresses the memory and computational requirements associated with the Hessian matrix. In high-dimensional optimization problems, storing and manipulating the full Hessian matrix can be expensive. L-BFGS overcomes this limitation by maintaining a limited-memory approximation of the Hessian, using only a small number of vectors.\nL-BFGS uses a recursive formula to update and approximate the inverse Hessian matrix. Instead of storing the full Hessian matrix explicitly, L-BFGS maintains a limited number of vector pairs to approximate the Hessian. This makes L-BFGS well-suited for large-scale optimization problems and enables it to operate efficiently in high-dimensional spaces.\nThe following code implements LBFGS of the function -sin(x)-cos(y)\n\nimport torch\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\n\ndef f(x):\n    return -torch.sin(x[0])-torch.cos(x[1])\n\n# L-BFGS\ndef closure():\n    lbfgs.zero_grad()\n    objective = f(x_lbfgs)\n    objective.backward()\n    return objective\n\nx_lbfgs = torch.ones(2, 1)\nx_lbfgs.requires_grad = True\n\nlbfgs = optim.LBFGS([x_lbfgs],\n                    history_size=10, \n                    max_iter=4, \n                    line_search_fn=\"strong_wolfe\")\n                    \nhistory_lbfgs = []\nfor i in range(100):\n    history_lbfgs.append(f(x_lbfgs).item())\n    lbfgs.step(closure)\n\nLet us also perform gradient descent on this, with learning rate of 10^-5.\n\nx_gd = torch.ones(2, 1)\nx_gd.requires_grad = True\ngd = optim.SGD([x_gd], lr=1e-5)\n\nhistory_gd = []\nfor i in range(100):\n    gd.zero_grad()\n    objective = f(x_gd)\n    objective.backward()\n    gd.step()\n    history_gd.append(objective.item())\n\nNow, to visualize the results, we use a contour plot:\n\nx_range = np.linspace(-5, 5, 400)\ny_range = np.linspace(-5, 5, 400)\nX, Y = np.meshgrid(x_range, y_range)\n\nZ = f(torch.tensor([X, Y])).detach().numpy()\n\nfig=plt.figure(figsize=(10,6))\nplt.contourf(X, Y, Z, levels=20, cmap=\"viridis\")\n\ncoordinates = np.array([2.0, 1.0])\nplt.plot(coordinates[0], coordinates[1], marker=\"o\", color=\"cyan\", label=\"Initial Coordinates\")\n\ncoordinates = x_lbfgs.detach().numpy()\nplt.plot(coordinates[0], coordinates[1], marker=\"o\", color=\"red\", label=\"LBFGS\")\n\ncoordinates = x_gd.detach().numpy()\nplt.plot(coordinates[0], coordinates[1], marker=\"o\", color=\"orange\", label=\"Grad Desc (lr=1e-5)\")\n\nplt.colorbar(label=\"Objective Value\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.title(\"Contour Plot of -sin(X)-cos(Y)\")\nplt.legend()\nplt.show()\n\n\n\n\nWe observe that in a 100 iterations, the gradient descent algorithm does not converge to the minima, but remains somewhere in between. Changing the learning rate might lead to the optimal value. For L-BFGS, the convergence is at the minima."
  },
  {
    "objectID": "posts/secondorder/index.html#remarks1",
    "href": "posts/secondorder/index.html#remarks1",
    "title": "Optimizing bivariate functions",
    "section": "",
    "text": "The LBFGS method is appealing for several reasons it is very simple to implement it requires only function and gradient values and no other information on the problem # and it can be faster than the partitioned quasi Newton method on problems where the element functions depend on more than or variables\nIn addition the LBFGS method appears to be preferable to PQN for large problems in which the Hessian matrix is not very sparse or for problems in which the information on the separablity of the ob jective function is difficult to obtain."
  },
  {
    "objectID": "posts/secondorder/index.html#footnotes",
    "href": "posts/secondorder/index.html#footnotes",
    "title": "Optimizing bivariate functions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLiu, D.C. and Nocedal, J. (no date) On the limited memory BFGS method for large scale optimization - mathematical programming, SpringerLink. Available at: https://link.springer.com/article/10.1007/BF01589116 (Accessed: 20 August 2023).↩︎"
  },
  {
    "objectID": "posts/jpeghuffman/jpeghuffman.html",
    "href": "posts/jpeghuffman/jpeghuffman.html",
    "title": "Huffman Coding in JPEG",
    "section": "",
    "text": "import huffman\nimport numpy as np\n\n# Create a 2D array (8x8) with DCT coefficients (replace with your actual coefficients)\ndct_coefficients = np.array([\n    [20, 30, 10, 25, 30, 0, 0, 0],\n    [0, 5, 0, 0, 0, 0, 5, 0],\n    [0, 0, 15, 0, 0, 0, 20, 0],\n    [0, 0, 0, 0, 1, 0, 0, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0]\n], dtype=np.int32)\nprint(\"Initial matrix\")\nprint(dct_coefficients)\nprint()\n# Function to calculate (r, s) pairs for DCT coefficients\ndef calculate_rs_pairs(dct_coefficients):\n    rs_pairs = []\n    run_length = 0\n\n    for row in dct_coefficients:\n        for element in row:\n            if element == 0:\n                run_length += 1\n            else:\n                bits_needed = len(bin(abs(element))) - 2  # Calculate the number of bits needed to represent c\n                rs_pairs.append(((run_length, bits_needed), element))\n                run_length = 0\n\n    return rs_pairs\n\n# Calculate (r, s) pairs\nrs_pairs = calculate_rs_pairs(dct_coefficients)\n\nprint(\"(r,s) pairs and c values:\")\nfor i in range(len(rs_pairs)):\n  print(rs_pairs[i])\nprint()\n\n# Calculate the frequency of (r, s) pairs\nfrequency_rs = {}\n\nd = {} # maps (r,s) pair to a unique symbol (for passing in huffman library)\nd2 = {} # Inverse of d\nalph = \"abcdefghijklmnopqrstuvwxyz\"\npos=0\n\nfor (r, s), _ in rs_pairs:\n  if (r,s) in list(d.keys()):\n    continue\n  else:\n    d[(r,s)] = alph[pos]\n    d2[alph[pos]] = (r,s)\n    pos+=1\n\nfor (r,s),_ in rs_pairs:\n    if d[(r, s)] in frequency_rs:\n        frequency_rs[d[(r, s)]] += 1\n    else:\n        frequency_rs[d[(r, s)]] = 1\n\n# Build Huffman codes for (r, s) pairs\nhuffman_rs = huffman.codebook([(x,frequency_rs[x]) for x in list(frequency_rs.keys())])\n\n# Create a dictionary to map (r, s) pairs to their Huffman codes\nrs_to_huffman = {pair: huffman_rs[pair] for pair in frequency_rs.keys()}\n\n# Encode ((r, s), c) triples\nencoded_data = \"\"\nfor (r, s), c in rs_pairs:\n    # encoded_data += rs_to_huffman[d[(r, s)]]   # Encode (r, s)\n    encoded_data += rs_to_huffman[d[(r, s)]] + format(c, f'0{s}b')  # Encode (r, s) and c\n\n# Decode the encoded data to recover the ((r, s), c) triples\ndecoded_data = []\ncurrent_bits = \"\"\ni=0\nwhile i&lt;len(encoded_data):\n    # print(i)\n    current_bits += encoded_data[i]\n    i+=1\n    for ch, huffman_code in rs_to_huffman.items():\n        if current_bits == huffman_code:\n            current_bits = \"\"\n            c_bits = \"\"\n            r,s = d2[ch]\n            # print(s)\n            for j in range(s):\n                c_bits += encoded_data[i+j]\n            decoded_data.append((ch,int(c_bits, 2)))\n            # print(ch,int(c_bits,2))\n            i+=s\n            # print(i)\n            c_bits = \"\"\n        # i+=1\n\n# Print the encoded and decoded data\nprint(\"Encoded Data:\")\nprint(encoded_data)\n\nprint(\"\\nDecoded Data:\")\ndl = []\nfor ch,c in decoded_data:\n  print(d2[ch],c)\n  dl.append((d2[ch],c))\n\nnewl = []\nfor ele in dl:\n  r,s = ele[0]\n  c = ele[1]\n  for i in range(r):\n    newl.append(0)\n  newl.append(c)\n\nll = len(newl)\nfor i in range(ll+1,64+1):\n  newl.append(0)\n\nimport numpy as np\nnewl = np.array(newl)\nnewl = np.reshape(newl,(8,8))\nprint()\nprint(\"The decoded matrix is:\")\nprint(newl)\n\nInitial matrix\n[[20 30 10 25 30  0  0  0]\n [ 0  5  0  0  0  0  5  0]\n [ 0  0 15  0  0  0 20  0]\n [ 0  0  0  0  1  0  0  0]\n [ 0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0]]\n\n(r,s) pairs and c values:\n((0, 5), 20)\n((0, 5), 30)\n((0, 4), 10)\n((0, 5), 25)\n((0, 5), 30)\n((4, 3), 5)\n((4, 3), 5)\n((3, 4), 15)\n((3, 5), 20)\n((5, 1), 1)\n\nEncoded Data:\n1110100111111010010101111001111111000101001011011111011101000101\n\nDecoded Data:\n(0, 5) 20\n(0, 5) 30\n(0, 4) 10\n(0, 5) 25\n(0, 5) 30\n(4, 3) 5\n(4, 3) 5\n(3, 4) 15\n(3, 5) 20\n(5, 1) 1\n\nThe decoded matrix is:\n[[20 30 10 25 30  0  0  0]\n [ 0  5  0  0  0  0  5  0]\n [ 0  0 15  0  0  0 20  0]\n [ 0  0  0  0  1  0  0  0]\n [ 0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0]]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kishan’s Blog Page",
    "section": "",
    "text": "Welcome to my Blog page! Here I’ll explain some exciting topics, based on Machine Learning, Mathematics, Coding etc., with visualisations to make it easier to grasp concepts. Every blog also contains code snippets, so you can run them locally too!\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nHuffman Coding in JPEG\n\n\n\n\n\n\n\nInformation Theory\n\n\nCoding\n\n\nData Compression\n\n\n\n\nHuffman coding for data compression to optimize memory and transmittion time.\n\n\n\n\n\n\nSep 11, 2023\n\n\n\n\n\n\n  \n\n\n\n\nHuffman Coding\n\n\n\n\n\n\n\nInformation Theory\n\n\nCoding\n\n\n\n\nHuffman coding for data compression to optimize memory and transmittion time.\n\n\n\n\n\n\nAug 27, 2023\n\n\n\n\n\n\n  \n\n\n\n\nOptimizing bivariate functions\n\n\n\n\n\n\n\nML\n\n\nMath\n\n\nCoding\n\n\n\n\nUnderstanding optimization of bivariate functions using Newton’s method and L-BFGS.\n\n\n\n\n\n\nAug 20, 2023\n\n\n\n\n\n\n  \n\n\n\n\nMatrix Transformations\n\n\n\n\n\n\n\nML\n\n\nMath\n\n\nCoding\n\n\n\n\nUnderstanding matrix multiplication as transformation and interpretation of low rank matrices\n\n\n\n\n\n\nJul 20, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Kishan Ved, a second year undergraduate at IIT Gandhinagar."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nIIT Gandhinagar | B.Tech | Computer Science Engineering"
  },
  {
    "objectID": "posts/huffmancoding/index.html",
    "href": "posts/huffmancoding/index.html",
    "title": "Huffman Coding",
    "section": "",
    "text": "In the world of information and data, efficiency is paramount. As we generate and consume vast amounts of digital content, the need to transmit and store this data in the most efficient manner has led to the development of various data compression techniques. One such remarkable technique is Huffman coding, which not only reduces data size but also plays a crucial role in modern data storage, transmission, and encryption. In this blog, we will delve into the mechanics of Huffman coding, its significance, and its applications across various domains.\n\n\nIn a world inundated with data, the efficiency of data storage and transmission has become paramount. Whether it’s sending files over the internet, storing data on devices with limited space, or even optimizing database structures, the ability to compress data without significant loss of information has become essential. A naive way would be to assign the same number of bits to every letter, but Huffman coding provides an optimal solution."
  },
  {
    "objectID": "posts/huffmancoding/index.html#need-for-data-compression",
    "href": "posts/huffmancoding/index.html#need-for-data-compression",
    "title": "Huffman Coding",
    "section": "",
    "text": "In a world inundated with data, the efficiency of data storage and transmission has become paramount. Whether it’s sending files over the internet, storing data on devices with limited space, or even optimizing database structures, the ability to compress data without significant loss of information has become essential. A naive way would be to assign the same number of bits to every letter, but Huffman coding provides an optimal solution."
  },
  {
    "objectID": "posts/huffmancoding/index.html#algorithm",
    "href": "posts/huffmancoding/index.html#algorithm",
    "title": "Huffman Coding",
    "section": "Algorithm",
    "text": "Algorithm\nThe Huffman coding algorithm follows a simple yet powerful process:\n\nFrequency Calculation:\nDetermine the frequency of each symbol (character) in the data to be encoded.\nConsider this long quote by Albert Einstein:\nLife is like riding a bicycle. To keep your balance, you must keep moving. The important thing is not to stop questioning. Curiosity has its own reason for existing. One cannot help but be in awe when contemplating the mysteries of eternity, of life, of the marvelous structure of reality. It is enough if one tries merely to comprehend a little of this mystery each day. The important thing is not to stop questioning; never lose a holy curiosity.\n\ns = \"Life is like riding a bicycle. To keep your balance, you must keep moving. The important thing is not to stop questioning. Curiosity has its own reason for existing. One cannot help but be in awe when contemplating the mysteries of eternity, of life, of the marvelous structure of reality. It is enough if one tries merely to comprehend a little of this mystery each day. The important thing is not to stop questioning; never lose a holy curiosity.\"\ns = s.lower()\narr = [0]*26\nd = {}\nfor i in range(len(s)):\n  if(s[i]==' ' or s[i]==',' or s[i]=='.' or s[i]==';'):\n    continue\n  arr[ord(s[i])-ord('a')]+=1\nfor i in range(len(arr)):\n  print(chr(i+ord('a')), arr[i])\n  if arr[i]!=0:\n    d[chr(i+ord('a'))] = arr[i]\n\na 16\nb 4\nc 10\nd 3\ne 43\nf 9\ng 9\nh 14\ni 36\nj 0\nk 3\nl 14\nm 10\nn 28\no 34\np 9\nq 2\nr 19\ns 23\nt 41\nu 12\nv 3\nw 3\nx 1\ny 13\nz 0\n\n\n\n\nCreating nodes for every letter:\nThere is a node corresponding to every symbol (letter), and it contains the frequency of the letter (or the number of times it appears in the data), the huffman coding for it and also pointers to left and right nodes.\n\nclass Node:\n    def __init__(self, freq, symbol, left=None, right=None):\n        self.freq = freq\n        self.symbol = symbol\n        self.left = left\n        self.right = right\n        self.huff = ''\n\n    def __lt__(self, nxt):\n        return self.freq &lt; nxt.freq\n\n\n\nPriority Queue / Min Heap:\nCreate a priority queue (min-heap) based on the symbol frequencies. In a min heap, the value at child nodes must be greater than that at the parent node.\n\nimport heapq\n\nchars = list(d.keys())\nfreq = list(d.values())\n\nnodes = []\nfor x in range(len(chars)):\n    heapq.heappush(nodes, Node(freq[x], chars[x]))\n\n\n\nBuilding the Huffman Tree:\n\n\nMerge nodes:\nRepeatedly remove the two nodes with the lowest frequencies from the priority queue and create a new node whose frequency is the sum of the frequencies of the two nodes. Insert this new node back into the priority queue. Continue this process until there is only one node left in the priority queue. This final node represents the root of the Huffman tree.\n\n\nConstruct the Huffman tree:\nThe last node remaining in the priority queue is the root of the Huffman tree. Trace back from this root node to the leaf nodes, creating the binary code for each character as you go. Assign ‘0’ for left branches and ‘1’ for right branches. When you reach a leaf node, you have the Huffman code for that character.\n\nwhile len(nodes) &gt; 1:\n    left = heapq.heappop(nodes)\n    right = heapq.heappop(nodes)\n    left.huff = '0'\n    right.huff = '1'\n    new_symbol = left.symbol + right.symbol\n    new_freq = left.freq + right.freq\n    new_node = Node(new_freq, new_symbol, left, right)\n    heapq.heappush(nodes, new_node)\n\n\n\n\nGenerating Codes:\nTraverse the Huffman tree to assign codes to each symbol based on its position in the tree.\n\ncodes = {} \nhuffman_tree_root = nodes[0]\n\ndef huffman_codes(node, val=''):\n    newVal = val + str(node.huff)\n    if not node.left and not node.right:\n        codes[node.symbol] = newVal\n    else:\n        huffman_codes(node.left, newVal)\n        huffman_codes(node.right, newVal)\n\nhuffman_codes(huffman_tree_root)\n\nfor i in range(len(codes)):\n  print(list(codes.keys())[i],codes[list(codes.keys())[i]])\n\no 000\ni 001\nr 0100\nc 01010\nm 01011\nt 011\ne 100\ns 1010\nu 10110\nd 1011100\nv 1011101\nw 1011110\nx 10111110\nq 10111111\ny 11000\nl 11001\nn 1101\nh 11100\nk 1110100\nb 1110101\nf 111011\na 11110\ng 111110\np 111111\n\n\n\n\nVisualizing the Huffman tree\nThe huffman tree can be visualized using the graphviz python library, the following code saves an image huffman_tree.png which contains the Huffman Tree.\n\nimport graphviz\n\ndef generate_graph(node, dot=None):\n    if dot is None:\n        dot = graphviz.Digraph(format='png')\n        dot.attr(dpi='300', bgcolor='white')  # Set background color\n\n    dot.node(str(id(node)), f\"{node.symbol}:{node.freq}\", style=\"filled\", fillcolor=\"lightblue\")  # Node color\n    if node.left:\n        dot.node(str(id(node.left)), f\"{node.left.symbol}:{node.left.freq}\", style=\"filled\", fillcolor=\"lightgreen\")  # Node color\n        dot.edge(str(id(node)), str(id(node.left)), label=\"0\", color=\"blue\")  # Edge color\n        generate_graph(node.left, dot)\n    if node.right:\n        dot.node(str(id(node.right)), f\"{node.right.symbol}:{node.right.freq}\", style=\"filled\", fillcolor=\"lightgreen\")  # Node color\n        dot.edge(str(id(node)), str(id(node.right)), label=\"1\", color=\"red\")  # Edge color\n        generate_graph(node.right, dot)\n\n    return dot\n\n# Uncomment the following 2 lines before running this cell\n# graph = generate_graph(huffman_tree_root)\n# graph.render(\"huffman_tree\", cleanup=True)\n\nThe Huffman Tree obtained in our case is:"
  },
  {
    "objectID": "posts/huffmancoding/index.html#why-is-huffman-coding-better",
    "href": "posts/huffmancoding/index.html#why-is-huffman-coding-better",
    "title": "Huffman Coding",
    "section": "Why is Huffman coding better?",
    "text": "Why is Huffman coding better?\nLet’s analyze the total number of bits that we need to transmit codes. Total number of bits = Letter frequency * Number of bits used for to represent that letter\n\nprint(\"Without Huffman coding, we need 5 bits to represent every letter.\\nThis means, to transmit all data, we need to transmit a total of: \")\nl1 = len(s)*5\nprint(l1,\"bits\")\n\nl2 = 0\nfor i in range(len(codes)):\n  l2 += len(codes[list(codes.keys())[i]]) * d[list(codes.keys())[i]]\nprint(\"However, using Huffman coding, we need only:\")\nprint(l2,\"bits\")\n\nWithout Huffman coding, we need 5 bits to represent every letter.\nThis means, to transmit all data, we need to transmit a total of: \n2240 bits\nHowever, using Huffman coding, we need only:\n1485 bits"
  },
  {
    "objectID": "posts/huffmancoding/index.html#compression-ratio",
    "href": "posts/huffmancoding/index.html#compression-ratio",
    "title": "Huffman Coding",
    "section": "Compression ratio:",
    "text": "Compression ratio:\nCompression ratio stands for the number of bits needed before compression to that after compression. The higher the compression ratio, the better. The compression ratio for the above quote after Huffman coding is:\n\nprint(\"Compression ratio: \",l1,\" : \",l2,\" = \",(l1/l2))\n\nCompression ratio:  2240  :  1485  =  1.5084175084175084"
  },
  {
    "objectID": "posts/linalg/index.html",
    "href": "posts/linalg/index.html",
    "title": "Matrix Transformations",
    "section": "",
    "text": "When a vector is multiplied by a suitable matrix, the operation essentially transforms the original vector into a new vector. This can be illustrated by the following example. The dimension of the resultant vector may also change (though it does not in this example).\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\nfrom sympy import Eq,Matrix,MatMul\n\n\n\nA = np.array([[2,1],[1,4]])\nx = np.array([1,1])\nAx = A @ x\nEq(Matrix(Ax),MatMul(Matrix(A),Matrix(x)),evaluate=False)\n\n\n\n\ndef plot_arrow(ax,v,color,label):\n    arrow = mpatches.FancyArrowPatch((0,0),(v[0],v[1]),mutation_scale=9,color=color,label=label)\n    ax.add_patch(arrow)\n    ax.legend(bbox_to_anchor=(1.6,1),borderaxespad=0)\n\n\n\ndef plot_transform(A,x):\n  Ax = A @ x\n  fig, ax = plt.subplots()\n  plot_arrow(ax,x,\"k\",f\"Original Vector x: {x}\")\n  plot_arrow(ax,Ax,\"g\",f\"Transformed vector Ax: {Ax}\")\n  plt.xlim((-5,5))\n  plt.ylim((-5,5))\n  plt.grid(alpha=0.1)\n  ax.set_aspect(\"equal\")\n\n\n\nplot_transform(np.array([[2.0,1.0],[1.0,4.0]]),np.array([1.0,1.0]))\n\n\n\n\n\n\n\ndef plot_rot(theta,v):\n  c = np.cos(theta)\n  s = np.sin(theta)\n  rot_mat = np.array([[c,-s],[s,c]])\n  w = rot_mat @ v\n  fig, ax = plt.subplots()\n  plot_arrow(ax,v,\"k\",f\"Original vector: {v}\")\n  plot_arrow(ax,w,\"g\",f\"Vector on rotation: {w}\")\n  plt.xlim((-6,6))\n  plt.ylim((-6,6))\n  plt.grid(alpha=0.4)\n  ax.set_aspect(\"equal\")\n\n\n\nplot_rot(np.pi/3,np.array([3.0,5.0]))\n\n\n\n\n\nRank of a matrix is the minimum number of linearly independent rows and columns or the number of non-zero eigenvalues in case of a square matrix.\nConsider the low rank matrix: \nDefining the function:\ndef plot_lr(v,slope):\n  A1 = np.array([1.0, 2.0])\n  A = np.vstack((A1,slope*A1)) # The low rank matrix\n  x = np.arange(-6,6,0.01)\n  y = slope*x\n  plot_transform(A,v)\n  plt.plot(x,y,lw=5,alpha=0.4,label=f\"y = {slope}x, Column Space of A\")\n  plt.legend(bbox_to_anchor=(1,1),borderaxespad=0)\nLets transform the vector [1.0 2.0] using the above transformation matrix:\nplot_lr(np.array([1.0, 2.0]),4)\nplt.tight_layout()\n\n\n\n\nA = np.array([[1.0,2.0],[4.0,8.0]])\nprint(\"The transformation matrix involved is :\")\nMatrix(A)\n\nprint(\"The rowspace of this matrix is spanned by : \")\nMatrix(np.array([1.0,2.0]))\n\nprint(\"The nullspace of a matrix is always perpendicular to the rowspace.\")\nprint(\"The nullspace of matrix A is spaned by : \")\nMatrix(np.array([-2.0,1.0]))\n\nprint(\"Any vector in the nullspace is converted to a zero matrix after transformation.\")\nplot_lr(np.array([2.0,-1.0]),4)\nplt.plot(x,-0.5*x,lw=5,alpha=0.4,label=f\"y = {-0.5}x, Nullspace of A\",color=\"g\")\nx = np.arange(-6,6,0.01)\nplt.legend(bbox_to_anchor=(1,1), borderaxespad=0)\nplt.tight_layout()\n\nConsider the vector v = [1.0, 1.0] and the same low rank transformation matrix as above.\nThe matrix obtained on transformation is: \nThe projection of v along the rowspace of A is:\nStep 1: Finding the projection matrix:\nr = np.array([1.0, 2.0])\nproj = np.outer(r,r)\nproj = proj / np.inner(r,r)\nMatrix(proj)\nThe projection matrix is: \nStep 2: Finding the projection of v along the rowspace of A\nb = np.array([1.0,1.0])\nv = proj @ b\nMatrix(v)\n\nThe matrix obtained on transformation of this projection vector is:\nAv = A @ v\nEq(MatMul(Matrix(A),Matrix(v)),Matrix(Av),evaluate=False)\n\nWe notice that this is the same as the matrix obtained before.\nLearnings:\n\nAny vector can be written as the vector sum of the projection on the rowspace and projection perpendicular to the rowspace (ie; in the nullspace). Only the component along the rowspace gets transformed to a non-zero matrix, the transformation of the component in the nullspace is always a zero matrix.\nAnalogy to PCA: As all vectors after transformation lie in the column space of A, this can be thought of as dimensionality reduction where there is a change in dimension from the initial vector space to the dimension of the column space of the transformation matrix"
  },
  {
    "objectID": "posts/linalg/index.html#matrix-multiplication-as-transformation-and-interpretation-of-low-rank-matrices",
    "href": "posts/linalg/index.html#matrix-multiplication-as-transformation-and-interpretation-of-low-rank-matrices",
    "title": "Matrix Transformations",
    "section": "",
    "text": "When a vector is multiplied by a suitable matrix, the operation essentially transforms the original vector into a new vector. This can be illustrated by the following example. The dimension of the resultant vector may also change (though it does not in this example).\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\nfrom sympy import Eq,Matrix,MatMul\n\n\n\nA = np.array([[2,1],[1,4]])\nx = np.array([1,1])\nAx = A @ x\nEq(Matrix(Ax),MatMul(Matrix(A),Matrix(x)),evaluate=False)\n\n\n\n\ndef plot_arrow(ax,v,color,label):\n    arrow = mpatches.FancyArrowPatch((0,0),(v[0],v[1]),mutation_scale=9,color=color,label=label)\n    ax.add_patch(arrow)\n    ax.legend(bbox_to_anchor=(1.6,1),borderaxespad=0)\n\n\n\ndef plot_transform(A,x):\n  Ax = A @ x\n  fig, ax = plt.subplots()\n  plot_arrow(ax,x,\"k\",f\"Original Vector x: {x}\")\n  plot_arrow(ax,Ax,\"g\",f\"Transformed vector Ax: {Ax}\")\n  plt.xlim((-5,5))\n  plt.ylim((-5,5))\n  plt.grid(alpha=0.1)\n  ax.set_aspect(\"equal\")\n\n\n\nplot_transform(np.array([[2.0,1.0],[1.0,4.0]]),np.array([1.0,1.0]))\n\n\n\n\n\n\n\ndef plot_rot(theta,v):\n  c = np.cos(theta)\n  s = np.sin(theta)\n  rot_mat = np.array([[c,-s],[s,c]])\n  w = rot_mat @ v\n  fig, ax = plt.subplots()\n  plot_arrow(ax,v,\"k\",f\"Original vector: {v}\")\n  plot_arrow(ax,w,\"g\",f\"Vector on rotation: {w}\")\n  plt.xlim((-6,6))\n  plt.ylim((-6,6))\n  plt.grid(alpha=0.4)\n  ax.set_aspect(\"equal\")\n\n\n\nplot_rot(np.pi/3,np.array([3.0,5.0]))\n\n\n\n\n\nRank of a matrix is the minimum number of linearly independent rows and columns or the number of non-zero eigenvalues in case of a square matrix.\nConsider the low rank matrix: \nDefining the function:\ndef plot_lr(v,slope):\n  A1 = np.array([1.0, 2.0])\n  A = np.vstack((A1,slope*A1)) # The low rank matrix\n  x = np.arange(-6,6,0.01)\n  y = slope*x\n  plot_transform(A,v)\n  plt.plot(x,y,lw=5,alpha=0.4,label=f\"y = {slope}x, Column Space of A\")\n  plt.legend(bbox_to_anchor=(1,1),borderaxespad=0)\nLets transform the vector [1.0 2.0] using the above transformation matrix:\nplot_lr(np.array([1.0, 2.0]),4)\nplt.tight_layout()\n\n\n\n\nA = np.array([[1.0,2.0],[4.0,8.0]])\nprint(\"The transformation matrix involved is :\")\nMatrix(A)\n\nprint(\"The rowspace of this matrix is spanned by : \")\nMatrix(np.array([1.0,2.0]))\n\nprint(\"The nullspace of a matrix is always perpendicular to the rowspace.\")\nprint(\"The nullspace of matrix A is spaned by : \")\nMatrix(np.array([-2.0,1.0]))\n\nprint(\"Any vector in the nullspace is converted to a zero matrix after transformation.\")\nplot_lr(np.array([2.0,-1.0]),4)\nplt.plot(x,-0.5*x,lw=5,alpha=0.4,label=f\"y = {-0.5}x, Nullspace of A\",color=\"g\")\nx = np.arange(-6,6,0.01)\nplt.legend(bbox_to_anchor=(1,1), borderaxespad=0)\nplt.tight_layout()\n\nConsider the vector v = [1.0, 1.0] and the same low rank transformation matrix as above.\nThe matrix obtained on transformation is: \nThe projection of v along the rowspace of A is:\nStep 1: Finding the projection matrix:\nr = np.array([1.0, 2.0])\nproj = np.outer(r,r)\nproj = proj / np.inner(r,r)\nMatrix(proj)\nThe projection matrix is: \nStep 2: Finding the projection of v along the rowspace of A\nb = np.array([1.0,1.0])\nv = proj @ b\nMatrix(v)\n\nThe matrix obtained on transformation of this projection vector is:\nAv = A @ v\nEq(MatMul(Matrix(A),Matrix(v)),Matrix(Av),evaluate=False)\n\nWe notice that this is the same as the matrix obtained before.\nLearnings:\n\nAny vector can be written as the vector sum of the projection on the rowspace and projection perpendicular to the rowspace (ie; in the nullspace). Only the component along the rowspace gets transformed to a non-zero matrix, the transformation of the component in the nullspace is always a zero matrix.\nAnalogy to PCA: As all vectors after transformation lie in the column space of A, this can be thought of as dimensionality reduction where there is a change in dimension from the initial vector space to the dimension of the column space of the transformation matrix"
  }
]