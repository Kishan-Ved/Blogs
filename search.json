[
  {
    "objectID": "posts/secondorder/index.html",
    "href": "posts/secondorder/index.html",
    "title": "Optimizing bivariate functions",
    "section": "",
    "text": "Hessian matrix is a square matrix of second order partial derivatives of a scalar valued function. It can be used to describe the local curvature of the function at a point and it is denoted by H.\n\\[\nH(f) = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \\cdots& \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\frac{\\partial^2 f}{\\partial x_2\\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} \\cdots& \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\ \\vdots &\\quad \\vdots \\quad \\ddots & \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2}  \\cdots& \\frac{\\partial^2 f}{\\partial x_n^2} \\end{bmatrix}\n\\]\nFor example, let’s take a bivariate function(n=2),\n\\[\nf(x,y) = xy^2\n\\]\n\\[\nH(f)= \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\ \\frac{\\partial^2 f}{\\partial y \\partial x} & \\frac{\\partial^2 f}{\\partial y^2} \\end{bmatrix}\n\\]\nHere, \\(\\frac{\\partial f}{\\partial x}=y^2\\), \\(\\frac{\\partial f}{\\partial y}=2xy\\), \\(\\frac{\\partial^2 f}{\\partial x^2}=0,\\frac{\\partial^2 f}{\\partial y^2}=2x,\\frac{\\partial^2 f}{\\partial x \\partial y}=\\frac{\\partial^2 f}{\\partial y \\partial x}=2y\\)\n\\[\nH(f)=\\begin{bmatrix} 0 & 2y \\\\ 2y & 2x \\end{bmatrix}\n\\]\nUsing this matrix, we can find out the nature of the curvature at any point \\((x_1,y_1)\\), by substituting this point in the Hessian.\nIf the Hessian matrix is positive definite(all eigenvalues are positive) at a point, it indicates that the function is locally convex(has a local minimum) around that point. If it is negative definite, the function is locally concave. If the eigenvalues have both positive and negative values, then this point has a mixture of concave and convex behaviour in different directions and such a point is called a saddle point.\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define a sample function\ndef f(x):\n    return x[0]*x[1]**2 \n\n# point where you want to compute Hessian matrix\n# requires_grad=True tells pytorch to keep track of x0 which form a computation graph to compute gradients easily.\nx0 = torch.tensor([2.0, 1.0], requires_grad=True)\n# create_graph=True is used to compute higher order derivatives in the computation graph\ngrads = torch.autograd.grad(f(x0), x0, create_graph=True)[0]\nHessian = torch.zeros((len(x0), len(x0)))\nfor i in range(len(x0)):\n    Hessian[i] = torch.autograd.grad(grads[i], x0, retain_graph=True)[0]\n\nHessian = Hessian.detach().numpy()\nplt.imshow(Hessian, cmap='coolwarm')\nplt.xticks(np.arange(len(x0)))\nplt.yticks(np.arange(len(x0)))\nplt.xlabel('Hessian Row Index')\nplt.ylabel('Hessian Column Index')\nplt.colorbar()\nplt.title('Visualization of the Hessian Matrix')\nplt.show()\n\n\n\n\n\n\nFollowing are the steps to find minimum or maximum of a function:\n\nMake an intial guess.\nAt the initial guess, we find out how steep the slope of the curve is and how quickly the slope is changing. Hence, we calculate the first derivative and the second derivative at this point.\nWe can approximate a quadratic function(parabolic bowl) at that point using the taylor series.\nNewton’s method then moves to the minimum of the parabolic bowl which is the new guess for the optimal point of the original function.\nThis process repeats and with each iteration you edge closer to the optimal value of the original function and finally newton’s method converges.\n\nAt any iteration, the value of \\(x\\) can be updated as,\n\\[\nx_{i+1} = x_i-H^{-1}\\nabla f(x_i) \\quad -(*)\n\\]\nwhere \\(H^{-1}\\) is the inverse of the hessian(which is initially assumed to be the identity matrix) and \\(\\nabla f(x_i)\\) is an array/vector containing the partial derivatives of \\(f\\) with respect to all the variables.\nFollowing is the code for optimizing \\(f(x,y)=-sin(x)-cos(y)\\).\nLet’s first plot \\(f(x,y)\\).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-5, 5, 100)\ny = np.linspace(-5, 5, 100)\nX, Y = np.meshgrid(x, y)\n\ndef f(X,Y):\n    return -np.sin(X) - np.cos(Y)\nZ = f(X,Y)\n\nfig = plt.figure(figsize=(8,6))\nax1= fig.add_subplot(111, projection='3d')\nax1.plot_surface(X, Y, Z, cmap='viridis')\nax1.set_xlabel('X')\nax1.set_ylabel('Y')\nax1.set_zlabel('Z')\nax1.set_title('f(x) = -sin(x) - cos(y)')\nplt.show()\n\n\n\n\n\nimport torch\n\ndef f(x):\n    return -torch.sin(x[0])-torch.cos(x[1])\n\niterations = 10\n\n\ndef newton(guess,f,iterations):\n  guesses=[]\n  guesses.append(guess)\n  for i in range(iterations):\n      f_value = f(guess)\n      gradient = torch.autograd.grad(f_value, guess, create_graph=True)[0]\n      hessian = torch.zeros((len(guess), len(guess)))\n      for j in range(len(guess)):\n          hessian_row = torch.autograd.grad(gradient[j], guess, retain_graph=True)[0]\n          hessian[j] = hessian_row\n      step = -torch.linalg.solve(hessian, gradient)\n      guess = guess + step\n      guesses.append(guess)\n  return guesses\n      \n\nguess = torch.tensor([2.0, 1.0], requires_grad=True)\nguesses=newton(guess,f,iterations)\nfor i in range(len(guesses)):\n  print(f\"Iteration {i}: guess = {guesses[i]}\")\n\nIteration 0: guess = tensor([2., 1.], requires_grad=True)\nIteration 1: guess = tensor([ 1.5423, -0.5574], grad_fn=&lt;AddBackward0&gt;)\nIteration 2: guess = tensor([1.5708, 0.0659], grad_fn=&lt;AddBackward0&gt;)\nIteration 3: guess = tensor([ 1.5708e+00, -9.5725e-05], grad_fn=&lt;AddBackward0&gt;)\nIteration 4: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 5: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 6: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 7: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 8: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 9: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 10: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\n\n\nSo after updating our guess using \\((*)\\) for a sufficient number of iterations, we get our final guess as \\(x=1.5708 \\quad and \\quad y=0.0\\).\nLet’s plot the contour plot of the above function to verify our results.\n\nimport time \n\nx = np.linspace(-6, 6, 100)\ny = np.linspace(-6, 6, 100)\nX, Y = np.meshgrid(x, y)\n\ndef f1(X,Y):\n    return -np.sin(X) - np.cos(Y)\nZ = f1(X,Y)\n\ndef plot_contour(guesses):\n  fig=plt.figure(figsize=(10,6))\n  ax = fig.add_subplot(111)\n  contour = ax.contourf(X,Y,Z)\n  plt.colorbar(contour)\n  ax.set_xlabel(\"X\")\n  ax.set_ylabel(\"Y\")\n  ax.set_title(\"Contour plot of f(x,y)\")\n  marker=\"\"\n  for i in range(2):\n    if i==0:\n      marker=\"o\"\n      color=\"cyan\"\n      ax.scatter(guesses[0][0].detach().numpy(), guesses[0][1].detach().numpy(), color=color, alpha=1,marker=marker)\n    else:\n      marker=\"x\"\n      color=\"red\"\n      ax.scatter(guesses[-1][0].detach().numpy(), guesses[-1][1].detach().numpy(), color=color, alpha=1,marker=marker)\n  plt.show()\n  \nplot_contour(guesses)\n\n\n\n\nhttps://www.flexclip.com/share/367853402cf50de52d9ad00687ce49806e7a8e5.html\nThrough the contour plot we can understand that even though our initial guess was the point \\((2,1)\\) we finally reached the minima of the function. In the above contour plot, the bluish circle is the initial guess and the red cross is the final guess.\nDepending upon different initial guesses, the final guess could land onto different minimas or possibly even a saddle point.\nLet’s say we take another point \\((1,-2)\\).\n\nguess = torch.tensor([1.0, -2.0], requires_grad=True)\niterations=10\nguesses = newton(guess,f,iterations)\nplot_contour(guesses)\n\n\n\n\nIn the case above we got a saddle point, this is one of the drawbacks of the newton method.\nAlthough the newton’s method for optimization converges faster than the gradient descent algorithm and one doesn’t have to also face the difficulty in deciding the learning rate as is faced in gradient descent, the computation of the Hessian and it’s inverse is computationally very expensive(having computational complexity of \\(O(n^3)\\) for functions with n variables.\nIn order to use this method for optimization, the hessian needs to be positive definite which may not always be possible.\nHence to overcome these scenarios, Quasi-newton optimization algorithms can be used like the BFGS, and the LBFGS, where we try to approximate the hessian instead of calculating it.\n\n\n\nThe BFGS algorithm constructs an approximation of the inverse Hessian matrix using a sequence of rank-two updates. This approximation captures information about the curvature of the objective function’s landscape and guides the optimization process. BFGS has good convergence properties and doesn’t require the explicit computation of the Hessian matrix, making it suitable for problems with a large number of variables.\nL-BFGS is a variant of the BFGS algorithm that addresses the memory and computational requirements associated with the Hessian matrix. In high-dimensional optimization problems, storing and manipulating the full Hessian matrix can be expensive. L-BFGS overcomes this limitation by maintaining a limited-memory approximation of the Hessian, using only a small number of vectors.\nL-BFGS uses a recursive formula to update and approximate the inverse Hessian matrix. Instead of storing the full Hessian matrix explicitly, L-BFGS maintains a limited number of vector pairs to approximate the Hessian. This makes L-BFGS well-suited for large-scale optimization problems and enables it to operate efficiently in high-dimensional spaces.\nThe following code implements LBFGS of the function -sin(x)-cos(y)\n\nimport torch\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\n\ndef f(x):\n    return -torch.sin(x[0])-torch.cos(x[1])\n\n# L-BFGS\ndef closure():\n    lbfgs.zero_grad()\n    objective = f(x_lbfgs)\n    objective.backward()\n    return objective\n\nx_lbfgs = torch.ones(2, 1)\nx_lbfgs.requires_grad = True\n\nlbfgs = optim.LBFGS([x_lbfgs],\n                    history_size=10, \n                    max_iter=4, \n                    line_search_fn=\"strong_wolfe\")\n                    \nhistory_lbfgs = []\nfor i in range(100):\n    history_lbfgs.append(f(x_lbfgs).item())\n    lbfgs.step(closure)\n\nLet us also perform gradient descent on this, with learning rate of 10^-5.\n\nx_gd = torch.ones(2, 1)\nx_gd.requires_grad = True\ngd = optim.SGD([x_gd], lr=1e-5)\n\nhistory_gd = []\nfor i in range(100):\n    gd.zero_grad()\n    objective = f(x_gd)\n    objective.backward()\n    gd.step()\n    history_gd.append(objective.item())\n\nNow, to visualize the results, we use a contour plot:\n\nx_range = np.linspace(-5, 5, 400)\ny_range = np.linspace(-5, 5, 400)\nX, Y = np.meshgrid(x_range, y_range)\n\nZ = f(torch.tensor([X, Y])).detach().numpy()\n\nfig=plt.figure(figsize=(10,6))\nplt.contourf(X, Y, Z, levels=20, cmap=\"viridis\")\n\ncoordinates = np.array([2.0, 1.0])\nplt.plot(coordinates[0], coordinates[1], marker=\"o\", color=\"cyan\", label=\"Initial Coordinates\")\n\ncoordinates = x_lbfgs.detach().numpy()\nplt.plot(coordinates[0], coordinates[1], marker=\"o\", color=\"red\", label=\"LBFGS\")\n\ncoordinates = x_gd.detach().numpy()\nplt.plot(coordinates[0], coordinates[1], marker=\"o\", color=\"orange\", label=\"Grad Desc (lr=1e-5)\")\n\nplt.colorbar(label=\"Objective Value\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.title(\"Contour Plot of -sin(X)-cos(Y)\")\nplt.legend()\nplt.show()\n\n\n\n\nWe observe that in a 100 iterations, the gradient descent algorithm does not converge to the minima, but remains somewhere in between. Changing the learning rate might lead to the optimal value. For L-BFGS, the convergence is at the minima.\n\n\n\nThe LBFGS method is appealing for several reasons it is very simple to implement it requires only function and gradient values and no other information on the problem # and it can be faster than the partitioned quasi Newton method on problems where the element functions depend on more than or variables\nIn addition the LBFGS method appears to be preferable to PQN for large problems in which the Hessian matrix is not very sparse or for problems in which the information on the separablity of the ob jective function is difficult to obtain."
  },
  {
    "objectID": "posts/secondorder/index.html#newtons-method-for-optimizing-bivariate-functions-using-hessian.",
    "href": "posts/secondorder/index.html#newtons-method-for-optimizing-bivariate-functions-using-hessian.",
    "title": "Optimizing bivariate functions",
    "section": "",
    "text": "Following are the steps to find minimum or maximum of a function:\n\nMake an intial guess.\nAt the initial guess, we find out how steep the slope of the curve is and how quickly the slope is changing. Hence, we calculate the first derivative and the second derivative at this point.\nWe can approximate a quadratic function(parabolic bowl) at that point using the taylor series.\nNewton’s method then moves to the minimum of the parabolic bowl which is the new guess for the optimal point of the original function.\nThis process repeats and with each iteration you edge closer to the optimal value of the original function and finally newton’s method converges.\n\nAt any iteration, the value of \\(x\\) can be updated as,\n\\[\nx_{i+1} = x_i-H^{-1}\\nabla f(x_i) \\quad -(*)\n\\]\nwhere \\(H^{-1}\\) is the inverse of the hessian(which is initially assumed to be the identity matrix) and \\(\\nabla f(x_i)\\) is an array/vector containing the partial derivatives of \\(f\\) with respect to all the variables.\nFollowing is the code for optimizing \\(f(x,y)=-sin(x)-cos(y)\\).\nLet’s first plot \\(f(x,y)\\).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-5, 5, 100)\ny = np.linspace(-5, 5, 100)\nX, Y = np.meshgrid(x, y)\n\ndef f(X,Y):\n    return -np.sin(X) - np.cos(Y)\nZ = f(X,Y)\n\nfig = plt.figure(figsize=(8,6))\nax1= fig.add_subplot(111, projection='3d')\nax1.plot_surface(X, Y, Z, cmap='viridis')\nax1.set_xlabel('X')\nax1.set_ylabel('Y')\nax1.set_zlabel('Z')\nax1.set_title('f(x) = -sin(x) - cos(y)')\nplt.show()\n\n\n\n\n\nimport torch\n\ndef f(x):\n    return -torch.sin(x[0])-torch.cos(x[1])\n\niterations = 10\n\n\ndef newton(guess,f,iterations):\n  guesses=[]\n  guesses.append(guess)\n  for i in range(iterations):\n      f_value = f(guess)\n      gradient = torch.autograd.grad(f_value, guess, create_graph=True)[0]\n      hessian = torch.zeros((len(guess), len(guess)))\n      for j in range(len(guess)):\n          hessian_row = torch.autograd.grad(gradient[j], guess, retain_graph=True)[0]\n          hessian[j] = hessian_row\n      step = -torch.linalg.solve(hessian, gradient)\n      guess = guess + step\n      guesses.append(guess)\n  return guesses\n      \n\nguess = torch.tensor([2.0, 1.0], requires_grad=True)\nguesses=newton(guess,f,iterations)\nfor i in range(len(guesses)):\n  print(f\"Iteration {i}: guess = {guesses[i]}\")\n\nIteration 0: guess = tensor([2., 1.], requires_grad=True)\nIteration 1: guess = tensor([ 1.5423, -0.5574], grad_fn=&lt;AddBackward0&gt;)\nIteration 2: guess = tensor([1.5708, 0.0659], grad_fn=&lt;AddBackward0&gt;)\nIteration 3: guess = tensor([ 1.5708e+00, -9.5725e-05], grad_fn=&lt;AddBackward0&gt;)\nIteration 4: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 5: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 6: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 7: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 8: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 9: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 10: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\n\n\nSo after updating our guess using \\((*)\\) for a sufficient number of iterations, we get our final guess as \\(x=1.5708 \\quad and \\quad y=0.0\\).\nLet’s plot the contour plot of the above function to verify our results.\n\nimport time \n\nx = np.linspace(-6, 6, 100)\ny = np.linspace(-6, 6, 100)\nX, Y = np.meshgrid(x, y)\n\ndef f1(X,Y):\n    return -np.sin(X) - np.cos(Y)\nZ = f1(X,Y)\n\ndef plot_contour(guesses):\n  fig=plt.figure(figsize=(10,6))\n  ax = fig.add_subplot(111)\n  contour = ax.contourf(X,Y,Z)\n  plt.colorbar(contour)\n  ax.set_xlabel(\"X\")\n  ax.set_ylabel(\"Y\")\n  ax.set_title(\"Contour plot of f(x,y)\")\n  marker=\"\"\n  for i in range(2):\n    if i==0:\n      marker=\"o\"\n      color=\"cyan\"\n      ax.scatter(guesses[0][0].detach().numpy(), guesses[0][1].detach().numpy(), color=color, alpha=1,marker=marker)\n    else:\n      marker=\"x\"\n      color=\"red\"\n      ax.scatter(guesses[-1][0].detach().numpy(), guesses[-1][1].detach().numpy(), color=color, alpha=1,marker=marker)\n  plt.show()\n  \nplot_contour(guesses)\n\n\n\n\nhttps://www.flexclip.com/share/367853402cf50de52d9ad00687ce49806e7a8e5.html\nThrough the contour plot we can understand that even though our initial guess was the point \\((2,1)\\) we finally reached the minima of the function. In the above contour plot, the bluish circle is the initial guess and the red cross is the final guess.\nDepending upon different initial guesses, the final guess could land onto different minimas or possibly even a saddle point.\nLet’s say we take another point \\((1,-2)\\).\n\nguess = torch.tensor([1.0, -2.0], requires_grad=True)\niterations=10\nguesses = newton(guess,f,iterations)\nplot_contour(guesses)\n\n\n\n\nIn the case above we got a saddle point, this is one of the drawbacks of the newton method.\nAlthough the newton’s method for optimization converges faster than the gradient descent algorithm and one doesn’t have to also face the difficulty in deciding the learning rate as is faced in gradient descent, the computation of the Hessian and it’s inverse is computationally very expensive(having computational complexity of \\(O(n^3)\\) for functions with n variables.\nIn order to use this method for optimization, the hessian needs to be positive definite which may not always be possible.\nHence to overcome these scenarios, Quasi-newton optimization algorithms can be used like the BFGS, and the LBFGS, where we try to approximate the hessian instead of calculating it."
  },
  {
    "objectID": "posts/secondorder/index.html#l-bfgs-for-optimizing-functions",
    "href": "posts/secondorder/index.html#l-bfgs-for-optimizing-functions",
    "title": "Optimizing bivariate functions",
    "section": "",
    "text": "The BFGS algorithm constructs an approximation of the inverse Hessian matrix using a sequence of rank-two updates. This approximation captures information about the curvature of the objective function’s landscape and guides the optimization process. BFGS has good convergence properties and doesn’t require the explicit computation of the Hessian matrix, making it suitable for problems with a large number of variables.\nL-BFGS is a variant of the BFGS algorithm that addresses the memory and computational requirements associated with the Hessian matrix. In high-dimensional optimization problems, storing and manipulating the full Hessian matrix can be expensive. L-BFGS overcomes this limitation by maintaining a limited-memory approximation of the Hessian, using only a small number of vectors.\nL-BFGS uses a recursive formula to update and approximate the inverse Hessian matrix. Instead of storing the full Hessian matrix explicitly, L-BFGS maintains a limited number of vector pairs to approximate the Hessian. This makes L-BFGS well-suited for large-scale optimization problems and enables it to operate efficiently in high-dimensional spaces.\nThe following code implements LBFGS of the function -sin(x)-cos(y)\n\nimport torch\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\n\ndef f(x):\n    return -torch.sin(x[0])-torch.cos(x[1])\n\n# L-BFGS\ndef closure():\n    lbfgs.zero_grad()\n    objective = f(x_lbfgs)\n    objective.backward()\n    return objective\n\nx_lbfgs = torch.ones(2, 1)\nx_lbfgs.requires_grad = True\n\nlbfgs = optim.LBFGS([x_lbfgs],\n                    history_size=10, \n                    max_iter=4, \n                    line_search_fn=\"strong_wolfe\")\n                    \nhistory_lbfgs = []\nfor i in range(100):\n    history_lbfgs.append(f(x_lbfgs).item())\n    lbfgs.step(closure)\n\nLet us also perform gradient descent on this, with learning rate of 10^-5.\n\nx_gd = torch.ones(2, 1)\nx_gd.requires_grad = True\ngd = optim.SGD([x_gd], lr=1e-5)\n\nhistory_gd = []\nfor i in range(100):\n    gd.zero_grad()\n    objective = f(x_gd)\n    objective.backward()\n    gd.step()\n    history_gd.append(objective.item())\n\nNow, to visualize the results, we use a contour plot:\n\nx_range = np.linspace(-5, 5, 400)\ny_range = np.linspace(-5, 5, 400)\nX, Y = np.meshgrid(x_range, y_range)\n\nZ = f(torch.tensor([X, Y])).detach().numpy()\n\nfig=plt.figure(figsize=(10,6))\nplt.contourf(X, Y, Z, levels=20, cmap=\"viridis\")\n\ncoordinates = np.array([2.0, 1.0])\nplt.plot(coordinates[0], coordinates[1], marker=\"o\", color=\"cyan\", label=\"Initial Coordinates\")\n\ncoordinates = x_lbfgs.detach().numpy()\nplt.plot(coordinates[0], coordinates[1], marker=\"o\", color=\"red\", label=\"LBFGS\")\n\ncoordinates = x_gd.detach().numpy()\nplt.plot(coordinates[0], coordinates[1], marker=\"o\", color=\"orange\", label=\"Grad Desc (lr=1e-5)\")\n\nplt.colorbar(label=\"Objective Value\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.title(\"Contour Plot of -sin(X)-cos(Y)\")\nplt.legend()\nplt.show()\n\n\n\n\nWe observe that in a 100 iterations, the gradient descent algorithm does not converge to the minima, but remains somewhere in between. Changing the learning rate might lead to the optimal value. For L-BFGS, the convergence is at the minima."
  },
  {
    "objectID": "posts/secondorder/index.html#remarks1",
    "href": "posts/secondorder/index.html#remarks1",
    "title": "Optimizing bivariate functions",
    "section": "",
    "text": "The LBFGS method is appealing for several reasons it is very simple to implement it requires only function and gradient values and no other information on the problem # and it can be faster than the partitioned quasi Newton method on problems where the element functions depend on more than or variables\nIn addition the LBFGS method appears to be preferable to PQN for large problems in which the Hessian matrix is not very sparse or for problems in which the information on the separablity of the ob jective function is difficult to obtain."
  },
  {
    "objectID": "posts/secondorder/index.html#footnotes",
    "href": "posts/secondorder/index.html#footnotes",
    "title": "Optimizing bivariate functions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLiu, D.C. and Nocedal, J. (no date) On the limited memory BFGS method for large scale optimization - mathematical programming, SpringerLink. Available at: https://link.springer.com/article/10.1007/BF01589116 (Accessed: 20 August 2023).↩︎"
  },
  {
    "objectID": "posts/mlalgos/logistic_regression_from_scratch.html",
    "href": "posts/mlalgos/logistic_regression_from_scratch.html",
    "title": "Logistic Regression from Scratch",
    "section": "",
    "text": "##Logistic Regression The following describes in detail the algorithm of Logistic Regression for mltiple features represented by the vector x.\nSuppose x0 and x1 are 2 fearutes that define whether the element belongs to class 0 or class 1, we can use logistic regression to define a boundary between the 2 classes.\n###Cost Function\n\nimport numpy as np\n\ndef cost_func(x,y,w,b):\n  cost = 0\n  n_points = y.shape[0]\n  for i in range(n_points):\n    f = 1/(1 + np.exp(-1*(np.dot(w,x[i])+b))) # Sigmoid function\n    cost += (-y[i]*np.log(f)-(1-y[i])*np.log(1-f))\n  cost = cost/(n_points)\n  return cost\n\n###Derivative Function\n\ndef derivative_func(x,y,w,b):\n  n_points = y.shape[0]\n  n_features = x[0].shape[0]\n  dw = np.zeros(n_features) # Dont use [0]*n_features, as then the datatype becomes int and it truncates every element up to the decimal, do dw[i] is always 0\n  db = 0\n  for i in range(n_points):\n    f = 1/(1 + np.exp(-1*(np.dot(w,x[i])+b))) # Sigmoid function\n    for j in range(n_features):\n      dw[j] += (f - y[i])*x[i][j]/n_points\n    db += (f - y[i])/n_points\n  return dw, db\n\n\nx_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\ny_train = np.array([0, 0, 0, 1, 1, 1])\nw = np.array([2.,3.])\nb = 1.\ndw,db = derivative_func(x_train, y_train, w, b)\nprint(dw, db)\n\n[0.49833339 0.49883943] 0.49861806546328574\n\n\n###Gradient Descent Function\n\ndef grad_desc(x,y,w,b,a,n,derivative_func, cost_func):\n  cost_arr = []\n  for i in range(n):\n    dw,db = derivative_func(x,y,w,b)\n    w = w - a * dw\n    b = b - a * db\n    cost = cost_func(x,y,w,b)\n    cost_arr.append(cost)\n    if(i%10000==0 or i==n-1):\n      print(\"iteration:\",i+1,\"w:\", w,\"b:\", b, \"cost:\", cost)\n  return w, b, cost, cost_arr\n\n###Implementation\n\nimport numpy as np\nx_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\ny_train = np.array([0, 0, 0, 1, 1, 1])\nw = np.array([0,0])\nb = 0\na = 0.01\nn = 100000\n\n\nimport matplotlib.pyplot as plt\nclass_0_points = x_train[y_train == 0]\nclass_1_points = x_train[y_train == 1]\n\nplt.scatter(class_0_points[:, 0], class_0_points[:, 1], c='red', marker='o', label='Class 0')\nplt.scatter(class_1_points[:, 0], class_1_points[:, 1], c='blue', marker='x', label='Class 1')\n\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('Scatter Plot of x_train')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n###Result of the model\n\nw,b,cost,cost_arr = grad_desc(x_train, y_train, w, b, a, n, derivative_func, cost_func)\n\niteration: 1 w: [0.0025     0.00166667] b: -2.7755575615628914e-19 cost: 0.6922493145189493\niteration: 10001 w: [2.30405443 2.07569485] b: -5.906224646656873 cost: 0.15929433526969025\niteration: 20001 w: [3.15399029 2.9391181 ] b: -8.308406449661286 cost: 0.08467022766207012\niteration: 30001 w: [3.68009634 3.47013631] b: -9.779995117023205 cost: 0.05708873095289405\niteration: 40001 w: [4.05958107 3.85211168] b: -10.836781277572722 cost: 0.042929169405013574\niteration: 50001 w: [4.35591528 4.14993563] b: -11.659973341953908 cost: 0.03435304388535467\niteration: 60001 w: [4.59881057 4.3938154 ] b: -12.333644748840326 cost: 0.02861432636904086\niteration: 70001 w: [4.80451141 4.60021177] b: -12.903521835706426 cost: 0.02450955388769534\niteration: 80001 w: [4.98285153 4.77906746] b: -13.39718959763814 cost: 0.02142997830044766\niteration: 90001 w: [5.14022947 4.93684162] b: -13.832552219301215 cost: 0.019035206450251734\niteration: 100000 w: [5.28102582 5.07795116] b: -14.22184473980357 cost: 0.017120404762803472\n\n\n##Plotting the decision boundary\n\nx0 = np.arange(0,3,0.001)\nx1 = (-b-w[0]*x0)/w[1] # This is the decision boundary\nplt.plot(x0,x1,label='Decision Boundary')\nplt.scatter(class_0_points[:, 0], class_0_points[:, 1], c='red', marker='o', label='Class 0')\nplt.scatter(class_1_points[:, 0], class_1_points[:, 1], c='blue', marker='x', label='Class 1')\n\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('Scatter Plot of x_train')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n##Non-linear decision boundary\nNow we try to modify the decision boundary to: w0.x + w1.y + w3.x^2 + w4.y^2 + b = 0\n\nimport numpy as np\n\nx_train = np.array([[1, 1], [1, -1], [-1, 1], [-1, -1], [1.5, 0], [-1.5, 0], [0, 1.5], [0, -1.5], [2, 2], [2, -2], [-2, 2], [-2, -2]])\n\n# Calculate the squares of each element in x_train\nsquared_x_train = np.square(x_train)\n\n# Concatenate the squared values to each sub-array in x_train\nx_train_with_squares = np.concatenate((x_train, squared_x_train), axis=1)\n\nprint(x_train_with_squares)\n\n[[ 1.    1.    1.    1.  ]\n [ 1.   -1.    1.    1.  ]\n [-1.    1.    1.    1.  ]\n [-1.   -1.    1.    1.  ]\n [ 1.5   0.    2.25  0.  ]\n [-1.5   0.    2.25  0.  ]\n [ 0.    1.5   0.    2.25]\n [ 0.   -1.5   0.    2.25]\n [ 2.    2.    4.    4.  ]\n [ 2.   -2.    4.    4.  ]\n [-2.    2.    4.    4.  ]\n [-2.   -2.    4.    4.  ]]\n\n\n\ny_train = np.array([0, 0, 0, 0, 0,0 ,0,0,1, 1, 1,1])\nw = np.array([1,1,1,1])\nb = 0\na = 0.01\nn = 100000\n\nw,b,cost,cost_arr = grad_desc(x_train_with_squares, y_train, w, b, a, n, derivative_func, cost_func)\n\niteration: 1 w: [0.99919605 0.99919605 0.99425383 0.99425383] b: -0.005447789625105651 cost: 1.5573300591796866\niteration: 10001 w: [8.41475138e-07 8.41475138e-07 1.18908181e+00 1.18908181e+00] b: -5.765043873820958 cost: 0.03368980146703054\niteration: 20001 w: [1.64239739e-08 1.64239739e-08 1.43071179e+00 1.43071179e+00] b: -6.99571255551648 cost: 0.016713393167275607\niteration: 30001 w: [1.60833801e-09 1.60833801e-09 1.57187128e+00 1.57187128e+00] b: -7.713800697151975 cost: 0.01108075752914621\niteration: 40001 w: [3.08300777e-10 3.08300776e-10 1.67177372e+00 1.67177372e+00] b: -8.22180568788111 cost: 0.008280933527407662\niteration: 50001 w: [8.55363287e-11 8.55363287e-11 1.74910732e+00 1.74910732e+00] b: -8.614974938292368 cost: 0.006608482664955559\niteration: 60001 w: [2.99969234e-11 2.99969233e-11 1.81219413e+00 1.81219413e+00] b: -8.935681550129729 cost: 0.0054972447093098\niteration: 70001 w: [1.23672660e-11 1.23672660e-11 1.86546702e+00 1.86546702e+00] b: -9.206484123204053 cost: 0.00470556144631653\niteration: 80001 w: [5.74014117e-12 5.74014108e-12 1.91156787e+00 1.91156787e+00] b: -9.440822251578682 cost: 0.0041130242660977045\niteration: 90001 w: [2.91658887e-12 2.91658881e-12 1.95219819e+00 1.95219819e+00] b: -9.647349774128628 cost: 0.0036529359870781514\niteration: 100000 w: [1.59167870e-12 1.59167867e-12 1.98851473e+00 1.98851473e+00] b: -9.831948965175554 cost: 0.0032854109483625386\n\n\nw[0] and w[1] tend to 0, this means that there is very little contribution from them (we can also remove those terms).\n\nclass_0_points = x_train[y_train == 0]\nclass_1_points = x_train[y_train == 1]\nplt.scatter(class_0_points[:, 0], class_0_points[:, 1], c='red', marker='o', label='Class 0')\nplt.scatter(class_1_points[:, 0], class_1_points[:, 1], c='blue', marker='x', label='Class 1')\n\nw0, w1, w3, w4 = w\n\nx = np.linspace(-5, 5, 100)\ny = np.linspace(-5, 5, 100)\nX, Y = np.meshgrid(x, y)\nZ = w0 * X + w1 * Y + w3 * X**2 + w4 * Y**2 + b\nplt.contour(X, Y, Z, levels=[0], colors='g', label=\"Decision boundary\")\n\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('Scatter Plot of x_train')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nUserWarning: The following kwargs were not used by contour: 'label'\n  plt.contour(X, Y, Z, levels=[0], colors='g', label=\"Decision boundary\")\n\n\n\n\n\n##Another example\n\ndef derivative_func(x,y,w,b):\n  n_points = y.shape[0]\n  n_features = x.shape[0]\n  dw = 0 # Dont use [0]*n_features, as then the datatype becomes int and it truncates every element up to the decimal, do dw[i] is always 0\n  db = 0\n  for i in range(n_points):\n    f = 1/(1 + np.exp(-1*(np.dot(w,x[i])+b))) # Sigmoid function\n    dw += (f - y[i])*x[i]/n_points\n    db += (f - y[i])/n_points\n  return dw, db\n\nimport numpy as np\nx_train = np.array([0. ,1. ,2. ,3. ,4. ,5.])\ny_train = np.array([0., 0., 0., 1., 1., 1.])\nw = 0.\nb = 0\na = 0.01\nn = 100000\n\nw,b,cost,cost_arr = grad_desc(x_train, y_train, w, b, a, n, derivative_func, cost_func)\n\niteration: 1 w: 0.0075 b: -2.7755575615628914e-19 cost: 0.6875866309962527\niteration: 10001 w: 2.1867501108184713 b: -5.16917809323396 cost: 0.11340307961302272\niteration: 20001 w: 2.861115544430589 b: -6.907170232585448 cost: 0.07798162891279302\niteration: 30001 w: 3.3342040584439254 b: -8.111040905678628 cost: 0.061142541357167964\niteration: 40001 w: 3.7092655036875835 b: -9.060256188655838 cost: 0.05069045939178313\niteration: 50001 w: 4.023241491259317 b: -9.852492089866404 cost: 0.04341259172881062\niteration: 60001 w: 4.294480337340172 b: -10.535609693427636 cost: 0.038002132335623905\niteration: 70001 w: 4.533744351000855 b: -11.137432454212933 cost: 0.033802915742773575\niteration: 80001 w: 4.748012696977043 b: -11.675891049855071 cost: 0.030441380223630285\niteration: 90001 w: 4.942123793732626 b: -12.163359911131781 cost: 0.027686301223508947\niteration: 100000 w: 5.119578248398071 b: -12.608762220986433 cost: 0.025385931832151234\n\n\n\nx = np.arange(0,3,0.001)\ny = w*x + b # This is the decision boundary\nplt.plot(x,y,label='Decision Boundary')\n\nclass_0_points = x_train[y_train == 0]\nclass_1_points = x_train[y_train == 1]\nplt.scatter(class_0_points,y_train[0:3], c='red', marker='X', label='Class 0')\nplt.scatter(class_1_points,y_train[3:], c='blue', marker='o', label='Class 1')\n\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('Scatter Plot of x_train')\nplt.ylim([-2,2])\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n##We can further refine the learning rate as in my gradient descent google colab"
  },
  {
    "objectID": "posts/mlalgos/gradient_descent_multivariable_vectorized_from_scratch.html",
    "href": "posts/mlalgos/gradient_descent_multivariable_vectorized_from_scratch.html",
    "title": "Vectorized Multivariable Linear Regression from Scratch",
    "section": "",
    "text": "The following describes in detail the vectorized imiplementation of the algorithm of gradient descent for mltiple features represented by the vector x.\n\n\n\ndef cost_func(x,y,w,b):\n  cost = 0\n  n_points = y.shape[0]\n  for i in range(n_points):\n    f = np.dot(w,x[i])+b\n    cost += (f -y[i])**2\n  cost = cost/(2*n_points)\n  return cost\n\n\n\n\n\ndef derivative_func(x,y,w,b):\n  n_points = y.shape[0]\n  n_features = x[0].shape[0]\n  dw = np.array([0]*n_features)\n  db = 0\n  for i in range(n_points):\n    f = np.dot(w,x[i]) + b\n    for j in range(n_features):\n      dw[j] += (f - y[i])*x[i][j]/n_points\n    db += (f - y[i])/n_points\n  return dw, db\n\n\n\n\n\ndef grad_desc(x,y,w,b,a,n,derivative_func, cost_func):\n  cost_arr = []\n  for i in range(n):\n    dw,db = derivative_func(x,y,w,b)\n    w = w - a * dw\n    b = b - a * db\n    cost = cost_func(x,y,w,b)\n    cost_arr.append(cost)\n    if(i%100==0 or i==n-1):\n      print(\"iteration:\",i+1,\"w:\", w,\"b:\", b, \"cost:\", cost)\n  return w, b, cost, cost_arr\n\n\n\n\n\nimport numpy as np\nx_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\ny_train = np.array([460, 232, 178])\nw = np.array([0,0,0,0])\nb = 0\na = 0.0000005\nn = 100000\n\n\nw,b,cost,cost_arr = grad_desc(x_train, y_train, w, b, a, n, derivative_func, cost_func)\n\niteration: 1 w: [2.413345e-01 5.580000e-04 1.830000e-04 6.034500e-03] b: 0.000145 cost: 2529.445907451492\niteration: 101 w: [ 0.2023515  0.0007205 -0.000998  -0.0021815] b: -0.00011984671618528004 cost: 695.9930856678557\niteration: 201 w: [ 0.2025335  0.0009705 -0.002148  -0.009355 ] b: -0.00035963602268744473 cost: 694.9289935685856\niteration: 301 w: [ 0.202715   0.0012205 -0.003298  -0.016518 ] b: -0.0005982983702994033 cost: 693.8710622039179\niteration: 401 w: [ 0.202896  0.001445 -0.004448 -0.023628] b: -0.0008358359588454846 cost: 692.8254981266464\niteration: 501 w: [ 0.2030755  0.001675  -0.005568  -0.0307165] b: -0.0010722555533169366 cost: 691.7882133976278\niteration: 601 w: [ 0.2032545  0.001925  -0.006668  -0.037777 ] b: -0.001307562678074441 cost: 690.7597559872951\niteration: 701 w: [ 0.203433   0.002175  -0.007768  -0.0447935] b: -0.001541761106294673 cost: 689.7419977161079\niteration: 801 w: [ 0.203611   0.002425  -0.008868  -0.0517985] b: -0.0017748583098112362 cost: 688.7303041602801\niteration: 901 w: [ 0.203787  0.002675 -0.009968 -0.05875 ] b: -0.002006856060060288 cost: 687.7305146449748\niteration: 1001 w: [ 0.203963   0.002925  -0.011068  -0.0656845] b: -0.0022377620291096846 cost: 686.7374556983464\niteration: 1101 w: [ 0.204138  0.003175 -0.012168 -0.072588] b: -0.0024675777448512647 cost: 685.7530337759337\niteration: 1201 w: [ 0.204312   0.003425  -0.013268  -0.0794485] b: -0.0026963111772917827 cost: 684.7788423460347\niteration: 1301 w: [ 0.204486  0.003675 -0.014368 -0.086298] b: -0.0029239663570966323 cost: 683.810415310279\niteration: 1401 w: [ 0.2046585  0.0039135 -0.015468  -0.0930935] b: -0.0031505485097711224 cost: 682.8536764534834\niteration: 1501 w: [ 0.20483    0.0041135 -0.016568  -0.0998695] b: -0.0033760631581245247 cost: 681.9040178588283\niteration: 1601 w: [ 0.205002   0.004328  -0.0176535 -0.106625 ] b: -0.0036005085376852666 cost: 680.9615129263806\niteration: 1701 w: [ 0.205172   0.004578  -0.0187035 -0.1133365] b: -0.003823897765948446 cost: 680.0296293812801\niteration: 1801 w: [ 0.2053415  0.004828  -0.0197535 -0.120036 ] b: -0.004046235485751484 cost: 679.1034286909584\niteration: 1901 w: [ 0.205511   0.005078  -0.0208035 -0.1266875] b: -0.004267518384910261 cost: 678.187686912896\niteration: 2001 w: [ 0.205679   0.005328  -0.0218535 -0.133312 ] b: -0.004487759825311172 cost: 677.2795368977503\niteration: 2101 w: [ 0.2058465  0.005596  -0.0229035 -0.1399215] b: -0.004706960723743415 cost: 676.3772234638445\niteration: 2201 w: [ 0.206013   0.005896  -0.0239535 -0.1464775] b: -0.004925125624179499 cost: 675.4856970809177\niteration: 2301 w: [ 0.206179   0.006196  -0.0250035 -0.153025 ] b: -0.005142256578274909 cost: 674.5991745576366\niteration: 2401 w: [ 0.206344   0.006496  -0.0260535 -0.15954  ] b: -0.005358360241940179 cost: 673.7207726269572\niteration: 2501 w: [ 0.2065085  0.0067745 -0.027125  -0.1660135] b: -0.0055734435777244845 cost: 672.8512633960598\niteration: 2601 w: [ 0.206672   0.0070245 -0.028225  -0.1724735] b: -0.0057875101964774515 cost: 671.9868482788764\niteration: 2701 w: [ 0.206835   0.0072745 -0.029325  -0.1788865] b: -0.006000561348086028 cost: 671.1322623266645\niteration: 2801 w: [ 0.2069975  0.0075245 -0.030425  -0.185284 ] b: -0.006212606472340106 cost: 670.2833886731445\niteration: 2901 w: [ 0.207159   0.0077745 -0.031525  -0.191667 ] b: -0.00642364451026305 cost: 669.4400745262191\niteration: 3001 w: [ 0.2073195  0.0080245 -0.032625  -0.197989 ] b: -0.0066336814964713135 cost: 668.6081996289139\niteration: 3101 w: [ 0.207479   0.0082745 -0.033725  -0.2042985] b: -0.006842726206046504 cost: 667.7815296096983\niteration: 3201 w: [ 0.207639   0.0085245 -0.034825  -0.2105845] b: -0.007050775211079575 cost: 666.9614207743449\niteration: 3301 w: [ 0.2077975  0.0087745 -0.035925  -0.2168315] b: -0.007257836447118926 cost: 666.1497838167132\niteration: 3401 w: [ 0.207956   0.0090245 -0.037025  -0.223065 ] b: -0.007463916293745738 cost: 665.3433667135956\niteration: 3501 w: [ 0.2081135  0.0092745 -0.038125  -0.229261 ] b: -0.0076690160776340536 cost: 664.5451307273105\niteration: 3601 w: [ 0.2082695  0.0095245 -0.039225  -0.2354185] b: -0.007873146832570208 cost: 663.7551382261652\niteration: 3701 w: [ 0.2084255  0.0097745 -0.040325  -0.2415725] b: -0.008076305656634936 cost: 662.9690130199062\niteration: 3801 w: [ 0.208581   0.0100245 -0.041425  -0.2476855] b: -0.008278497331775565 cost: 662.1913455782793\niteration: 3901 w: [ 0.208735   0.010317  -0.0424825 -0.2537675] b: -0.008479729273751815 cost: 661.4215053114478\niteration: 4001 w: [ 0.2088885  0.010617  -0.0435325 -0.259838 ] b: -0.008680002444220802 cost: 660.6565300223458\niteration: 4101 w: [ 0.209042   0.010917  -0.0445825 -0.265858 ] b: -0.008879322789329899 cost: 659.9009983958877\niteration: 4201 w: [ 0.209194   0.011217  -0.0456325 -0.271867 ] b: -0.009077697385785134 cost: 659.1500791236687\niteration: 4301 w: [ 0.209346   0.011517  -0.0466825 -0.2778635] b: -0.009275126539940259 cost: 658.4039296265429\niteration: 4401 w: [ 0.209496   0.011817  -0.0477325 -0.283797 ] b: -0.00947161669159306 cost: 657.6685605755812\niteration: 4501 w: [ 0.2096465  0.012117  -0.0487825 -0.289721 ] b: -0.00966717154421513 cost: 656.9375086290565\niteration: 4601 w: [ 0.2097965  0.012417  -0.0498325 -0.2956425] b: -0.009861794300524621 cost: 656.2099347808465\niteration: 4701 w: [ 0.209945   0.012717  -0.0508825 -0.3015015] b: -0.010055491971623896 cost: 655.4929004964749\niteration: 4801 w: [ 0.2100935  0.013008  -0.0519325 -0.3073465] b: -0.010248264069244464 cost: 654.7806761471173\niteration: 4901 w: [ 0.2102415  0.013258  -0.0529825 -0.3131835] b: -0.010440117224497688 cost: 654.0727457961598\niteration: 5001 w: [ 0.210388   0.013508  -0.0540325 -0.3189695] b: -0.010631055309118353 cost: 653.373832197853\niteration: 5101 w: [ 0.2105345  0.013758  -0.0550825 -0.3247415] b: -0.010821084725986743 cost: 652.6795723436072\niteration: 5201 w: [ 0.21068    0.014008  -0.0561325 -0.3304895] b: -0.011010203536947682 cost: 651.9910995793263\niteration: 5301 w: [ 0.210825   0.0142615 -0.0571825 -0.336195 ] b: -0.011198420491851319 cost: 651.3104716657031\niteration: 5401 w: [ 0.21097    0.0145615 -0.0582325 -0.341889 ] b: -0.011385740109903942 cost: 650.6338118623238\niteration: 5501 w: [ 0.2111135  0.0148615 -0.0592825 -0.3475665] b: -0.011572162149229777 cost: 649.9619720741375\niteration: 5601 w: [ 0.2112565  0.0151615 -0.0603325 -0.353189 ] b: -0.011757693805196872 cost: 649.2992767667502\niteration: 5701 w: [ 0.211399   0.0154615 -0.0613825 -0.3588115] b: -0.011942339549135714 cost: 648.6394511780375\niteration: 5801 w: [ 0.211541   0.015804  -0.0624325 -0.36442  ] b: -0.012126100113712401 cost: 647.9837941514609\niteration: 5901 w: [ 0.211682   0.016154  -0.0634825 -0.3699655] b: -0.012308980410778101 cost: 647.3379762581632\niteration: 6001 w: [ 0.211822   0.016504  -0.0645325 -0.3755155] b: -0.012490987986764828 cost: 646.6944523563071\niteration: 6101 w: [ 0.2119625  0.016825  -0.0655825 -0.3810445] b: -0.012672123409456514 cost: 646.0562288339962\niteration: 6201 w: [ 0.2121005  0.017125  -0.0666325 -0.3865165] b: -0.012852392093039107 cost: 645.4271963544116\niteration: 6301 w: [ 0.212239   0.017425  -0.0676825 -0.391982 ] b: -0.013031797225555575 cost: 644.8015924971579\niteration: 6401 w: [ 0.212378   0.017725  -0.0687325 -0.39744  ] b: -0.013210340216809507 cost: 644.1795163791699\niteration: 6501 w: [ 0.212515   0.018025  -0.0697825 -0.402849 ] b: -0.013388026728848293 cost: 643.5654740729954\niteration: 6601 w: [ 0.212652   0.018325  -0.0708325 -0.4082395] b: -0.013564858977700602 cost: 642.9560887832505\niteration: 6701 w: [ 0.2127885  0.018625  -0.0718825 -0.4136275] b: -0.01374084259258523 cost: 642.3496126278868\niteration: 6801 w: [ 0.212924   0.018925  -0.0729325 -0.418967 ] b: -0.01391598386789509 cost: 641.750973972813\niteration: 6901 w: [ 0.2130585  0.019225  -0.0739825 -0.4242755] b: -0.014090284070279208 cost: 641.1582356076469\niteration: 7001 w: [ 0.213193   0.019525  -0.0750325 -0.4295835] b: -0.014263746409743042 cost: 640.568109942329\niteration: 7101 w: [ 0.213327   0.019825  -0.0760825 -0.4348655] b: -0.014436375252410026 cost: 639.9832945934037\niteration: 7201 w: [ 0.2134595  0.020125  -0.0771325 -0.4401045] b: -0.014608172944214843 cost: 639.4055458161864\niteration: 7301 w: [ 0.2135925  0.020425  -0.0781825 -0.445343 ] b: -0.01477914443959268 cost: 638.8303412585573\niteration: 7401 w: [ 0.213725   0.020712  -0.0792325 -0.450556 ] b: -0.014949292091960484 cost: 638.2603743385561\niteration: 7501 w: [ 0.213856   0.020962  -0.0802345 -0.455718 ] b: -0.015118622477034573 cost: 637.6994142844079\niteration: 7601 w: [ 0.213987   0.021212  -0.0812345 -0.460876 ] b: -0.015287140198373884 cost: 637.141328724567\niteration: 7701 w: [ 0.2141175  0.021462  -0.0822345 -0.4660195] b: -0.015454844998592595 cost: 636.5871528919143\niteration: 7801 w: [ 0.214247   0.021717  -0.0832295 -0.4711345] b: -0.015621740978801963 cost: 636.0383759096808\niteration: 7901 w: [ 0.214376   0.022017  -0.0841795 -0.4762155] b: -0.01578783423794423 cost: 635.4960978953343\niteration: 8001 w: [ 0.214504   0.022317  -0.0851295 -0.481276 ] b: -0.015953127487535804 cost: 634.9582480210778\niteration: 8101 w: [ 0.2146325  0.022617  -0.0860795 -0.4863355] b: -0.016117622136583992 cost: 634.4228215205202\niteration: 8201 w: [ 0.214759   0.022917  -0.0870295 -0.4913435] b: -0.01628132562817647 cost: 633.8949096425367\niteration: 8301 w: [ 0.214886   0.023217  -0.0879795 -0.4963405] b: -0.016444239940155186 cost: 633.3703751730842\niteration: 8401 w: [ 0.2150125  0.023517  -0.0889295 -0.501336 ] b: -0.016606366647562168 cost: 632.8482581924086\niteration: 8501 w: [ 0.2151375  0.023817  -0.0898795 -0.506274 ] b: -0.016767709530766145 cost: 632.3341237830863\niteration: 8601 w: [ 0.215262   0.024117  -0.0908295 -0.5111985] b: -0.016928273313438692 cost: 631.8235400798213\niteration: 8701 w: [ 0.215387   0.024417  -0.0917795 -0.516123 ] b: -0.01708806319070792 cost: 631.3151561998055\niteration: 8801 w: [ 0.215511   0.024713  -0.0927335 -0.521015 ] b: -0.01724707952602148 cost: 630.8121060432607\niteration: 8901 w: [ 0.2156345  0.024963  -0.0937335 -0.525869 ] b: -0.017405325612922583 cost: 630.3142816967135\niteration: 9001 w: [ 0.2157575  0.025213  -0.0947335 -0.5307105] b: -0.017562809157736888 cost: 629.8198151695871\niteration: 9101 w: [ 0.21588    0.025463  -0.0957335 -0.5355455] b: -0.017719526577670713 cost: 629.3281094657372\niteration: 9201 w: [ 0.216002   0.025713  -0.0967335 -0.5403505] b: -0.017875484390204293 cost: 628.8414195222083\niteration: 9301 w: [ 0.2161225  0.025963  -0.0977335 -0.545109 ] b: -0.018030688326887326 cost: 628.3612912918138\niteration: 9401 w: [ 0.2162435  0.026213  -0.0987335 -0.5498765] b: -0.01818514099529271 cost: 627.8823616070082\niteration: 9501 w: [ 0.2163645  0.026463  -0.0997335 -0.55463  ] b: -0.01833884113036277 cost: 627.406829126374\niteration: 9601 w: [ 0.2164835  0.026713  -0.1007335 -0.55933  ] b: -0.018491796348226376 cost: 626.9384182136115\niteration: 9701 w: [ 0.2166025  0.026963  -0.1017335 -0.5640205] b: -0.0186440142509367 cost: 626.4729114274149\niteration: 9801 w: [ 0.2167215  0.0272455 -0.1027335 -0.5686955] b: -0.018795488411498525 cost: 626.0106572418074\niteration: 9901 w: [ 0.2168395  0.0275455 -0.1037335 -0.5733615] b: -0.018946223421659778 cost: 625.5511251713053\niteration: 10001 w: [ 0.2169565  0.0278455 -0.1047335 -0.5779825] b: -0.019096231305416184 cost: 625.0977588586564\niteration: 10101 w: [ 0.217073   0.0281555 -0.1057335 -0.582588 ] b: -0.019245509266673105 cost: 624.6477093062289\niteration: 10201 w: [ 0.2171905  0.0285055 -0.1067335 -0.5871965] b: -0.019394061233485523 cost: 624.1990628401919\niteration: 10301 w: [ 0.2173055  0.0288555 -0.1077335 -0.591755 ] b: -0.019541889810656344 cost: 623.756940828035\niteration: 10401 w: [ 0.217421   0.0291565 -0.1087335 -0.596304 ] b: -0.019688998492371303 cost: 623.3178810248277\niteration: 10501 w: [ 0.2175355  0.0294565 -0.1097335 -0.600827 ] b: -0.01983539125795989 cost: 622.883076942993\niteration: 10601 w: [ 0.2176505  0.0297565 -0.1107335 -0.60536  ] b: -0.019981068756584914 cost: 622.4492294594884\niteration: 10701 w: [ 0.217764   0.0300565 -0.1117335 -0.609837 ] b: -0.020126033527300156 cost: 622.0223009308603\niteration: 10801 w: [ 0.217877   0.0303565 -0.1127335 -0.614291 ] b: -0.020270294817821766 cost: 621.5992617284829\niteration: 10901 w: [ 0.21799    0.0306565 -0.1137335 -0.618751 ] b: -0.02041385061047308 cost: 621.1774937822787\niteration: 11001 w: [ 0.2181025  0.0309565 -0.1147335 -0.623193 ] b: -0.020556707946261358 cost: 620.7591386769445\niteration: 11101 w: [ 0.218214   0.0312565 -0.1157335 -0.627589 ] b: -0.02069886562388622 cost: 620.346653647179\niteration: 11201 w: [ 0.2183255  0.0315565 -0.1167335 -0.631963 ] b: -0.02084033257743231 cost: 619.9378677314106\niteration: 11301 w: [ 0.218436   0.0318565 -0.1177335 -0.6363535] b: -0.020981104941769094 cost: 619.5293800742135\niteration: 11401 w: [ 0.2185465  0.0321565 -0.1187335 -0.6407095] b: -0.02112118541533878 cost: 619.1256610414683\niteration: 11501 w: [ 0.218656   0.0324565 -0.1197335 -0.645023 ] b: -0.021260582653791514 cost: 618.7273677374055\niteration: 11601 w: [ 0.218765   0.0327565 -0.1207335 -0.6493335] b: -0.021399300515166655 cost: 618.3310315353523\niteration: 11701 w: [ 0.2188745  0.0330565 -0.1217335 -0.6536335] b: -0.021537336933569844 cost: 617.937292002899\niteration: 11801 w: [ 0.218983   0.0333565 -0.1227335 -0.657916 ] b: -0.021674693962655475 cost: 617.5467414903138\niteration: 11901 w: [ 0.2190905  0.0336565 -0.1237335 -0.6621515] b: -0.0218113813877972 cost: 617.1618783685964\niteration: 12001 w: [ 0.2191975  0.0339175 -0.1247335 -0.6663905] b: -0.021947397814013597 cost: 616.7785953545737\niteration: 12101 w: [ 0.2193045  0.0341675 -0.1257335 -0.6706005] b: -0.022082747048806018 cost: 616.3994721755062\niteration: 12201 w: [ 0.219412   0.0344175 -0.1267335 -0.6748215] b: -0.02221742695768332 cost: 616.0210374030172\niteration: 12301 w: [ 0.2195175  0.0346675 -0.1277335 -0.6789725] b: -0.02235145143549944 cost: 615.6501030944033\niteration: 12401 w: [ 0.2196225  0.0349175 -0.1287335 -0.6831335] b: -0.0224848178827133 cost: 615.2799070611549\niteration: 12501 w: [ 0.219728   0.0351675 -0.1297335 -0.6872765] b: -0.022617527643841392 cost: 614.9127850177091\niteration: 12601 w: [ 0.219833   0.0354175 -0.1307335 -0.6914245] b: -0.0227495818960631 cost: 614.5468166786753\niteration: 12701 w: [ 0.219937   0.0357035 -0.1316975 -0.695498 ] b: -0.022880989178960497 cost: 614.1890482651543\niteration: 12801 w: [ 0.2200405  0.0360035 -0.1326475 -0.69959  ] b: -0.023011749972109922 cost: 613.8314674589074\niteration: 12901 w: [ 0.2201435  0.0363035 -0.1335975 -0.703653 ] b: -0.023141865970558107 cost: 613.4777823447339\niteration: 13001 w: [ 0.220247   0.0366035 -0.1345475 -0.707723 ] b: -0.02327133873010081 cost: 613.1250323657525\niteration: 13101 w: [ 0.2203485  0.0369035 -0.1354975 -0.711743 ] b: -0.02340017468047057 cost: 612.7778432097031\niteration: 13201 w: [ 0.2204495  0.0372035 -0.1364475 -0.715743 ] b: -0.023528377632724386 cost: 612.4337408013147\niteration: 13301 w: [ 0.220551   0.0375035 -0.1373975 -0.719737 ] b: -0.023655947599212898 cost: 612.0915776312164\niteration: 13401 w: [ 0.220652   0.0378035 -0.1383475 -0.723737 ] b: -0.023782887371431118 cost: 611.7503919077939\niteration: 13501 w: [ 0.2207525  0.0381035 -0.1392975 -0.727682 ] b: -0.0239091993245801 cost: 611.4150384762629\niteration: 13601 w: [ 0.220852   0.0384035 -0.1402475 -0.731619 ] b: -0.02403488506694832 cost: 611.0817408338819\niteration: 13701 w: [ 0.2209515  0.0386875 -0.1411975 -0.735535 ] b: -0.024159953743669894 cost: 610.7516098278452\niteration: 13801 w: [ 0.2210505  0.0389375 -0.1421475 -0.739453 ] b: -0.024284399432392856 cost: 610.4229305640621\niteration: 13901 w: [ 0.2211495  0.0391875 -0.1430975 -0.7433555] b: -0.02440823061268634 cost: 610.0968620724638\niteration: 14001 w: [ 0.2212475  0.0394375 -0.1440475 -0.7472055] b: -0.024531448498309936 cost: 609.7762667281536\niteration: 14101 w: [ 0.221345   0.0396875 -0.1449975 -0.75105  ] b: -0.024654058484439773 cost: 609.4574515351217\niteration: 14201 w: [ 0.221443   0.0399375 -0.1459475 -0.7549   ] b: -0.024776058627795857 cost: 609.1395605427014\niteration: 14301 w: [ 0.2215395  0.0401875 -0.1468975 -0.7587235] b: -0.02489745463311686 cost: 608.8250590471938\niteration: 14401 w: [ 0.2216365  0.0404375 -0.1478475 -0.7625305] b: -0.025018246447289245 cost: 608.513150635224\niteration: 14501 w: [ 0.221732   0.0406875 -0.1487975 -0.7662865] b: -0.0251384419363099 cost: 608.2064466004118\niteration: 14601 w: [ 0.221828   0.0409375 -0.1497475 -0.7700675] b: -0.02525803949500909 cost: 607.899137813877\niteration: 14701 w: [ 0.2219235  0.0411875 -0.1506975 -0.773818 ] b: -0.025377043527862346 cost: 607.5954341258337\niteration: 14801 w: [ 0.222018   0.0414375 -0.1516475 -0.77757  ] b: -0.025495452376246567 cost: 607.2929046248208\niteration: 14901 w: [ 0.222112   0.0416875 -0.1525975 -0.78127  ] b: -0.025613273671796284 cost: 606.9955455849491\niteration: 15001 w: [ 0.222206   0.0419375 -0.1535475 -0.7849655] b: -0.025730508955738533 cost: 606.6997721135755\niteration: 15101 w: [ 0.222299   0.0421875 -0.1544975 -0.7886515] b: -0.02584716140403884 cost: 606.4059523084273\niteration: 15201 w: [ 0.2223935  0.0424375 -0.1554475 -0.7923395] b: -0.025963231464783777 cost: 606.113223033845\niteration: 15301 w: [ 0.222485   0.0427205 -0.1563645 -0.7959785] b: -0.026078719669841502 cost: 605.825774379996\niteration: 15401 w: [ 0.2225775  0.0430205 -0.1572645 -0.799608 ] b: -0.026193634847617844 cost: 605.5404515207609\niteration: 15501 w: [ 0.2226685  0.0433205 -0.1581645 -0.803218 ] b: -0.02630797824718566 cost: 605.2577528413842\niteration: 15601 w: [ 0.22276    0.0436465 -0.1590645 -0.8068355] b: -0.02642174994469171 cost: 604.975542649424\niteration: 15701 w: [ 0.2228515  0.0439465 -0.1599645 -0.8104355] b: -0.026534950684120408 cost: 604.6959461327008\niteration: 15801 w: [ 0.222942   0.0442465 -0.1608645 -0.813985 ] b: -0.02664758577218656 cost: 604.4211587364466\niteration: 15901 w: [ 0.223032   0.0445465 -0.1617645 -0.817535 ] b: -0.026759661095677168 cost: 604.1474876756603\niteration: 16001 w: [ 0.223121   0.0448465 -0.1626645 -0.82106  ] b: -0.02687117796326525 cost: 603.8767475121662\niteration: 16101 w: [ 0.223211   0.0451465 -0.1635645 -0.8246095] b: -0.02698213300753872 cost: 603.6054067327044\niteration: 16201 w: [ 0.2232995  0.0454465 -0.1644645 -0.828118 ] b: -0.02709253179619542 cost: 603.338109411341\niteration: 16301 w: [ 0.2233885  0.0457465 -0.1653645 -0.831605 ] b: -0.02720237706930822 cost: 603.0734448329539\niteration: 16401 w: [ 0.2234755  0.0460465 -0.1662645 -0.8350605] b: -0.02731167200178289 cost: 602.8120955019917\niteration: 16501 w: [ 0.2235635  0.0463465 -0.1671645 -0.838531 ] b: -0.027420418152392365 cost: 602.5507931241274\niteration: 16601 w: [ 0.2236505  0.0466465 -0.1680645 -0.841981 ] b: -0.027528617355603375 cost: 602.2920132344569\niteration: 16701 w: [ 0.223738   0.0469465 -0.1689645 -0.845419 ] b: -0.02763626984870761 cost: 602.0351472733996\niteration: 16801 w: [ 0.2238245  0.0472465 -0.1698645 -0.848819 ] b: -0.02774338257091851 cost: 601.781971731565\niteration: 16901 w: [ 0.2239095  0.0475465 -0.1707645 -0.852201 ] b: -0.027849960070771652 cost: 601.5310860833888\niteration: 17001 w: [ 0.2239955  0.0478465 -0.1716645 -0.855594 ] b: -0.027956000095674418 cost: 601.280496791055\niteration: 17101 w: [ 0.2240815  0.0481465 -0.1725645 -0.858969 ] b: -0.028061503217908484 cost: 601.0321791382336\niteration: 17201 w: [ 0.2241665  0.0484465 -0.1734645 -0.862335 ] b: -0.02816647496249229 cost: 600.785512290176\niteration: 17301 w: [ 0.2242505  0.0487465 -0.1743645 -0.8656545] b: -0.028270915757273776 cost: 600.5430041094243\niteration: 17401 w: [ 0.2243345  0.0490465 -0.1752645 -0.8689625] b: -0.028374833813324698 cost: 600.302272665091\niteration: 17501 w: [ 0.2244185  0.0493465 -0.1761645 -0.872278 ] b: -0.028478226503474452 cost: 600.0620443034203\niteration: 17601 w: [ 0.224502   0.0496465 -0.1770645 -0.875578 ] b: -0.028581094737951146 cost: 599.8238493606705\niteration: 17701 w: [ 0.2245855  0.0499075 -0.1780035 -0.878873 ] b: -0.028683441628782586 cost: 599.5864780048678\niteration: 17801 w: [ 0.2246685  0.0501575 -0.1789535 -0.882123 ] b: -0.028785270033620674 cost: 599.3529134291119\niteration: 17901 w: [ 0.2247505  0.0504075 -0.1799035 -0.885349 ] b: -0.028886586852719803 cost: 599.1218874398852\niteration: 18001 w: [ 0.2248325  0.0506575 -0.1808535 -0.888599 ] b: -0.0289873872507459 cost: 598.8902552467946\niteration: 18101 w: [ 0.224914   0.0509075 -0.1818035 -0.891805 ] b: -0.029087676851767906 cost: 598.6624413867178\niteration: 18201 w: [ 0.224996   0.0511575 -0.1827535 -0.895052 ] b: -0.029187455423271568 cost: 598.4329274248081\niteration: 18301 w: [ 0.225077   0.0514075 -0.1837035 -0.898213 ] b: -0.029286726831094834 cost: 598.2098997223087\niteration: 18401 w: [ 0.225157   0.0516575 -0.1846535 -0.9013855] b: -0.029385497944038615 cost: 597.9870568240138\niteration: 18501 w: [ 0.2252375  0.0519075 -0.1856035 -0.90454  ] b: -0.029483766806903622 cost: 597.7662813098792\niteration: 18601 w: [ 0.2253175  0.0521575 -0.1865535 -0.90769  ] b: -0.029581535070724117 cost: 597.5467052503907\niteration: 18701 w: [ 0.225397   0.0524075 -0.1875035 -0.9108425] b: -0.029678803236851573 cost: 597.3278836386268\niteration: 18801 w: [ 0.2254765  0.0526575 -0.1884535 -0.913966 ] b: -0.029775573873902755 cost: 597.1117936514489\niteration: 18901 w: [ 0.2255545  0.0529525 -0.1893585 -0.917044 ] b: -0.029871854940054024 cost: 596.9000182722007\niteration: 19001 w: [ 0.2256335  0.0532525 -0.1902585 -0.920144 ] b: -0.02996764381481427 cost: 596.6878057834251\niteration: 19101 w: [ 0.2257105  0.0535525 -0.1911585 -0.923202 ] b: -0.030062947666483684 cost: 596.4790787932457\niteration: 19201 w: [ 0.2257895  0.0538525 -0.1920585 -0.926298 ] b: -0.030157760385747162 cost: 596.268863960318\niteration: 19301 w: [ 0.2258665  0.0541525 -0.1929585 -0.9293565] b: -0.03025208707109722 cost: 596.0618328563316\niteration: 19401 w: [ 0.2259435  0.0544525 -0.1938585 -0.932372 ] b: -0.030345931795073677 cost: 595.8582870482653\niteration: 19501 w: [ 0.22602    0.0547525 -0.1947585 -0.935387 ] b: -0.03043929869763318 cost: 595.6556073043978\niteration: 19601 w: [ 0.2260955  0.0550525 -0.1956585 -0.938387 ] b: -0.030532190708298445 cost: 595.4546716581052\niteration: 19701 w: [ 0.2261715  0.0553525 -0.1965585 -0.9413945] b: -0.030624605111184526 cost: 595.2541097034198\niteration: 19801 w: [ 0.226248   0.0556525 -0.1974585 -0.9443945] b: -0.03071654391543985 cost: 595.0548272177073\niteration: 19901 w: [ 0.226323   0.055948  -0.1983585 -0.947353 ] b: -0.030808008508034515 cost: 594.8588797489125\niteration: 20001 w: [ 0.2263975  0.056198  -0.1992585 -0.950303 ] b: -0.03089900821608077 cost: 594.6645143205441\niteration: 20101 w: [ 0.2264715  0.056448  -0.2001585 -0.953236 ] b: -0.030989540724734088 cost: 594.4719559722831\niteration: 20201 w: [ 0.226546   0.056698  -0.2010585 -0.9561575] b: -0.031079610285199125 cost: 594.2808641987467\niteration: 20301 w: [ 0.226621   0.056948  -0.2019585 -0.959099 ] b: -0.031169212585408033 cost: 594.089382321483\niteration: 20401 w: [ 0.2266945  0.057198  -0.2028585 -0.961999 ] b: -0.03125835336460141 cost: 593.9011220856247\niteration: 20501 w: [ 0.226768   0.057448  -0.2037585 -0.9648995] b: -0.031347034380156076 cost: 593.7136055893023\niteration: 20601 w: [ 0.2268405  0.057698  -0.2046585 -0.96775  ] b: -0.03143525829466008 cost: 593.5297588774491\niteration: 20701 w: [ 0.226913   0.057948  -0.2055585 -0.9706005] b: -0.031523030256172514 cost: 593.3466593044674\niteration: 20801 w: [ 0.2269855  0.058198  -0.2064585 -0.973461 ] b: -0.03161034795099629 cost: 593.1637328043008\niteration: 20901 w: [ 0.2270575  0.058448  -0.2073585 -0.976311 ] b: -0.03169721273547193 cost: 592.9821592767\niteration: 21001 w: [ 0.2271295  0.058698  -0.2082585 -0.9791445] b: -0.031783627973511856 cost: 592.8022709309206\niteration: 21101 w: [ 0.2272005  0.058948  -0.2091585 -0.981954 ] b: -0.03186959593254691 cost: 592.6244816666643\niteration: 21201 w: [ 0.227271   0.059198  -0.2100585 -0.9847385] b: -0.031955118891547736 cost: 592.4488285694205\niteration: 21301 w: [ 0.2273415  0.059448  -0.2109585 -0.9875095] b: -0.03204020318925374 cost: 592.2746461288074\niteration: 21401 w: [ 0.227413   0.059698  -0.2118585 -0.990308 ] b: -0.03212484466369454 cost: 592.0996322811992\niteration: 21501 w: [ 0.2274825  0.059948  -0.2127585 -0.993058 ] b: -0.032209045856998644 cost: 591.9280399144474\niteration: 21601 w: [ 0.227553   0.060198  -0.2136585 -0.9958385] b: -0.03229280796740813 cost: 591.7554532343294\niteration: 21701 w: [ 0.227622   0.060448  -0.2145585 -0.9985625] b: -0.03237613245359281 cost: 591.5866948102239\niteration: 21801 w: [ 0.227691   0.060698  -0.2154585 -1.0012625] b: -0.032459028825831876 cost: 591.4199363990265\niteration: 21901 w: [ 0.2277595  0.060948  -0.2163585 -1.003978 ] b: -0.03254149309225528 cost: 591.2530038709505\niteration: 22001 w: [ 0.2278285  0.061198  -0.2172585 -1.006678 ] b: -0.032623529803074224 cost: 591.0875922861928\niteration: 22101 w: [ 0.227897   0.061448  -0.2181585 -1.0093755] b: -0.032705134098729445 cost: 590.922988556733\niteration: 22201 w: [ 0.227965   0.061698  -0.2190585 -1.0120665] b: -0.032786313600360585 cost: 590.759406288281\niteration: 22301 w: [ 0.228033   0.061948  -0.2199585 -1.0147375] b: -0.03286706473063845 cost: 590.5975642687405\niteration: 22401 w: [ 0.228099   0.062198  -0.2208585 -1.0173525] b: -0.03294739682079655 cost: 590.4393748598166\niteration: 22501 w: [ 0.2281665  0.062428  -0.2217585 -1.019995 ] b: -0.03302731262759392 cost: 590.2804691124735\niteration: 22601 w: [ 0.228233   0.062678  -0.2226585 -1.0226305] b: -0.033106810102979764 cost: 590.1224610798586\niteration: 22701 w: [ 0.228299   0.062932  -0.2235585 -1.0252305] b: -0.033185890804676224 cost: 589.9669416345338\niteration: 22801 w: [ 0.228366   0.063232  -0.2244585 -1.0278745] b: -0.03326455330480152 cost: 589.8094626462408\niteration: 22901 w: [ 0.228432   0.063532  -0.2253585 -1.030469 ] b: -0.033342801129786825 cost: 589.6552155308925\niteration: 23001 w: [ 0.2284965  0.063832  -0.2262585 -1.033019 ] b: -0.03342064024019282 cost: 589.5039065495638\niteration: 23101 w: [ 0.2285615  0.064132  -0.2271585 -1.035591 ] b: -0.03349807166603075 cost: 589.3520580402729\niteration: 23201 w: [ 0.2286265  0.064432  -0.2280585 -1.038141 ] b: -0.03357509639770777 cost: 589.2019537850598\niteration: 23301 w: [ 0.2286915  0.064732  -0.2289585 -1.040691 ] b: -0.03365171702513824 cost: 589.0524502184331\niteration: 23401 w: [ 0.2287555  0.065032  -0.2298585 -1.043234 ] b: -0.03372793269256857 cost: 588.9039059229541\niteration: 23501 w: [ 0.22882    0.065332  -0.2307585 -1.045784 ] b: -0.03380374387021662 cost: 588.7556020886944\niteration: 23601 w: [ 0.2288835  0.065632  -0.2316585 -1.048277 ] b: -0.03387915359030647 cost: 588.6107860098186\niteration: 23701 w: [ 0.2289465  0.065932  -0.2325585 -1.0507635] b: -0.03395416925930713 cost: 588.4668727039144\niteration: 23801 w: [ 0.2290095  0.066232  -0.2334585 -1.0532635] b: -0.034028788166007955 cost: 588.3228533495923\niteration: 23901 w: [ 0.229072   0.066532  -0.2343585 -1.0557175] b: -0.034103011966940044 cost: 588.1817101748717\niteration: 24001 w: [ 0.2291345  0.066832  -0.2352585 -1.058194 ] b: -0.03417684460499529 cost: 588.0400048215535\niteration: 24101 w: [ 0.2291975  0.067132  -0.2361585 -1.0606755] b: -0.03425027975225192 cost: 587.8986178886387\niteration: 24201 w: [ 0.2292595  0.067432  -0.2370585 -1.0631255] b: -0.03432332467844226 cost: 587.7593542571486\niteration: 24301 w: [ 0.229321   0.067732  -0.2379585 -1.065549 ] b: -0.03439598191029202 cost: 587.6219467202673\niteration: 24401 w: [ 0.229382   0.068032  -0.2388585 -1.067961 ] b: -0.03446825314321018 cost: 587.4856451352441\niteration: 24501 w: [ 0.229443   0.068332  -0.2397585 -1.070361 ] b: -0.03454014753791199 cost: 587.3504653415265\niteration: 24601 w: [ 0.2295035  0.068632  -0.2406585 -1.072761 ] b: -0.03461165826637223 cost: 587.215819525676\niteration: 24701 w: [ 0.2295645  0.068932  -0.2415585 -1.075161 ] b: -0.03468278609644376 cost: 587.0817057127721\niteration: 24801 w: [ 0.2296255  0.069232  -0.2424585 -1.077561 ] b: -0.03475352957996196 cost: 586.9481249879263\niteration: 24901 w: [ 0.2296855  0.069532  -0.2433585 -1.079932 ] b: -0.034823891889103224 cost: 586.816464124715\niteration: 25001 w: [ 0.2297445  0.069832  -0.2442585 -1.082273 ] b: -0.03489388267399602 cost: 586.6867521192315\niteration: 25101 w: [ 0.2298045  0.070132  -0.2451585 -1.084623 ] b: -0.03496349871261061 cost: 586.5571197797434\niteration: 25201 w: [ 0.229863   0.070432  -0.2460585 -1.0869325] b: -0.0350327430143417 cost: 586.4299103100594\niteration: 25301 w: [ 0.229922   0.070704  -0.2469585 -1.0892605] b: -0.03510162088340405 cost: 586.3024909515779\niteration: 25401 w: [ 0.2299815  0.070954  -0.2478585 -1.0916055] b: -0.035170124101316244 cost: 586.1749080756\niteration: 25501 w: [ 0.23004    0.071204  -0.2487585 -1.0939055] b: -0.035238260838501914 cost: 586.0499285070129\niteration: 25601 w: [ 0.2300985  0.071454  -0.2496585 -1.096214 ] b: -0.03530603098095956 cost: 585.9250456315397\niteration: 25701 w: [ 0.230157   0.071704  -0.2505585 -1.0984995] b: -0.035373432904286434 cost: 585.801716711953\niteration: 25801 w: [ 0.2302135  0.071954  -0.2514585 -1.1007495] b: -0.03544047941469967 cost: 585.6805035609458\niteration: 25901 w: [ 0.230271   0.072204  -0.2523585 -1.1029995] b: -0.03550716566380518 cost: 585.5597585295052\niteration: 26001 w: [ 0.2303285  0.072454  -0.2532585 -1.105257 ] b: -0.035573490160500036 cost: 585.4391421194335\niteration: 26101 w: [ 0.230385   0.072704  -0.2541585 -1.107507 ] b: -0.035639455878560135 cost: 585.3193393114123\niteration: 26201 w: [ 0.2304425  0.072954  -0.2550585 -1.109757 ] b: -0.03570506138918167 cost: 585.2000043863312\niteration: 26301 w: [ 0.2304985  0.073204  -0.2559585 -1.111981 ] b: -0.03577031283924514 cost: 585.0823081462393\niteration: 26401 w: [ 0.2305555  0.073454  -0.2568585 -1.114202 ] b: -0.035835206768040535 cost: 584.9652029548472\niteration: 26501 w: [ 0.230611   0.073704  -0.2577585 -1.116402 ] b: -0.035899750630749204 cost: 584.8494917920449\niteration: 26601 w: [ 0.230666   0.073954  -0.2586585 -1.1185575] b: -0.035963951829464914 cost: 584.736200649114\niteration: 26701 w: [ 0.2307215  0.074204  -0.2595585 -1.120743 ] b: -0.03602780596903525 cost: 584.6220183015874\niteration: 26801 w: [ 0.230777   0.074454  -0.2604585 -1.122943 ] b: -0.036091307838234124 cost: 584.50764343916\niteration: 26901 w: [ 0.230832   0.074704  -0.2613585 -1.1250935] b: -0.03615446516129961 cost: 584.3958804264032\niteration: 27001 w: [ 0.2308865  0.074954  -0.2622585 -1.127249 ] b: -0.036217276007025186 cost: 584.2843306559529\niteration: 27101 w: [ 0.230942   0.075204  -0.2631575 -1.129444 ] b: -0.036279741540073546 cost: 584.1715191658969\niteration: 27201 w: [ 0.2309955  0.075454  -0.2640075 -1.131544 ] b: -0.03634186722265422 cost: 584.0641202463933\niteration: 27301 w: [ 0.231049   0.075704  -0.2648575 -1.133644 ] b: -0.03640365750762085 cost: 583.9571307072051\niteration: 27401 w: [ 0.2311025  0.0759575 -0.265704  -1.1357625] b: -0.03646511194130591 cost: 583.8498018988405\niteration: 27501 w: [ 0.231156   0.0762575 -0.266504  -1.1378625] b: -0.036526229855296806 cost: 583.7442272701054\niteration: 27601 w: [ 0.231209   0.0765575 -0.267304  -1.1399625] b: -0.03658701315868626 cost: 583.6390618390426\niteration: 27701 w: [ 0.231262   0.0768575 -0.268104  -1.142059 ] b: -0.03664746317113464 cost: 583.5344523286376\niteration: 27801 w: [ 0.2313145  0.0771575 -0.268904  -1.144146 ] b: -0.03670758274728636 cost: 583.4306493396557\niteration: 27901 w: [ 0.231368   0.0774575 -0.269704  -1.1462385] b: -0.03676736434747774 cost: 583.327018319911\niteration: 28001 w: [ 0.2314195  0.0777575 -0.270504  -1.148287 ] b: -0.03682681910070681 cost: 583.2256253025431\niteration: 28101 w: [ 0.23147    0.0780575 -0.271304  -1.150287 ] b: -0.03688595444246288 cost: 583.1266304988022\niteration: 28201 w: [ 0.231522   0.0783575 -0.272104  -1.1523345] b: -0.03694476554783443 cost: 583.0260468945207\niteration: 28301 w: [ 0.231574   0.0786575 -0.272904  -1.1543845] b: -0.037003248744037834 cost: 582.9257494374963\niteration: 28401 w: [ 0.2316245  0.0789575 -0.273704  -1.156388 ] b: -0.03706140981977542 cost: 582.8277429568899\niteration: 28501 w: [ 0.2316755  0.0792525 -0.274504  -1.158393 ] b: -0.03711925348814608 cost: 582.7300763452338\niteration: 28601 w: [ 0.2317275  0.0795025 -0.275304  -1.160443 ] b: -0.03717677158621253 cost: 582.6312222039475\niteration: 28701 w: [ 0.231778   0.0797525 -0.276104  -1.1624535] b: -0.03723396624406065 cost: 582.5343513982098\niteration: 28801 w: [ 0.2318275  0.0800025 -0.276904  -1.1644035] b: -0.037290847107994476 cost: 582.4402837453388\niteration: 28901 w: [ 0.2318775  0.0802525 -0.277704  -1.1663635] b: -0.03734741736588498 cost: 582.3461689874248\niteration: 29001 w: [ 0.231928   0.0805025 -0.278504  -1.168334 ] b: -0.03740367254773504 cost: 582.2519930753316\niteration: 29101 w: [ 0.231977   0.0807525 -0.279304  -1.170284 ] b: -0.03745961774146073 cost: 582.1589906539273\niteration: 29201 w: [ 0.2320265  0.0810025 -0.280104  -1.172234 ] b: -0.03751524991403125 cost: 582.0663409135265\niteration: 29301 w: [ 0.2320755  0.0812525 -0.280904  -1.174177 ] b: -0.037570570510696205 cost: 581.9743196583163\niteration: 29401 w: [ 0.2321255  0.0815025 -0.281704  -1.176127 ] b: -0.03762558376528412 cost: 581.8823742526064\niteration: 29501 w: [ 0.2321745  0.0817525 -0.282504  -1.178077 ] b: -0.03768028659555024 cost: 581.790783140823\niteration: 29601 w: [ 0.2322235  0.0820025 -0.283304  -1.179993 ] b: -0.03773467896340583 cost: 581.7008612963946\niteration: 29701 w: [ 0.2322705  0.0822525 -0.284104  -1.1818505] b: -0.037788772464048104 cost: 581.6135375926061\niteration: 29801 w: [ 0.2323185  0.0825025 -0.284904  -1.1837505] b: -0.03784256607677312 cost: 581.524902996663\niteration: 29901 w: [ 0.2323665  0.0827525 -0.285704  -1.1856505] b: -0.03789605642095031 cost: 581.4366040876556\niteration: 30001 w: [ 0.232414   0.0830025 -0.286504  -1.1875125] b: -0.03794924505114637 cost: 581.3500862598542\niteration: 30101 w: [ 0.232461   0.0832525 -0.287304  -1.189364 ] b: -0.038002138879864575 cost: 581.2642892822115\niteration: 30201 w: [ 0.232509   0.0835025 -0.288104  -1.191264 ] b: -0.038054734023587715 cost: 581.1769821691576\niteration: 30301 w: [ 0.232557   0.0837525 -0.288904  -1.193145 ] b: -0.03810702570206265 cost: 581.0907234453425\niteration: 30401 w: [ 0.2326035  0.0840025 -0.289704  -1.194995 ] b: -0.038159019239984805 cost: 581.0059534748533\niteration: 30501 w: [ 0.23265    0.0842525 -0.290504  -1.196802 ] b: -0.038210721202811014 cost: 580.9231010096198\niteration: 30601 w: [ 0.232696   0.0845025 -0.291304  -1.1986445] b: -0.03826213457334974 cost: 580.8392390338167\niteration: 30701 w: [ 0.2327425  0.0847525 -0.292104  -1.2004475] b: -0.03831325491083526 cost: 580.7571483266508\niteration: 30801 w: [ 0.2327875  0.0850025 -0.292904  -1.2022475] b: -0.038364088279058836 cost: 580.6754721971766\niteration: 30901 w: [ 0.232833   0.0852525 -0.293704  -1.2040475] b: -0.03841463635969496 cost: 580.5940976853171\niteration: 31001 w: [ 0.232879   0.0855025 -0.294504  -1.205854 ] b: -0.03846489687908244 cost: 580.5127879100409\niteration: 31101 w: [ 0.232925   0.0857525 -0.295304  -1.207654 ] b: -0.03851486725696284 cost: 580.4320176154398\niteration: 31201 w: [ 0.2329705  0.0860025 -0.296104  -1.209454 ] b: -0.038564549111440846 cost: 580.3515498560345\niteration: 31301 w: [ 0.233016   0.0862525 -0.296904  -1.211254 ] b: -0.03861394464289773 cost: 580.271384083231\niteration: 31401 w: [ 0.23306    0.0865025 -0.297704  -1.212981 ] b: -0.03866306022889402 cost: 580.1941290797067\niteration: 31501 w: [ 0.2331045  0.0867525 -0.298504  -1.214731 ] b: -0.03871189940458871 cost: 580.1163341865199\niteration: 31601 w: [ 0.2331485  0.0870025 -0.299304  -1.216481 ] b: -0.03876045611532014 cost: 580.0388257038768\niteration: 31701 w: [ 0.2331925  0.0872525 -0.300104  -1.218215 ] b: -0.03880873314917972 cost: 579.9621667934034\niteration: 31801 w: [ 0.233236   0.0875025 -0.300904  -1.219915 ] b: -0.03885673724861454 cost: 579.8869820571664\niteration: 31901 w: [ 0.2332795  0.0877525 -0.301704  -1.221651 ] b: -0.0389044692318585 cost: 579.810810084755\niteration: 32001 w: [ 0.2333245  0.0880025 -0.302504  -1.223401 ] b: -0.038951917774882945 cost: 579.7344305934936\niteration: 32101 w: [ 0.233368   0.0882525 -0.303304  -1.225117 ] b: -0.038999089943017894 cost: 579.6595155814375\niteration: 32201 w: [ 0.233411   0.0885025 -0.304104  -1.226817 ] b: -0.03904599065257062 cost: 579.5854275156081\niteration: 32301 w: [ 0.233454   0.0887405 -0.304916  -1.228515 ] b: -0.03909262220892757 cost: 579.5115402500659\niteration: 32401 w: [ 0.233497   0.0889405 -0.305766  -1.230215 ] b: -0.03913898019763201 cost: 579.4374179785354\niteration: 32501 w: [ 0.2335395  0.0891405 -0.306616  -1.231867 ] b: -0.03918506809116004 cost: 579.3651987236226\niteration: 32601 w: [ 0.2335815  0.0893405 -0.307466  -1.233517 ] b: -0.03923089502544056 cost: 579.293303926633\niteration: 32701 w: [ 0.2336235  0.0895405 -0.308316  -1.235167 ] b: -0.039276454919974196 cost: 579.2216646028121\niteration: 32801 w: [ 0.233666   0.0897405 -0.309166  -1.236841 ] b: -0.039321748947745525 cost: 579.1494749983648\niteration: 32901 w: [ 0.233708   0.0899405 -0.310016  -1.238491 ] b: -0.03936677744506123 cost: 579.0783502171353\niteration: 33001 w: [ 0.23375    0.0901825 -0.310866  -1.240141 ] b: -0.03941154076535484 cost: 579.0072381661133\niteration: 33101 w: [ 0.2337915  0.0904325 -0.311716  -1.241791 ] b: -0.03945604130048666 cost: 578.9363361525034\niteration: 33201 w: [ 0.233833   0.0906825 -0.312566  -1.243424 ] b: -0.039500276418159834 cost: 578.8662496264814\niteration: 33301 w: [ 0.2338745  0.0909325 -0.313416  -1.245044 ] b: -0.039544253036813914 cost: 578.7968398683205\niteration: 33401 w: [ 0.233915   0.0912015 -0.314266  -1.246644 ] b: -0.03958797167206071 cost: 578.7282213140238\niteration: 33501 w: [ 0.2339555  0.0915015 -0.315116  -1.248244 ] b: -0.039631433847373415 cost: 578.6596645429348\niteration: 33601 w: [ 0.2339955  0.0918015 -0.315966  -1.2498235] b: -0.03967463920262613 cost: 578.5920126118914\niteration: 33701 w: [ 0.2340345  0.0921015 -0.316816  -1.251378 ] b: -0.03971759518180093 cost: 578.5254024140827\niteration: 33801 w: [ 0.2340755  0.0924015 -0.317666  -1.252978 ] b: -0.03976029560045821 cost: 578.4575577119253\niteration: 33901 w: [ 0.234116   0.0927015 -0.318516  -1.254578 ] b: -0.03980274014542608 cost: 578.3899545900872\niteration: 34001 w: [ 0.234156   0.0930015 -0.319366  -1.256156 ] b: -0.0398449291811528 cost: 578.3232921326702\niteration: 34101 w: [ 0.234195   0.0933015 -0.320216  -1.257706 ] b: -0.03988686953426681 cost: 578.2577512513948\niteration: 34201 w: [ 0.2342345  0.0936015 -0.321066  -1.2592665] b: -0.03992855968521551 cost: 578.192105442392\niteration: 34301 w: [ 0.2342745  0.0939015 -0.321916  -1.260846 ] b: -0.039969997145264925 cost: 578.1260926471763\niteration: 34401 w: [ 0.2343135  0.0942015 -0.322766  -1.262376 ] b: -0.0400111836306711 cost: 578.0618610106209\niteration: 34501 w: [ 0.234351   0.0945015 -0.323616  -1.263876 ] b: -0.04005213058034522 cost: 577.9987844177801\niteration: 34601 w: [ 0.234389   0.0948015 -0.324466  -1.265376 ] b: -0.040092835569364216 cost: 577.9359195552594\niteration: 34701 w: [ 0.2344275  0.0951015 -0.325316  -1.266885 ] b: -0.04013329949006035 cost: 577.8729892010455\niteration: 34801 w: [ 0.2344665  0.0954015 -0.326166  -1.2684235] b: -0.0401735179249774 cost: 577.8093686928104\niteration: 34901 w: [ 0.2345045  0.0957015 -0.327016  -1.2699235] b: -0.04021349173072119 cost: 577.7471477518251\niteration: 35001 w: [ 0.2345425  0.0960015 -0.327866  -1.2714235] b: -0.0402532243530567 cost: 577.6851392670904\niteration: 35101 w: [ 0.23458    0.0963015 -0.328716  -1.2729235] b: -0.04029271580407977 cost: 577.6233440632799\niteration: 35201 w: [ 0.234618   0.0966015 -0.329566  -1.2744265] b: -0.04033196703313952 cost: 577.561670194767\niteration: 35301 w: [ 0.2346565  0.0969015 -0.330416  -1.2759265] b: -0.040370974913990804 cost: 577.5002985706521\niteration: 35401 w: [ 0.2346935  0.0972015 -0.331266  -1.2773845] b: -0.04040974820161255 cost: 577.4403948883896\niteration: 35501 w: [ 0.23473    0.0975015 -0.332116  -1.2788345] b: -0.04044828782236295 cost: 577.3809305612007\niteration: 35601 w: [ 0.2347665  0.0978015 -0.332966  -1.2802845] b: -0.04048659269761744 cost: 577.3216652853081\niteration: 35701 w: [ 0.2348025  0.0981015 -0.333816  -1.281697 ] b: -0.04052466800032149 cost: 577.2637036243619\niteration: 35801 w: [ 0.234839   0.0984015 -0.334666  -1.2831375] b: -0.04056251456725318 cost: 577.2051096373219\niteration: 35901 w: [ 0.234876   0.0987015 -0.335516  -1.2845875] b: -0.04060012693596323 cost: 577.1464342581274\niteration: 36001 w: [ 0.2349125  0.0990015 -0.336366  -1.2860375] b: -0.040637507884454825 cost: 577.0879582969292\niteration: 36101 w: [ 0.2349485  0.0993015 -0.337216  -1.287469 ] b: -0.04067465497604283 cost: 577.0302170243261\niteration: 36201 w: [ 0.2349845  0.0996015 -0.338066  -1.288869 ] b: -0.04071157545788297 cost: 576.9735767128163\niteration: 36301 w: [ 0.2350205  0.0999015 -0.338916  -1.290291 ] b: -0.04074827073290848 cost: 576.9164916057211\niteration: 36401 w: [ 0.235057   0.1002015 -0.339766  -1.291741 ] b: -0.04078473294128609 cost: 576.8587984629943\niteration: 36501 w: [ 0.235093   0.1005015 -0.340616  -1.29315  ] b: -0.04082096442696895 cost: 576.8024695150779\niteration: 36601 w: [ 0.235127   0.1008015 -0.341466  -1.294503 ] b: -0.040856975545559504 cost: 576.7479141739208\niteration: 36701 w: [ 0.235161   0.1011015 -0.342316  -1.295853 ] b: -0.040892768210689545 cost: 576.693617432196\niteration: 36801 w: [ 0.2351955  0.1014015 -0.343166  -1.297203 ] b: -0.0409283433718738 cost: 576.6394934582967\niteration: 36901 w: [ 0.2352305  0.101653  -0.344016  -1.2986015] b: -0.04096369792004398 cost: 576.5844670090263\niteration: 37001 w: [ 0.2352655  0.101903  -0.344866  -1.2999845] b: -0.04099882822106821 cost: 576.5300651668858\niteration: 37101 w: [ 0.2353     0.102153  -0.345716  -1.3013345] b: -0.04103373899600653 cost: 576.4767575590865\niteration: 37201 w: [ 0.235335   0.102403  -0.346566  -1.3026845] b: -0.04106843126447955 cost: 576.4236229323824\niteration: 37301 w: [ 0.2353685  0.102653  -0.347416  -1.3040345] b: -0.041102906494701615 cost: 576.3706626628178\niteration: 37401 w: [ 0.2354035  0.102903  -0.348266  -1.305402 ] b: -0.041137162685250535 cost: 576.3173965234547\niteration: 37501 w: [ 0.235438   0.103153  -0.349116  -1.3067665] b: -0.04117119741448527 cost: 576.264390047907\niteration: 37601 w: [ 0.235472   0.103403  -0.349966  -1.3081055] b: -0.04120501502001705 cost: 576.2122501390596\niteration: 37701 w: [ 0.2355055  0.103653  -0.350816  -1.3094055] b: -0.04123862059829533 cost: 576.1613307696916\niteration: 37801 w: [ 0.235538   0.103903  -0.351666  -1.3107055] b: -0.04127201738045683 cost: 576.1105734746174\niteration: 37901 w: [ 0.235571   0.104153  -0.352516  -1.3120055] b: -0.041305203555341244 cost: 576.0599765863802\niteration: 38001 w: [ 0.235604  0.104403 -0.353366 -1.313289] b: -0.04133818302203708 cost: 576.0099787025753\niteration: 38101 w: [ 0.235637  0.104653 -0.354216 -1.314589] b: -0.04137095150264688 cost: 575.9597017287678\niteration: 38201 w: [ 0.23567   0.104903 -0.355066 -1.315889] b: -0.04140351122899681 cost: 575.9095857080565\niteration: 38301 w: [ 0.235703  0.105153 -0.355916 -1.317189] b: -0.0414358603899377 cost: 575.8596306389535\niteration: 38401 w: [ 0.235736  0.105403 -0.356766 -1.318484] b: -0.04146799908161879 cost: 575.8099669237896\niteration: 38501 w: [ 0.235768  0.105653 -0.357616 -1.319734] b: -0.04149993308611568 cost: 575.7616325257842\niteration: 38601 w: [ 0.2358005  0.105903  -0.358466  -1.3210175] b: -0.04153166516286352 cost: 575.7125806755608\niteration: 38701 w: [ 0.2358335  0.106153  -0.359316  -1.3223175] b: -0.04156318692053569 cost: 575.6632608230395\niteration: 38801 w: [ 0.2358665  0.106403  -0.360166  -1.323613 ] b: -0.04159449877467235 cost: 575.6142172033825\niteration: 38901 w: [ 0.235898  0.106653 -0.361016 -1.324855] b: -0.04162560638728071 cost: 575.5666988942295\niteration: 39001 w: [ 0.235928  0.106903 -0.361866 -1.326055] b: -0.041656520251694945 cost: 575.5203958072647\niteration: 39101 w: [ 0.235959  0.107153 -0.362716 -1.327255] b: -0.04168723964423964 cost: 575.4742294167376\niteration: 39201 w: [ 0.23599   0.107403 -0.363566 -1.328485] b: -0.0417177610879797 cost: 575.4274461198178\niteration: 39301 w: [ 0.2360215  0.107653  -0.364416  -1.329735 ] b: -0.04174807861468223 cost: 575.3803061381557\niteration: 39401 w: [ 0.2360535  0.107903  -0.365266  -1.330982 ] b: -0.04177819475450038 cost: 575.333389350813\niteration: 39501 w: [ 0.236084  0.108153 -0.366116 -1.332182] b: -0.0418081177183429 cost: 575.2877893616308\niteration: 39601 w: [ 0.236115  0.108403 -0.366966 -1.333382] b: -0.041837842252267825 cost: 575.2423270333826\niteration: 39701 w: [ 0.236145  0.108653 -0.367816 -1.334582] b: -0.04186737091647987 cost: 575.1970031746131\niteration: 39801 w: [ 0.2361755  0.108903  -0.368666  -1.3357985] b: -0.04189670787333269 cost: 575.1514131145881\niteration: 39901 w: [ 0.2362075  0.109153  -0.369516  -1.3370485] b: -0.041925844302299675 cost: 575.1051460350462\niteration: 40001 w: [ 0.2362385  0.109403  -0.370366  -1.3382535] b: -0.04195478432736597 cost: 575.0601211514692\niteration: 40101 w: [ 0.2362685  0.109653  -0.371216  -1.3394385] b: -0.041983530345845586 cost: 575.0157194179855\niteration: 40201 w: [ 0.236298   0.109903  -0.372066  -1.3405885] b: -0.04201208589575054 cost: 574.9722946301621\niteration: 40301 w: [ 0.236327   0.110153  -0.372916  -1.3417385] b: -0.042040457465445595 cost: 574.9289972297917\niteration: 40401 w: [ 0.236356  0.110403 -0.373766 -1.342889] b: -0.042068641775232674 cost: 574.885815128229\niteration: 40501 w: [ 0.236386  0.110653 -0.374616 -1.344063] b: -0.04209663862340553 cost: 574.8422014578688\niteration: 40601 w: [ 0.236415  0.110903 -0.375466 -1.345213] b: -0.042124446106123335 cost: 574.7992880765408\niteration: 40701 w: [ 0.2364445  0.111153  -0.376316  -1.346363 ] b: -0.042152066367081305 cost: 574.756501115953\niteration: 40801 w: [ 0.236474  0.111403 -0.377166 -1.347513] b: -0.04217950014430178 cost: 574.7138414023984\niteration: 40901 w: [ 0.236503  0.111653 -0.378016 -1.348663] b: -0.04220674817577341 cost: 574.6713090778348\niteration: 41001 w: [ 0.236532  0.111903 -0.378866 -1.349809] b: -0.0422338089291292 cost: 574.6289967207371\niteration: 41101 w: [ 0.2365605  0.112153  -0.379716  -1.3509265] b: -0.042260689109262006 cost: 574.5874693017602\niteration: 41201 w: [ 0.2365895  0.112403  -0.380566  -1.3520765] b: -0.04228738300700726 cost: 574.5453144606171\niteration: 41301 w: [ 0.236619   0.112653  -0.381416  -1.3532265] b: -0.04231389119597559 cost: 574.5032860694122\niteration: 41401 w: [ 0.236648   0.112903  -0.382266  -1.3543765] b: -0.04234021404981142 cost: 574.4613853337008\niteration: 41501 w: [ 0.236677  0.113153 -0.383116 -1.355483] b: -0.04236635242007891 cost: 574.4205985054763\niteration: 41601 w: [ 0.2367035  0.1134285 -0.3839405 -1.3565575] b: -0.04239231620439948 cost: 574.3809331566257\niteration: 41701 w: [ 0.23673    0.1137285 -0.3847405 -1.3576075] b: -0.04241810721118105 cost: 574.3421989955024\niteration: 41801 w: [ 0.2367575  0.1140285 -0.3855405 -1.3586855] b: -0.04244372624968999 cost: 574.3029434223896\niteration: 41901 w: [ 0.2367855  0.1143285 -0.3863405 -1.3597855] b: -0.04246916793194147 cost: 574.2633090579128\niteration: 42001 w: [ 0.236813   0.1146285 -0.3871405 -1.3608855] b: -0.04249443216381352 cost: 574.223791155423\niteration: 42101 w: [ 0.236841   0.1149285 -0.3879405 -1.3619855] b: -0.042519517132565306 cost: 574.1843891544167\niteration: 42201 w: [ 0.236868   0.1152285 -0.3887405 -1.3630495] b: -0.04254442545416703 cost: 574.1458955543695\niteration: 42301 w: [ 0.236894   0.1155285 -0.3895405 -1.3640995] b: -0.04256916521620936 cost: 574.1078181774484\niteration: 42401 w: [ 0.236921   0.1158285 -0.3903405 -1.3651495] b: -0.042593732836203727 cost: 574.0698461567857\niteration: 42501 w: [ 0.236948   0.1160995 -0.3911405 -1.3662285] b: -0.04261813000249716 cost: 574.0315164935558\niteration: 42601 w: [ 0.236976   0.1163495 -0.3919405 -1.3673285] b: -0.04264234802749705 cost: 573.9929640236277\niteration: 42701 w: [ 0.237004   0.1165995 -0.3927405 -1.3684285] b: -0.04266638980262582 cost: 573.9545276857469\niteration: 42801 w: [ 0.237031   0.1168495 -0.3935405 -1.3694835] b: -0.04269025604872505 cost: 573.9171708014238\niteration: 42901 w: [ 0.237057   0.1170995 -0.3943405 -1.370503 ] b: -0.04271395374938623 cost: 573.8806778818463\niteration: 43001 w: [ 0.237082   0.1173495 -0.3951405 -1.371503 ] b: -0.042737487833860124 cost: 573.844699437578\niteration: 43101 w: [ 0.237108   0.1175995 -0.3959405 -1.372503 ] b: -0.04276085952795905 cost: 573.8088172913805\niteration: 43201 w: [ 0.2371335  0.1178495 -0.3967405 -1.373518 ] b: -0.0427840687217941 cost: 573.7727164630452\niteration: 43301 w: [ 0.23716    0.1180995 -0.3975405 -1.374568 ] b: -0.042807108662038476 cost: 573.7359822518579\niteration: 43401 w: [ 0.237186   0.1183495 -0.3983405 -1.3755805] b: -0.04282998068979318 cost: 573.7001357958096\niteration: 43501 w: [ 0.2372105  0.1185995 -0.3991405 -1.3765805] b: -0.042852690359312814 cost: 573.6646487781856\niteration: 43601 w: [ 0.237236   0.1188495 -0.3999405 -1.3775805] b: -0.04287523804404365 cost: 573.6292573797967\niteration: 43701 w: [ 0.237262   0.1190995 -0.4007405 -1.3785805] b: -0.04289762083747462 cost: 573.5939622494903\niteration: 43801 w: [ 0.237287   0.1193495 -0.4015405 -1.3795805] b: -0.04291984494137986 cost: 573.5587648026735\niteration: 43901 w: [ 0.2373125  0.1195995 -0.4023405 -1.3805805] b: -0.04294190198420261 cost: 573.5236636218713\niteration: 44001 w: [ 0.2373385  0.1198495 -0.4031405 -1.381598 ] b: -0.04296379505920361 cost: 573.4883040313749\niteration: 44101 w: [ 0.237364   0.1200995 -0.4039405 -1.382598 ] b: -0.04298552599591126 cost: 573.4533979540172\niteration: 44201 w: [ 0.237389   0.1203495 -0.4047405 -1.383598 ] b: -0.04300709426809053 cost: 573.418588964991\niteration: 44301 w: [ 0.2374145  0.1205995 -0.4055405 -1.384598 ] b: -0.0430284987909089 cost: 573.3838762225417\niteration: 44401 w: [ 0.237439   0.1208495 -0.4063405 -1.3855765] b: -0.043049742568824316 cost: 573.3496891408569\niteration: 44501 w: [ 0.2374635  0.1210995 -0.4071405 -1.3865265] b: -0.043070830870778606 cost: 573.3161590382939\niteration: 44601 w: [ 0.2374875  0.1213495 -0.4079405 -1.3874685] b: -0.043091764106472634 cost: 573.2828750504999\niteration: 44701 w: [ 0.2375105  0.1215995 -0.4087405 -1.388377 ] b: -0.04311254849486624 cost: 573.2503361312698\niteration: 44801 w: [ 0.237535   0.1218495 -0.4095405 -1.389327 ] b: -0.0431331810381253 cost: 573.2170650067814\niteration: 44901 w: [ 0.237559   0.1220995 -0.4103405 -1.390277 ] b: -0.043153660252572196 cost: 573.1838817771294\niteration: 45001 w: [ 0.2375825  0.1223495 -0.4111405 -1.391227 ] b: -0.04317398468854931 cost: 573.1507868905456\niteration: 45101 w: [ 0.237607   0.1225995 -0.4119405 -1.392177 ] b: -0.043194153625139675 cost: 573.1177784209367\niteration: 45201 w: [ 0.237631   0.1228495 -0.4127405 -1.393127 ] b: -0.0432141645198454 cost: 573.0848582926463\niteration: 45301 w: [ 0.2376545  0.1230995 -0.4135405 -1.394052 ] b: -0.04323402453976285 cost: 573.0525040747171\niteration: 45401 w: [ 0.2376775  0.1233495 -0.4143405 -1.394952 ] b: -0.04325373872034051 cost: 573.0207093344746\niteration: 45501 w: [ 0.2377005  0.1235995 -0.4151405 -1.395861 ] b: -0.043273303211227625 cost: 572.988823184883\niteration: 45601 w: [ 0.2377245  0.1238495 -0.4159405 -1.396811 ] b: -0.04329271654528412 cost: 572.9562434849137\niteration: 45701 w: [ 0.2377485  0.1240995 -0.4167405 -1.397761 ] b: -0.043311973696829716 cost: 572.9237514765476\niteration: 45801 w: [ 0.2377735  0.1243495 -0.4175405 -1.398711 ] b: -0.043331079045608255 cost: 572.891346316377\niteration: 45901 w: [ 0.237797   0.1245995 -0.4183405 -1.399661 ] b: -0.043350028227333595 cost: 572.859029658229\niteration: 46001 w: [ 0.23782    0.1248495 -0.4191405 -1.4005535] b: -0.04336882459814166 cost: 572.8278657280372\niteration: 46101 w: [ 0.2378415  0.1250995 -0.4199405 -1.4014035] b: -0.04338748200757183 cost: 572.797564157023\niteration: 46201 w: [ 0.2378625  0.1253495 -0.4207405 -1.4022535] b: -0.04340600146954301 cost: 572.767334223718\niteration: 46301 w: [ 0.237885   0.1255995 -0.4215405 -1.4031035] b: -0.043424378619056715 cost: 572.7371736295078\niteration: 46401 w: [ 0.237907   0.1258495 -0.4223405 -1.403997 ] b: -0.043442615939793455 cost: 572.706292852243\niteration: 46501 w: [ 0.23793    0.1260995 -0.4231405 -1.404897 ] b: -0.04346070460299656 cost: 572.6753715643873\niteration: 46601 w: [ 0.237953   0.1263495 -0.4239405 -1.405797 ] b: -0.043478646795048044 cost: 572.6445293814484\niteration: 46701 w: [ 0.2379755  0.1265995 -0.4247405 -1.406663 ] b: -0.04349644417190212 cost: 572.6143770761721\niteration: 46801 w: [ 0.2379965  0.1268495 -0.4255405 -1.407513 ] b: -0.04351410419364293 cost: 572.5845847100355\niteration: 46901 w: [ 0.238018   0.1270995 -0.4263405 -1.408363 ] b: -0.04353162267454058 cost: 572.5548631748914\niteration: 47001 w: [ 0.23804    0.1273495 -0.4271405 -1.409213 ] b: -0.04354900290062445 cost: 572.5252121354614\niteration: 47101 w: [ 0.2380615  0.1275995 -0.4279405 -1.410063 ] b: -0.043566245243081 cost: 572.4956323522135\niteration: 47201 w: [ 0.2380835  0.1278495 -0.4287405 -1.410928 ] b: -0.04358334701491288 cost: 572.4658598890682\niteration: 47301 w: [ 0.2381065  0.1280995 -0.4295405 -1.411828 ] b: -0.043600302970251276 cost: 572.4355486724738\niteration: 47401 w: [ 0.2381285  0.1283495 -0.4303405 -1.412707 ] b: -0.04361711456308684 cost: 572.4056821494374\niteration: 47501 w: [ 0.23815    0.1285995 -0.4311405 -1.413557 ] b: -0.04363378394255007 cost: 572.3763934986426\niteration: 47601 w: [ 0.238172   0.1288495 -0.4319405 -1.414407 ] b: -0.043650315473948806 cost: 572.3471754141245\niteration: 47701 w: [ 0.2381935  0.1290995 -0.4327405 -1.415254 ] b: -0.04366670694392344 cost: 572.318079981069\niteration: 47801 w: [ 0.2382135  0.1293495 -0.4335405 -1.4160545] b: -0.04368296499597868 cost: 572.2898498190917\niteration: 47901 w: [ 0.238234   0.1295995 -0.4343405 -1.4168545] b: -0.04369909069808017 cost: 572.2616911211487\niteration: 48001 w: [ 0.238254   0.1298495 -0.4351405 -1.4176545] b: -0.04371508640276859 cost: 572.233596067335\niteration: 48101 w: [ 0.238275   0.1300995 -0.4359405 -1.418478 ] b: -0.04373094847758602 cost: 572.2051674540347\niteration: 48201 w: [ 0.238296   0.1303495 -0.4367405 -1.4192845] b: -0.04374667758125656 cost: 572.1770908405823\niteration: 48301 w: [ 0.238316   0.1305995 -0.4375405 -1.4200845] b: -0.04376227470442577 cost: 572.1491874302409\niteration: 48401 w: [ 0.238336   0.1308495 -0.4383405 -1.4208845] b: -0.043777738942043114 cost: 572.1213474737636\niteration: 48501 w: [ 0.238357   0.1310995 -0.4391405 -1.4216845] b: -0.04379307066504745 cost: 572.0935698922207\niteration: 48601 w: [ 0.238377   0.1313495 -0.4399405 -1.4224845] b: -0.043808273158978506 cost: 572.0658560601026\niteration: 48701 w: [ 0.2383975  0.1315995 -0.4407405 -1.4232845] b: -0.043823344244381245 cost: 572.0382052554959\niteration: 48801 w: [ 0.238417   0.1318495 -0.4415405 -1.4240845] b: -0.043838285385110934 cost: 572.0106187440057\niteration: 48901 w: [ 0.238438   0.1320995 -0.4423405 -1.42488  ] b: -0.04385309403304935 cost: 571.9831666008774\niteration: 49001 w: [ 0.238457   0.1323495 -0.4431405 -1.4256415] b: -0.043867776888403244 cost: 571.956328861757\niteration: 49101 w: [ 0.238478   0.1325995 -0.4439405 -1.4264415] b: -0.043882330073129965 cost: 571.9289274425673\niteration: 49201 w: [ 0.238498   0.1328495 -0.4447405 -1.4272415] b: -0.043896752974754163 cost: 571.9015894529904\niteration: 49301 w: [ 0.2385185  0.1330995 -0.4455405 -1.4280415] b: -0.043911045235483885 cost: 571.8743146421923\niteration: 49401 w: [ 0.2385385  0.1333495 -0.4463405 -1.4288415] b: -0.04392520212560996 cost: 571.8471031499669\niteration: 49501 w: [ 0.238559   0.1335995 -0.4471405 -1.4296415] b: -0.04393922802379701 cost: 571.8199546539037\niteration: 49601 w: [ 0.238578   0.1338495 -0.4479405 -1.430406 ] b: -0.043953126590839296 cost: 571.7934288668474\niteration: 49701 w: [ 0.238597   0.1340995 -0.4487405 -1.4311495] b: -0.04396690250434432 cost: 571.7672898843078\niteration: 49801 w: [ 0.238615   0.1343495 -0.4495405 -1.4318495] b: -0.04398056015238576 cost: 571.741885052648\niteration: 49901 w: [ 0.2386325  0.1345995 -0.4503405 -1.4325495] b: -0.043994098472817704 cost: 571.7165297840289\niteration: 50001 w: [ 0.2386515  0.1348205 -0.4511405 -1.4332785] b: -0.0440075227404304 cost: 571.6909376475869\niteration: 50101 w: [ 0.23867    0.1350205 -0.4519405 -1.4340285] b: -0.04402082510182937 cost: 571.6651943734959\niteration: 50201 w: [ 0.2386895  0.1352205 -0.4527405 -1.4347785] b: -0.04403400219790913 cost: 571.6395057232006\niteration: 50301 w: [ 0.2387085  0.1354205 -0.4535405 -1.4355285] b: -0.044047058042500566 cost: 571.6138736786957\niteration: 50401 w: [ 0.238728   0.1356205 -0.4543405 -1.4362785] b: -0.04405998754110906 cost: 571.5882968889058\niteration: 50501 w: [ 0.2387475  0.1358205 -0.4551405 -1.4370285] b: -0.04407279288595525 cost: 571.5627762223476\niteration: 50601 w: [ 0.238766   0.1360205 -0.4559405 -1.4377445] b: -0.044085477553486095 cost: 571.5378236616749\niteration: 50701 w: [ 0.2387835  0.1362205 -0.4567405 -1.4384445] b: -0.044098046452340896 cost: 571.5131622069822\niteration: 50801 w: [ 0.238801   0.1364205 -0.4575405 -1.4391445] b: -0.04411050086059633 cost: 571.4885502132248\niteration: 50901 w: [ 0.2388195  0.1366205 -0.4583405 -1.4398445] b: -0.04412283969101553 cost: 571.4639863819477\niteration: 51001 w: [ 0.2388375  0.1368205 -0.4591405 -1.4405675] b: -0.044135060898999365 cost: 571.4391320743304\niteration: 51101 w: [ 0.238857   0.1370205 -0.4599405 -1.4413175] b: -0.04414716053130088 cost: 571.4139316192799\niteration: 51201 w: [ 0.238876   0.1372205 -0.4607405 -1.4420675] b: -0.04415913495838772 cost: 571.3887874409287\niteration: 51301 w: [ 0.2388955  0.1374205 -0.4615405 -1.4428175] b: -0.0441709867368011 cost: 571.363698831815\niteration: 51401 w: [ 0.2389145  0.1376205 -0.4623405 -1.443562 ] b: -0.04418271572393476 cost: 571.3387461016308\niteration: 51501 w: [ 0.238932   0.137821  -0.46314   -1.4442615] b: -0.04419432737805757 cost: 571.3145042195908\niteration: 51601 w: [ 0.238949   0.138063  -0.463898  -1.4449195] b: -0.044205827799104404 cost: 571.2913476147653\niteration: 51701 w: [ 0.2389655  0.138313  -0.464648  -1.4455695] b: -0.04421721681972525 cost: 571.2684340461198\niteration: 51801 w: [ 0.2389815  0.138563  -0.465398  -1.4462195] b: -0.04422850434069259 cost: 571.2455633956558\niteration: 51901 w: [ 0.2389985  0.138813  -0.466148  -1.4468695] b: -0.044239678708783134 cost: 571.2227341420393\niteration: 52001 w: [ 0.2390145  0.139063  -0.466898  -1.4475195] b: -0.044250746851745126 cost: 571.1999482838279\niteration: 52101 w: [ 0.2390325  0.139313  -0.467648  -1.4482155] b: -0.04426170401705745 cost: 571.1765562495957\niteration: 52201 w: [ 0.2390505  0.139563  -0.468398  -1.4489155] b: -0.04427254465004354 cost: 571.1531566804923\niteration: 52301 w: [ 0.239068   0.139813  -0.469148  -1.4496155] b: -0.04428327161545885 cost: 571.1298060747328\niteration: 52401 w: [ 0.2390845  0.140063  -0.469898  -1.450274 ] b: -0.04429389017660648 cost: 571.1070804803992\niteration: 52501 w: [ 0.239101  0.140313 -0.470648 -1.450924] b: -0.04430439662860817 cost: 571.0845156320821\niteration: 52601 w: [ 0.239118  0.140563 -0.471398 -1.451574] b: -0.044314795067265496 cost: 571.0619929125285\niteration: 52701 w: [ 0.239134  0.140813 -0.472148 -1.452224] b: -0.04432508804827828 cost: 571.0395131527289\niteration: 52801 w: [ 0.239151  0.141063 -0.472898 -1.452874] b: -0.04433527193365774 cost: 571.0170752746512\niteration: 52901 w: [ 0.239167  0.141313 -0.473648 -1.453524] b: -0.04434534745751853 cost: 570.9946802925166\niteration: 53001 w: [ 0.2391835  0.141563  -0.474398  -1.454174 ] b: -0.04435531863289233 cost: 570.9723274671761\niteration: 53101 w: [ 0.2392    0.141813 -0.475148 -1.454825] b: -0.04436518053874599 cost: 570.9500036134561\niteration: 53201 w: [ 0.2392175  0.142063  -0.475898  -1.4555225] b: -0.04437493125918676 cost: 570.9270996986912\niteration: 53301 w: [ 0.239235  0.142313 -0.476648 -1.456201] b: -0.04438456818399991 cost: 570.9044968378925\niteration: 53401 w: [ 0.2392515  0.142563  -0.477398  -1.456851 ] b: -0.044394092257732165 cost: 570.8823183350595\niteration: 53501 w: [ 0.239268  0.142813 -0.478148 -1.457501] b: -0.04440351456080184 cost: 570.860182230946\niteration: 53601 w: [ 0.2392845  0.143063  -0.478898  -1.458151 ] b: -0.04441282671879517 cost: 570.8380885264514\niteration: 53701 w: [ 0.2393005  0.143313  -0.479648  -1.458793 ] b: -0.04442203232312543 cost: 570.8161421960624\niteration: 53801 w: [ 0.239316  0.143563 -0.480398 -1.459393] b: -0.04443113542144272 cost: 570.7947837673903\niteration: 53901 w: [ 0.239331  0.143813 -0.481148 -1.459993] b: -0.04444013876439281 cost: 570.7734623473094\niteration: 54001 w: [ 0.2393465  0.144063  -0.481898  -1.460593 ] b: -0.04444904417862618 cost: 570.7521769537893\niteration: 54101 w: [ 0.2393615  0.144313  -0.482648  -1.461193 ] b: -0.044457845839779735 cost: 570.7309284984201\niteration: 54201 w: [ 0.239377  0.144563 -0.483398 -1.461793] b: -0.04446655213257856 cost: 570.7097161381254\niteration: 54301 w: [ 0.239392  0.144813 -0.484148 -1.462391] b: -0.044475154939276416 cost: 570.6885660860677\niteration: 54401 w: [ 0.2394075  0.145063  -0.484898  -1.462991 ] b: -0.04448366212289125 cost: 570.6674266770738\niteration: 54501 w: [ 0.239422  0.145313 -0.485648 -1.463591] b: -0.044492065937663396 cost: 570.646324651441\niteration: 54601 w: [ 0.2394375  0.145563  -0.486398  -1.464191 ] b: -0.04450037403963673 cost: 570.6252579376106\niteration: 54701 w: [ 0.239453  0.145813 -0.487148 -1.464791] b: -0.04450857951135688 cost: 570.6042279047145\niteration: 54801 w: [ 0.239468   0.1460755 -0.487898  -1.465391 ] b: -0.04451668734511934 cost: 570.5831645555419\niteration: 54901 w: [ 0.2394835  0.1463755 -0.488648  -1.465991 ] b: -0.044524691593873233 cost: 570.5619267333468\niteration: 55001 w: [ 0.2394985  0.1466755 -0.489398  -1.466591 ] b: -0.044532600467770384 cost: 570.5407256843602\niteration: 55101 w: [ 0.2395135  0.1469755 -0.490148  -1.467191 ] b: -0.044540406320805144 cost: 570.5195611994791\niteration: 55201 w: [ 0.2395285  0.1472755 -0.490898  -1.467791 ] b: -0.04454811571589193 cost: 570.4984332770302\niteration: 55301 w: [ 0.2395435  0.1475755 -0.491648  -1.4683875] b: -0.04455572340303722 cost: 570.4773844482463\niteration: 55401 w: [ 0.2395575  0.1478755 -0.492398  -1.4689375] b: -0.04456323919682102 cost: 570.4569353087255\niteration: 55501 w: [ 0.2395715  0.1481755 -0.493148  -1.469496 ] b: -0.04457065797154219 cost: 570.4364147788573\niteration: 55601 w: [ 0.239587   0.1484755 -0.493898  -1.470096 ] b: -0.04457798249458192 cost: 570.4154274737182\niteration: 55701 w: [ 0.239602   0.1487755 -0.494648  -1.470696 ] b: -0.04458520803349047 cost: 570.3944767350475\niteration: 55801 w: [ 0.2396165  0.1490755 -0.495398  -1.471296 ] b: -0.04459233313599394 cost: 570.3735629365909\niteration: 55901 w: [ 0.239632   0.1493755 -0.496148  -1.471896 ] b: -0.044599356714052685 cost: 570.3526849354695\niteration: 56001 w: [ 0.239647   0.1496755 -0.496898  -1.472496 ] b: -0.04460628387337297 cost: 570.331843872712\niteration: 56101 w: [ 0.239661   0.1499755 -0.497648  -1.4730585] b: -0.04461310856722972 cost: 570.3114793660353\niteration: 56201 w: [ 0.239675   0.1502755 -0.498398  -1.4736085] b: -0.04461984425898379 cost: 570.2912929971058\niteration: 56301 w: [ 0.239689   0.1505755 -0.499148  -1.4741585] b: -0.04462648736223333 cost: 570.2711377636118\niteration: 56401 w: [ 0.2397025  0.1508755 -0.499898  -1.4746925] b: -0.044633039926924044 cost: 570.2511989012149\niteration: 56501 w: [ 0.239715   0.1511755 -0.500648  -1.4751925] b: -0.044639506135810456 cost: 570.2316814574103\niteration: 56601 w: [ 0.2397275  0.1514755 -0.501398  -1.4756925] b: -0.0446458883217852 cost: 570.2121900848757\niteration: 56701 w: [ 0.23974    0.1517755 -0.502148  -1.4761925] b: -0.04465218576046076 cost: 570.192724783249\niteration: 56801 w: [ 0.239753   0.1520755 -0.502898  -1.476706 ] b: -0.04465840011527091 cost: 570.1731317458647\niteration: 56901 w: [ 0.2397665  0.1523755 -0.503648  -1.477256 ] b: -0.044664523709915506 cost: 570.1531526954597\niteration: 57001 w: [ 0.239781   0.1526755 -0.504398  -1.477806 ] b: -0.044670555839731285 cost: 570.1332041366994\niteration: 57101 w: [ 0.239794   0.1529755 -0.505148  -1.478356 ] b: -0.04467649505190555 cost: 570.1132874964165\niteration: 57201 w: [ 0.2398085  0.1532755 -0.505898  -1.478906 ] b: -0.04468234135110249 cost: 570.0934007972817\niteration: 57301 w: [ 0.239822   0.1535755 -0.506648  -1.479456 ] b: -0.04468809692781731 cost: 570.0735459058966\niteration: 57401 w: [ 0.239836   0.1538755 -0.507398  -1.480006 ] b: -0.044693758872060584 cost: 570.0537218065622\niteration: 57501 w: [ 0.23985    0.1541755 -0.508148  -1.480556 ] b: -0.04469933228908955 cost: 570.0339288367919\niteration: 57601 w: [ 0.2398625  0.1544755 -0.508898  -1.4810705] b: -0.04470481296680544 cost: 570.0145566313226\niteration: 57701 w: [ 0.2398755  0.1547755 -0.509648  -1.4815705] b: -0.04471020815553974 cost: 569.9953696812341\niteration: 57801 w: [ 0.239888   0.1550755 -0.510398  -1.4820705] b: -0.04471552192522094 cost: 569.9762090808101\niteration: 57901 w: [ 0.2399005  0.1553755 -0.511148  -1.4825705] b: -0.04472075027235149 cost: 569.9570745457365\niteration: 58001 w: [ 0.239913   0.1556755 -0.511898  -1.4830705] b: -0.04472589320116645 cost: 569.9379660755556\niteration: 58101 w: [ 0.239926   0.1559755 -0.512648  -1.4835705] b: -0.04473095472348089 cost: 569.9188835482136\niteration: 58201 w: [ 0.239939   0.1562755 -0.513398  -1.484102 ] b: -0.04473592974685667 cost: 569.8994907263377\niteration: 58301 w: [ 0.239953   0.1565755 -0.514148  -1.484652 ] b: -0.04474081251747856 cost: 569.879930360033\niteration: 58401 w: [ 0.2399665  0.1568755 -0.514898  -1.485202 ] b: -0.04474560643547577 cost: 569.8604012945524\niteration: 58501 w: [ 0.2399805  0.1571755 -0.515648  -1.485752 ] b: -0.04475030349020866 cost: 569.8409029863919\niteration: 58601 w: [ 0.239994   0.1574755 -0.516398  -1.486298 ] b: -0.044754911697236444 cost: 569.8214779421842\niteration: 58701 w: [ 0.2400065  0.1577755 -0.517148  -1.486798 ] b: -0.04475943173374751 cost: 569.8025627717394\niteration: 58801 w: [ 0.2400185  0.1580755 -0.517898  -1.4872595] b: -0.044763870528273 cost: 569.7840727218494\niteration: 58901 w: [ 0.24003    0.1583755 -0.518648  -1.4877095] b: -0.04476823554538529 cost: 569.765724311177\niteration: 59001 w: [ 0.240041   0.1586755 -0.519398  -1.4881595] b: -0.0447725226465158 cost: 569.7473974735639\niteration: 59101 w: [ 0.240052   0.1589755 -0.520148  -1.4886095] b: -0.04477673219981719 cost: 569.729092338658\niteration: 59201 w: [ 0.2400635  0.1592755 -0.520898  -1.4890595] b: -0.04478086785247668 cost: 569.7108081686955\niteration: 59301 w: [ 0.240075   0.1595755 -0.521648  -1.4895095] b: -0.044784926693603455 cost: 569.6925455654234\niteration: 59401 w: [ 0.2400865  0.1598755 -0.522398  -1.4899595] b: -0.04478891127734012 cost: 569.6743045283498\niteration: 59501 w: [ 0.240098   0.1601755 -0.523148  -1.4904095] b: -0.044792816871129626 cost: 569.6560850575707\niteration: 59601 w: [ 0.240109   0.1604755 -0.523898  -1.4908645] b: -0.0447966461507861 cost: 569.6378368637107\niteration: 59701 w: [ 0.2401215  0.1607755 -0.524648  -1.4913645] b: -0.044800393298342706 cost: 569.619161347444\niteration: 59801 w: [ 0.240134   0.1610755 -0.525398  -1.4918645] b: -0.04480405801626756 cost: 569.6005118880126\niteration: 59901 w: [ 0.240147   0.1613755 -0.526148  -1.4923645] b: -0.04480763520812887 cost: 569.5818882639572\niteration: 60001 w: [ 0.2401595  0.1616755 -0.526898  -1.4928645] b: -0.04481113070740227 cost: 569.5632908317524\niteration: 60101 w: [ 0.240171   0.1619755 -0.527648  -1.493334 ] b: -0.04481454605901117 cost: 569.5450182054849\niteration: 60201 w: [ 0.2401825  0.1622755 -0.528398  -1.493784 ] b: -0.044817882300208826 cost: 569.526958672386\niteration: 60301 w: [ 0.2401935  0.1625755 -0.529148  -1.494234 ] b: -0.044821140312463585 cost: 569.5089209046606\niteration: 60401 w: [ 0.240205   0.1628755 -0.529898  -1.494684 ] b: -0.044824322649991324 cost: 569.490904266945\niteration: 60501 w: [ 0.2402165  0.1631755 -0.530648  -1.495134 ] b: -0.044827431866871645 cost: 569.4729091918731\niteration: 60601 w: [ 0.2402275  0.1634755 -0.531398  -1.495584 ] b: -0.04483046541656892 cost: 569.4549357833702\niteration: 60701 w: [ 0.240239   0.1637755 -0.532148  -1.496034 ] b: -0.044833418566627974 cost: 569.4369836018276\niteration: 60801 w: [ 0.24025    0.1640755 -0.532898  -1.496484 ] b: -0.04483629569279948 cost: 569.4190532196315\niteration: 60901 w: [ 0.2402615  0.1643755 -0.533648  -1.496934 ] b: -0.04483909898493552 cost: 569.4011439298841\niteration: 61001 w: [ 0.240273   0.1646755 -0.534398  -1.497384 ] b: -0.04484182589650491 cost: 569.3832562014524\niteration: 61101 w: [ 0.240284   0.16497   -0.535148  -1.4978395] b: -0.04484447610915759 cost: 569.3653696146876\niteration: 61201 w: [ 0.2402965  0.1652235 -0.535898  -1.498336 ] b: -0.044847041533183925 cost: 569.3473522030133\niteration: 61301 w: [ 0.24031    0.1654735 -0.536648  -1.498836 ] b: -0.04484952532626583 cost: 569.3293470960521\niteration: 61401 w: [ 0.240322   0.1657235 -0.537398  -1.4993245] b: -0.04485192752755638 cost: 569.3114743426181\niteration: 61501 w: [ 0.2403325  0.1659735 -0.538148  -1.4997305] b: -0.04485425073040904 cost: 569.2943831870945\niteration: 61601 w: [ 0.240342   0.1662235 -0.538898  -1.5001305] b: -0.044856510479960324 cost: 569.2773651819281\niteration: 61701 w: [ 0.2403525  0.1664735 -0.539648  -1.5005305] b: -0.04485869992424291 cost: 569.2603635876148\niteration: 61801 w: [ 0.2403625  0.1667235 -0.540398  -1.5009305] b: -0.0448608179737973 cost: 569.2433798392283\niteration: 61901 w: [ 0.240373   0.1669735 -0.541148  -1.5013305] b: -0.044862869732760276 cost: 569.226413020541\niteration: 62001 w: [ 0.240383   0.1672235 -0.541898  -1.5017305] b: -0.044864851925541994 cost: 569.2094638208309\niteration: 62101 w: [ 0.2403935  0.1674735 -0.542648  -1.5021305] b: -0.044866771113509694 cost: 569.1925317766621\niteration: 62201 w: [ 0.2404035  0.1677235 -0.543398  -1.5025305] b: -0.044868615641347094 cost: 569.1756171246001\niteration: 62301 w: [ 0.2404135  0.1679735 -0.544148  -1.5029305] b: -0.04487039680683701 cost: 569.158719838163\niteration: 62401 w: [ 0.2404235  0.1682235 -0.544898  -1.5033305] b: -0.04487210514081172 cost: 569.1418399173649\niteration: 62501 w: [ 0.2404335  0.1684735 -0.545648  -1.5037305] b: -0.04487374428999055 cost: 569.1249773617413\niteration: 62601 w: [ 0.240444   0.1687235 -0.546398  -1.5041305] b: -0.044875319358474726 cost: 569.1081316326905\niteration: 62701 w: [ 0.240454   0.1689735 -0.547148  -1.5045305] b: -0.04487682342724546 cost: 569.0913036216986\niteration: 62801 w: [ 0.2404645  0.1692235 -0.547898  -1.5049485] b: -0.04487826062547549 cost: 569.0743368942079\niteration: 62901 w: [ 0.240476   0.1694735 -0.548648  -1.5053945] b: -0.04487962312379703 cost: 569.0571471180903\niteration: 63001 w: [ 0.2404865  0.1697235 -0.549398  -1.5057945] b: -0.044880911909810785 cost: 569.0403733284305\niteration: 63101 w: [ 0.240496   0.1699735 -0.550148  -1.5061945] b: -0.04488213517528422 cost: 569.0236173541431\niteration: 63201 w: [ 0.2405065  0.1702235 -0.550898  -1.5065945] b: -0.044883287094389254 cost: 569.0068779218115\niteration: 63301 w: [ 0.2405165  0.1704735 -0.551648  -1.5069945] b: -0.044884370585207375 cost: 568.9901562626421\niteration: 63401 w: [ 0.240527   0.1707235 -0.552398  -1.5073945] b: -0.04488538637984364 cost: 568.9734515987129\niteration: 63501 w: [ 0.240537   0.1709735 -0.553148  -1.5077945] b: -0.04488633484604603 cost: 568.9567644795526\niteration: 63601 w: [ 0.240547   0.1712235 -0.553898  -1.5081945] b: -0.04488721562285334 cost: 568.9400947220805\niteration: 63701 w: [ 0.2405575  0.1714735 -0.554648  -1.5085945] b: -0.0448880254347744 cost: 568.9234420029147\niteration: 63801 w: [ 0.240567   0.1717235 -0.555398  -1.5089945] b: -0.044888768292820215 cost: 568.9068072909803\niteration: 63901 w: [ 0.2405775  0.1719735 -0.556148  -1.5093945] b: -0.04488944019289172 cost: 568.8901889254711\niteration: 64001 w: [ 0.2405875  0.1722235 -0.556898  -1.5097945] b: -0.04489004186700648 cost: 568.8735884276006\niteration: 64101 w: [ 0.2405975  0.1724735 -0.557648  -1.5101945] b: -0.044890578783624895 cost: 568.857005289795\niteration: 64201 w: [ 0.240608   0.1727235 -0.558398  -1.5105945] b: -0.04489104584543653 cost: 568.8404388652581\niteration: 64301 w: [ 0.2406185  0.1729735 -0.559148  -1.5109945] b: -0.04489144633489671 cost: 568.8238900289779\niteration: 64401 w: [ 0.2406285  0.1732235 -0.559898  -1.5113945] b: -0.0448917766120924 cost: 568.8073586019825\niteration: 64501 w: [ 0.240638   0.1734735 -0.560648  -1.5117685] b: -0.04489204068447237 cost: 568.7910525097269\niteration: 64601 w: [ 0.240648   0.1737235 -0.561398  -1.512156 ] b: -0.044892240358248015 cost: 568.7746542129256\niteration: 64701 w: [ 0.2406565  0.1739735 -0.562148  -1.512506 ] b: -0.04489237466835711 cost: 568.75856970587\niteration: 64801 w: [ 0.2406655  0.1742235 -0.562898  -1.512856 ] b: -0.04489244696729769 cost: 568.7424986328639\niteration: 64901 w: [ 0.2406745  0.1744735 -0.563648  -1.513206 ] b: -0.04489246527326841 cost: 568.726441212959\niteration: 65001 w: [ 0.240683   0.1747235 -0.564398  -1.513556 ] b: -0.04489241428738068 cost: 568.7103977298219\niteration: 65101 w: [ 0.240692   0.1749735 -0.565148  -1.513906 ] b: -0.04489231004334036 cost: 568.6943674824792\niteration: 65201 w: [ 0.240701   0.1752235 -0.565898  -1.514256 ] b: -0.04489214270710021 cost: 568.6783508876156\niteration: 65301 w: [ 0.24071    0.1754735 -0.566648  -1.514606 ] b: -0.04489191483203941 cost: 568.6623479450496\niteration: 65401 w: [ 0.2407185  0.1757235 -0.567398  -1.514956 ] b: -0.04489162970012087 cost: 568.6463590023856\niteration: 65501 w: [ 0.240728   0.1759735 -0.568148  -1.515306 ] b: -0.044891274927176844 cost: 568.630383016019\niteration: 65601 w: [ 0.2407365  0.1762235 -0.568898  -1.515656 ] b: -0.04489086618262559 cost: 568.6144211104248\niteration: 65701 w: [ 0.240745   0.1764735 -0.569648  -1.516006 ] b: -0.04489039508976024 cost: 568.5984731861477\niteration: 65801 w: [ 0.2407545  0.1767235 -0.570398  -1.516356 ] b: -0.044889863108977156 cost: 568.582537824422\niteration: 65901 w: [ 0.240763   0.1769735 -0.571148  -1.516706 ] b: -0.04488927206488172 cost: 568.5666169357443\niteration: 66001 w: [ 0.240772   0.1772235 -0.571898  -1.517056 ] b: -0.04488861649553792 cost: 568.5507092868971\niteration: 66101 w: [ 0.240781   0.1774735 -0.572648  -1.517406 ] b: -0.04488790514804882 cost: 568.5348152887956\niteration: 66201 w: [ 0.240789   0.1777235 -0.573398  -1.517728 ] b: -0.044887133771344435 cost: 568.5191427128302\niteration: 66301 w: [ 0.2407965  0.1779735 -0.574148  -1.518028 ] b: -0.044886305983297584 cost: 568.503644320882\niteration: 66401 w: [ 0.2408045  0.1782235 -0.574898  -1.518328 ] b: -0.044885425877233175 cost: 568.4881558335743\niteration: 66501 w: [ 0.240813   0.1784735 -0.575648  -1.5186625] b: -0.04488449506208519 cost: 568.4724254399188\niteration: 66601 w: [ 0.2408215  0.1787235 -0.576398  -1.5190125] b: -0.04488350076372432 cost: 568.4565950135748\niteration: 66701 w: [ 0.240831   0.1789735 -0.577148  -1.5193625] b: -0.044882445239250175 cost: 568.4407774799153\niteration: 66801 w: [ 0.2408395  0.1792235 -0.577898  -1.5197125] b: -0.04488133286362339 cost: 568.4249740850212\niteration: 66901 w: [ 0.2408485  0.1794735 -0.578648  -1.5200625] b: -0.04488015270993369 cost: 568.4091840944394\niteration: 67001 w: [ 0.2408575  0.1797235 -0.579398  -1.5204125] b: -0.04487891971888266 cost: 568.3934077530002\niteration: 67101 w: [ 0.240866   0.1799735 -0.580148  -1.5207625] b: -0.044877620413157765 cost: 568.3776453658315\niteration: 67201 w: [ 0.240875   0.1802235 -0.580898  -1.5211125] b: -0.0448762588036135 cost: 568.3618961875501\niteration: 67301 w: [ 0.240884   0.1804735 -0.581648  -1.5214625] b: -0.04487484254413159 cost: 568.3461606577763\niteration: 67401 w: [ 0.2408925  0.1807235 -0.582398  -1.5218125] b: -0.044873359614753236 cost: 568.3304392760039\niteration: 67501 w: [ 0.2409015  0.1809735 -0.583148  -1.5221625] b: -0.04487182021994447 cost: 568.3147309081618\niteration: 67601 w: [ 0.2409105  0.1812235 -0.583898  -1.5225125] b: -0.04487021671168406 cost: 568.2990361879282\niteration: 67701 w: [ 0.2409195  0.1814735 -0.584648  -1.5228625] b: -0.044868554558079185 cost: 568.2833551152875\niteration: 67801 w: [ 0.240928   0.1817235 -0.585398  -1.5232125] b: -0.0448668326690262 cost: 568.2676882497838\niteration: 67901 w: [ 0.240937   0.1819735 -0.586148  -1.5235625] b: -0.04486504813285369 cost: 568.2520343372929\niteration: 68001 w: [ 0.2409455  0.1822235 -0.586898  -1.523876 ] b: -0.04486320764871655 cost: 568.2366421404846\niteration: 68101 w: [ 0.2409525  0.1824735 -0.587648  -1.5241685] b: -0.04486131698491053 cost: 568.2214041689945\niteration: 68201 w: [ 0.240959   0.1827235 -0.588398  -1.5244185] b: -0.04485937758432829 cost: 568.2064625147353\niteration: 68301 w: [ 0.2409655  0.1829735 -0.589148  -1.5246685] b: -0.04485739010157927 cost: 568.191528501538\niteration: 68401 w: [ 0.240972   0.1832235 -0.589898  -1.5249185] b: -0.04485535453896185 cost: 568.1766021292979\niteration: 68501 w: [ 0.2409785  0.1834735 -0.590648  -1.525183 ] b: -0.044853275964410515 cost: 568.1615867385609\niteration: 68601 w: [ 0.240986   0.1837235 -0.591398  -1.525483 ] b: -0.04485114663100071 cost: 568.1463438544681\niteration: 68701 w: [ 0.2409935  0.1839735 -0.592148  -1.525783 ] b: -0.04484896431598089 cost: 568.1311113877174\niteration: 68801 w: [ 0.2410015  0.1842235 -0.592898  -1.526083 ] b: -0.04484673230101014 cost: 568.115888847355\niteration: 68901 w: [ 0.241009   0.1844735 -0.593648  -1.526383 ] b: -0.044844449131341994 cost: 568.1006770696602\niteration: 69001 w: [ 0.2410165  0.1847235 -0.594398  -1.526683 ] b: -0.04484211189488024 cost: 568.0854757087199\niteration: 69101 w: [ 0.241024   0.1849735 -0.595148  -1.526983 ] b: -0.044839717679744914 cost: 568.0702847641898\niteration: 69201 w: [ 0.241032   0.1852235 -0.595898  -1.527283 ] b: -0.044837270860516566 cost: 568.0551037607585\niteration: 69301 w: [ 0.24104    0.1854735 -0.596648  -1.527583 ] b: -0.04483477617612403 cost: 568.0399334800187\niteration: 69401 w: [ 0.2410475  0.1857235 -0.597398  -1.527883 ] b: -0.04483222634257584 cost: 568.0247734941025\niteration: 69501 w: [ 0.241055   0.1859735 -0.598148  -1.528183 ] b: -0.04482962245553335 cost: 568.0096239240279\niteration: 69601 w: [ 0.2410625  0.1862235 -0.598898  -1.528483 ] b: -0.044826965974957146 cost: 567.9944847696721\niteration: 69701 w: [ 0.24107    0.1864735 -0.599648  -1.528783 ] b: -0.044824255810442755 cost: 567.9793560307802\niteration: 69801 w: [ 0.2410775  0.1867235 -0.600398  -1.529083 ] b: -0.044821494514963575 cost: 567.964237707296\niteration: 69901 w: [ 0.2410855  0.1869735 -0.601148  -1.529383 ] b: -0.04481868209123726 cost: 567.9491295015928\niteration: 70001 w: [ 0.241093   0.1872235 -0.601898  -1.5296815] b: -0.0448158157771353 cost: 567.9340411849798\niteration: 70101 w: [ 0.241099   0.1874735 -0.602648  -1.5299325] b: -0.044812903178222785 cost: 567.9192575548324\niteration: 70201 w: [ 0.2411055  0.1877235 -0.603398  -1.5301825] b: -0.04480994178728949 cost: 567.9044873510401\niteration: 70301 w: [ 0.241112   0.1879735 -0.604148  -1.5304325] b: -0.04480693491556161 cost: 567.8897247860717\niteration: 70401 w: [ 0.2411185  0.1882235 -0.604898  -1.5306825] b: -0.044803880743585434 cost: 567.8749698597245\niteration: 70501 w: [ 0.2411245  0.1884735 -0.605648  -1.5309325] b: -0.04480078546725343 cost: 567.8602227370592\niteration: 70601 w: [ 0.2411305  0.1887235 -0.606398  -1.5311825] b: -0.04479764544539645 cost: 567.8454835057097\niteration: 70701 w: [ 0.241137   0.1889735 -0.607148  -1.5314325] b: -0.04479446068042328 cost: 567.8307511515355\niteration: 70801 w: [ 0.241144   0.1892235 -0.607898  -1.5317035] b: -0.04479122732824677 cost: 567.8159001742825\niteration: 70901 w: [ 0.241152   0.1894735 -0.608648  -1.5320035] b: -0.04478794283028032 cost: 567.8008842338573\niteration: 71001 w: [ 0.2411595  0.1897235 -0.609398  -1.5323035] b: -0.04478460978039001 cost: 567.7858789900508\niteration: 71101 w: [ 0.2411675  0.1899735 -0.610148  -1.5326035] b: -0.04478121797998786 cost: 567.7708840388574\niteration: 71201 w: [ 0.241175   0.1902235 -0.610898  -1.5329035] b: -0.044777775811414816 cost: 567.7558994763973\niteration: 71301 w: [ 0.2411825  0.1904735 -0.611648  -1.5332035] b: -0.04477427817658842 cost: 567.740925326507\niteration: 71401 w: [ 0.24119    0.1907235 -0.612398  -1.5335035] b: -0.044770729450173415 cost: 567.7259615892932\niteration: 71501 w: [ 0.241198   0.1909735 -0.613148  -1.5338035] b: -0.044767129634804455 cost: 567.7110081634311\niteration: 71601 w: [ 0.241205   0.1912235 -0.613898  -1.5341035] b: -0.0447634783687217 cost: 567.696065352114\niteration: 71701 w: [ 0.241213   0.1914735 -0.614648  -1.5344035] b: -0.0447597716469623 cost: 567.6811324617726\niteration: 71801 w: [ 0.24122    0.1917235 -0.615398  -1.5347035] b: -0.04475601056515604 cost: 567.6662107624259\niteration: 71901 w: [ 0.241228   0.1919735 -0.616148  -1.5350035] b: -0.0447521984050765 cost: 567.6512984070831\niteration: 72001 w: [ 0.241236   0.1922235 -0.616898  -1.5353035] b: -0.04474833662653165 cost: 567.6363967715947\niteration: 72101 w: [ 0.2412425  0.1924735 -0.617648  -1.5355705] b: -0.04474442339446299 cost: 567.6216912480177\niteration: 72201 w: [ 0.2412485  0.1927235 -0.618398  -1.535797 ] b: -0.044740460851290756 cost: 567.6072208608837\niteration: 72301 w: [ 0.2412535  0.1929735 -0.619148  -1.535997 ] b: -0.044736458378039246 cost: 567.5929048009751\niteration: 72401 w: [ 0.2412585  0.1932235 -0.619898  -1.536197 ] b: -0.04473242230707027 cost: 567.5785940011182\niteration: 72501 w: [ 0.2412635  0.1934735 -0.620648  -1.536397 ] b: -0.04472835227582463 cost: 567.5642884611924\niteration: 72601 w: [ 0.241269   0.1937235 -0.621398  -1.536597 ] b: -0.04472424027081226 cost: 567.5499880663137\niteration: 72701 w: [ 0.2412735  0.1939735 -0.622148  -1.536797 ] b: -0.044720095037857566 cost: 567.535693159498\niteration: 72801 w: [ 0.241279   0.1942235 -0.622898  -1.536997 ] b: -0.044715912935543264 cost: 567.5214030691868\niteration: 72901 w: [ 0.2412845  0.1944735 -0.623648  -1.537197 ] b: -0.04471168740771142 cost: 567.507118619557\niteration: 73001 w: [ 0.241289   0.1947235 -0.624398  -1.537397 ] b: -0.044707426471463016 cost: 567.492839108406\niteration: 73101 w: [ 0.2412935  0.1949735 -0.625148  -1.537597 ] b: -0.044703133043405736 cost: 567.4785656677225\niteration: 73201 w: [ 0.241299   0.1952235 -0.625898  -1.537797 ] b: -0.04469880421081756 cost: 567.4642961851722\niteration: 73301 w: [ 0.2413045  0.1954735 -0.626648  -1.538007 ] b: -0.044694428422869165 cost: 567.449978736396\niteration: 73401 w: [ 0.241311   0.1957235 -0.627398  -1.538257 ] b: -0.04469001138652787 cost: 567.4354533251985\niteration: 73501 w: [ 0.2413175  0.1959735 -0.628148  -1.538507 ] b: -0.04468555477135992 cost: 567.4209355500349\niteration: 73601 w: [ 0.2413235  0.1962235 -0.628898  -1.538757 ] b: -0.04468105384316036 cost: 567.4064254621868\niteration: 73701 w: [ 0.24133    0.1964735 -0.629648  -1.539007 ] b: -0.04467650605396739 cost: 567.3919227848545\niteration: 73801 w: [ 0.2413365  0.1967235 -0.630398  -1.539257 ] b: -0.044671908491398724 cost: 567.3774277423294\niteration: 73901 w: [ 0.241343   0.1969735 -0.631148  -1.539507 ] b: -0.044667265529727836 cost: 567.3629403349152\niteration: 74001 w: [ 0.241349   0.1972235 -0.631898  -1.539757 ] b: -0.0446625800858231 cost: 567.3484605192311\niteration: 74101 w: [ 0.241355   0.1974735 -0.632648  -1.540007 ] b: -0.04465785216191007 cost: 567.3339885891546\niteration: 74201 w: [ 0.2413615  0.1977235 -0.633398  -1.540257 ] b: -0.04465308103157559 cost: 567.3195237411394\niteration: 74301 w: [ 0.241368   0.1979735 -0.634148  -1.540507 ] b: -0.04464826268935859 cost: 567.3050665276254\niteration: 74401 w: [ 0.2413745  0.1982235 -0.634898  -1.540757 ] b: -0.04464339531594874 cost: 567.2906169483437\niteration: 74501 w: [ 0.241381   0.1984735 -0.635648  -1.541007 ] b: -0.04463848729315726 cost: 567.2761750040089\niteration: 74601 w: [ 0.241387   0.1987235 -0.636398  -1.541257 ] b: -0.044633532065139724 cost: 567.2617408045877\niteration: 74701 w: [ 0.241393   0.1989735 -0.637148  -1.541507 ] b: -0.04462853473495631 cost: 567.2473144898116\niteration: 74801 w: [ 0.241399   0.1992235 -0.637898  -1.5417395] b: -0.044623494392478716 cost: 567.2329823641644\niteration: 74901 w: [ 0.2414045  0.1994735 -0.638648  -1.5419395] b: -0.044618411746205615 cost: 567.2188178909146\niteration: 75001 w: [ 0.2414095  0.1997235 -0.639398  -1.5421395] b: -0.04461328717659812 cost: 567.2046591030679\niteration: 75101 w: [ 0.2414145  0.1999735 -0.640148  -1.5423395] b: -0.0446081312513026 cost: 567.190505572774\niteration: 75201 w: [ 0.2414195  0.2002235 -0.640898  -1.5425395] b: -0.04460293887134891 cost: 567.1763572994224\niteration: 75301 w: [ 0.241425   0.2004735 -0.641648  -1.5427395] b: -0.04459770858128816 cost: 567.1622139174234\niteration: 75401 w: [ 0.24143    0.2007235 -0.642398  -1.5429395] b: -0.04459244038298292 cost: 567.1480760508351\niteration: 75501 w: [ 0.241435   0.2009735 -0.643148  -1.5431395] b: -0.04458714047201242 cost: 567.1339434414232\niteration: 75601 w: [ 0.2414405  0.2012235 -0.643898  -1.5433395] b: -0.044581796098599 cost: 567.1198159994211\niteration: 75701 w: [ 0.2414455  0.2014735 -0.644648  -1.5435395] b: -0.04457640981484352 cost: 567.1056937951935\niteration: 75801 w: [ 0.2414505  0.2017235 -0.645398  -1.5437395] b: -0.0445709929170357 cost: 567.0915768479932\niteration: 75901 w: [ 0.241455   0.2019735 -0.646148  -1.5439395] b: -0.044565543221060815 cost: 567.0774655650389\niteration: 76001 w: [ 0.2414605  0.2022235 -0.646898  -1.5441405] b: -0.04456004932449957 cost: 567.0633540064424\niteration: 76101 w: [ 0.2414665  0.2024735 -0.647648  -1.544374 ] b: -0.04455451390285402 cost: 567.0490951615458\niteration: 76201 w: [ 0.241473   0.2027235 -0.648398  -1.544624 ] b: -0.04454893986455376 cost: 567.0347658845934\niteration: 76301 w: [ 0.241479   0.2029735 -0.649148  -1.544874 ] b: -0.04454332156907549 cost: 567.0204443621012\niteration: 76401 w: [ 0.2414855  0.2032235 -0.649898  -1.545124 ] b: -0.044537656468473706 cost: 567.0061301770573\niteration: 76501 w: [ 0.241492   0.2034735 -0.650648  -1.545374 ] b: -0.04453194456494381 cost: 566.9918236245034\niteration: 76601 w: [ 0.2414985  0.2037235 -0.651398  -1.545624 ] b: -0.04452618549644902 cost: 566.9775247043\niteration: 76701 w: [ 0.2415035  0.2039735 -0.652148  -1.545833 ] b: -0.044520385995847525 cost: 566.9634200128506\niteration: 76801 w: [ 0.2415085  0.2042235 -0.652898  -1.546033 ] b: -0.044514554075105975 cost: 566.9493617327123\niteration: 76901 w: [ 0.241514   0.2044735 -0.653648  -1.546233 ] b: -0.0445086817259632 cost: 566.9353085171541\niteration: 77001 w: [ 0.2415185  0.2047235 -0.654398  -1.546433 ] b: -0.0445027689502574 cost: 566.9212609367133\niteration: 77101 w: [ 0.2415235  0.2049735 -0.655148  -1.546633 ] b: -0.044496824129579116 cost: 566.9072184216469\niteration: 77201 w: [ 0.241529   0.2052235 -0.655898  -1.546833 ] b: -0.04449084471558758 cost: 566.8931806534987\niteration: 77301 w: [ 0.241534   0.2054735 -0.656648  -1.547033 ] b: -0.0444848186871729 cost: 566.879148541308\niteration: 77401 w: [ 0.241539   0.2057235 -0.657398  -1.547233 ] b: -0.04447875515444184 cost: 566.8651216837847\niteration: 77501 w: [ 0.2415445  0.2059735 -0.658148  -1.547433 ] b: -0.04447266104162603 cost: 566.8510998520667\niteration: 77601 w: [ 0.241549   0.2062235 -0.658898  -1.547619 ] b: -0.0444665281574926 cost: 566.8371445071833\niteration: 77701 w: [ 0.241553   0.2064735 -0.659648  -1.547769 ] b: -0.04446036404281835 cost: 566.8233501539166\niteration: 77801 w: [ 0.2415565  0.2067235 -0.660398  -1.547919 ] b: -0.044454171711708665 cost: 566.8095592967883\niteration: 77901 w: [ 0.2415605  0.2069735 -0.661148  -1.548069 ] b: -0.04444795080130273 cost: 566.7957715491276\niteration: 78001 w: [ 0.241564   0.2072235 -0.661898  -1.548219 ] b: -0.044441696576763716 cost: 566.7819874719131\niteration: 78101 w: [ 0.2415685  0.2074735 -0.662648  -1.548369 ] b: -0.044435414140217955 cost: 566.7682062768596\niteration: 78201 w: [ 0.241572   0.2077235 -0.663398  -1.548519 ] b: -0.04442910640779916 cost: 566.7544285946202\niteration: 78301 w: [ 0.2415755  0.2079735 -0.664148  -1.548669 ] b: -0.04442276609430268 cost: 566.7406544947395\niteration: 78401 w: [ 0.2415795  0.2082235 -0.664898  -1.548819 ] b: -0.04441639392987798 cost: 566.726883364453\niteration: 78501 w: [ 0.2415835  0.2084735 -0.665648  -1.548969 ] b: -0.04440999574532973 cost: 566.7131156423799\niteration: 78601 w: [ 0.2415875  0.2087235 -0.666398  -1.549119 ] b: -0.0444035693561241 cost: 566.6993513282141\niteration: 78701 w: [ 0.241591   0.2089735 -0.667148  -1.549269 ] b: -0.04439711112036111 cost: 566.6855904016103\niteration: 78801 w: [ 0.241595   0.2092235 -0.667898  -1.549419 ] b: -0.044390622496886606 cost: 566.6718326915116\niteration: 78901 w: [ 0.241599   0.2094735 -0.668648  -1.549569 ] b: -0.04438410749485907 cost: 566.6580783894475\niteration: 79001 w: [ 0.2416025  0.2097235 -0.669398  -1.549719 ] b: -0.04437756575125167 cost: 566.644327439185\niteration: 79101 w: [ 0.241606   0.2099735 -0.670148  -1.549869 ] b: -0.0443709903452587 cost: 566.6305800704831\niteration: 79201 w: [ 0.2416105  0.2102235 -0.670898  -1.550019 ] b: -0.044364386014641544 cost: 566.6168354506932\niteration: 79301 w: [ 0.241615   0.2104735 -0.671648  -1.550205 ] b: -0.04435775012600436 cost: 566.6029477281119\niteration: 79401 w: [ 0.24162    0.2107235 -0.672398  -1.550405 ] b: -0.044351078665761046 cost: 566.5890079620382\niteration: 79501 w: [ 0.241625   0.2109735 -0.673148  -1.550605 ] b: -0.04434437228402714 cost: 566.5750734493666\niteration: 79601 w: [ 0.2416305  0.2112235 -0.673898  -1.550805 ] b: -0.04433762260313563 cost: 566.5611440712677\niteration: 79701 w: [ 0.2416355  0.2114735 -0.674648  -1.551005 ] b: -0.04433083363246524 cost: 566.5472199579689\niteration: 79801 w: [ 0.2416405  0.2117235 -0.675398  -1.551205 ] b: -0.04432401083893338 cost: 566.5333010979483\niteration: 79901 w: [ 0.241645   0.2119735 -0.676148  -1.551405 ] b: -0.04431715495320546 cost: 566.5193879249645\niteration: 80001 w: [ 0.2416505  0.2122235 -0.676898  -1.551605 ] b: -0.044310255775840315 cost: 566.5054791362836\niteration: 80101 w: [ 0.2416555  0.2124735 -0.677648  -1.551805 ] b: -0.04430331950221026 cost: 566.4915760338886\niteration: 80201 w: [ 0.2416605  0.2127235 -0.678398  -1.552005 ] b: -0.0442963505062136 cost: 566.4776781845525\niteration: 80301 w: [ 0.241666   0.2129735 -0.679148  -1.552205 ] b: -0.044289341867313335 cost: 566.4637853288026\niteration: 80401 w: [ 0.241671   0.2132235 -0.679898  -1.552405 ] b: -0.04428229577335388 cost: 566.4498978784472\niteration: 80501 w: [ 0.2416755  0.2134735 -0.680648  -1.552605 ] b: -0.04427521222624583 cost: 566.4360161490459\niteration: 80601 w: [ 0.241681   0.2137235 -0.681398  -1.552805 ] b: -0.044268097057288236 cost: 566.4221387348672\niteration: 80701 w: [ 0.2416865  0.2139735 -0.682148  -1.553005 ] b: -0.04426093788097838 cost: 566.4082669578703\niteration: 80801 w: [ 0.2416915  0.2142235 -0.682898  -1.553205 ] b: -0.044253737613752384 cost: 566.3944004097254\niteration: 80901 w: [ 0.241696   0.2144735 -0.683648  -1.5534015] b: -0.04424650407418981 cost: 566.3805523811844\niteration: 81001 w: [ 0.2417     0.2147235 -0.684398  -1.5535535] b: -0.0442392484030999 cost: 566.366874485128\niteration: 81101 w: [ 0.241704   0.2149735 -0.685148  -1.5537035] b: -0.04423194912860855 cost: 566.3532074100382\niteration: 81201 w: [ 0.241708   0.2152235 -0.685898  -1.5538535] b: -0.04422462533743393 cost: 566.3395437422951\niteration: 81301 w: [ 0.2417115  0.2154735 -0.686648  -1.5540035] b: -0.044217278124250164 cost: 566.3258834800346\niteration: 81401 w: [ 0.2417155  0.2157235 -0.687398  -1.5541535] b: -0.04420989145970252 cost: 566.3122264133088\niteration: 81501 w: [ 0.241719   0.2159735 -0.688148  -1.5543035] b: -0.044202482468955556 cost: 566.2985729254839\niteration: 81601 w: [ 0.2417235  0.2162235 -0.688898  -1.5544535] b: -0.044195038402049726 cost: 566.2849225006541\niteration: 81701 w: [ 0.241727   0.2164735 -0.689648  -1.5546035] b: -0.04418756508952961 cost: 566.2712754017374\niteration: 81801 w: [ 0.2417305  0.2167235 -0.690398  -1.5547535] b: -0.044180069091013546 cost: 566.257631883448\niteration: 81901 w: [ 0.2417335  0.2169735 -0.691148  -1.554873 ] b: -0.04417253342569516 cost: 566.2441003949476\niteration: 82001 w: [ 0.241736   0.2172235 -0.691898  -1.554973 ] b: -0.04416499034453212 cost: 566.2306406449751\niteration: 82101 w: [ 0.241739   0.2174735 -0.692648  -1.555073 ] b: -0.04415741090929084 cost: 566.217182443745\niteration: 82201 w: [ 0.2417415  0.2177235 -0.693398  -1.555173 ] b: -0.04414982754621891 cost: 566.2037263877603\niteration: 82301 w: [ 0.241744   0.2179735 -0.694148  -1.555273 ] b: -0.04414220273050675 cost: 566.1902722096141\niteration: 82401 w: [ 0.2417465  0.2182235 -0.694898  -1.555373 ] b: -0.04413457872555968 cost: 566.1768199157001\niteration: 82501 w: [ 0.241749   0.2184735 -0.695648  -1.555473 ] b: -0.044126913634315756 cost: 566.1633694995927\niteration: 82601 w: [ 0.2417515  0.2187235 -0.696398  -1.555573 ] b: -0.04411924061215847 cost: 566.1499209663347\niteration: 82701 w: [ 0.241754   0.2189735 -0.697148  -1.555673 ] b: -0.04411153634278879 cost: 566.1364743123085\niteration: 82801 w: [ 0.2417575  0.2191995 -0.697922  -1.555797 ] b: -0.04410381153208676 cost: 566.1227044451356\niteration: 82901 w: [ 0.2417615  0.2193995 -0.698722  -1.555947 ] b: -0.04409605466964385 cost: 566.1085857709974\niteration: 83001 w: [ 0.2417655  0.2195995 -0.699522  -1.556097 ] b: -0.04408827476290967 cost: 566.0944705156153\niteration: 83101 w: [ 0.2417695  0.2197995 -0.700322  -1.556247 ] b: -0.044080464162358085 cost: 566.0803586777562\niteration: 83201 w: [ 0.241773   0.2199995 -0.701122  -1.556397 ] b: -0.044072620683480467 cost: 566.0662505793833\niteration: 83301 w: [ 0.2417775  0.2201995 -0.701922  -1.556547 ] b: -0.044064751250098316 cost: 566.0521452544546\niteration: 83401 w: [ 0.2417815  0.2203995 -0.702722  -1.556697 ] b: -0.044056852584561934 cost: 566.038043669485\niteration: 83501 w: [ 0.241785   0.2205995 -0.703522  -1.556847 ] b: -0.04404892104512502 cost: 566.0239457445279\niteration: 83601 w: [ 0.241789   0.2207995 -0.704322  -1.556997 ] b: -0.04404095845498934 cost: 566.0098509672554\niteration: 83701 w: [ 0.2417935  0.2209995 -0.705122  -1.557147 ] b: -0.044032968094685834 cost: 565.9957594175944\niteration: 83801 w: [ 0.241797   0.2211995 -0.705922  -1.557297 ] b: -0.04402494887260283 cost: 565.9816716654638\niteration: 83901 w: [ 0.241801   0.2213995 -0.706722  -1.557447 ] b: -0.04401689787563255 cost: 565.9675871402037\niteration: 84001 w: [ 0.241805   0.2215995 -0.707522  -1.557597 ] b: -0.044008814740949906 cost: 565.953506031749\niteration: 84101 w: [ 0.241809   0.2217995 -0.708322  -1.557747 ] b: -0.04400071112859554 cost: 565.9394283419414\niteration: 84201 w: [ 0.241813   0.2219995 -0.709122  -1.557897 ] b: -0.04399257246668448 cost: 565.9253540683698\niteration: 84301 w: [ 0.241817   0.2221995 -0.709922  -1.558047 ] b: -0.04398440057849661 cost: 565.9112832112706\niteration: 84401 w: [ 0.241821   0.2223995 -0.710722  -1.558197 ] b: -0.04397620202353111 cost: 565.8972157716677\niteration: 84501 w: [ 0.241825   0.2225995 -0.711522  -1.558347 ] b: -0.0439679753459165 cost: 565.883151749276\niteration: 84601 w: [ 0.241829   0.2227995 -0.712322  -1.558497 ] b: -0.04395971581078866 cost: 565.8690911432633\niteration: 84701 w: [ 0.241833   0.2229995 -0.713122  -1.558647 ] b: -0.043951425605690694 cost: 565.8550339539391\niteration: 84801 w: [ 0.241837   0.2231995 -0.713922  -1.558797 ] b: -0.043943108739772856 cost: 565.840980181919\niteration: 84901 w: [ 0.241841   0.2233995 -0.714722  -1.558947 ] b: -0.04393476157113743 cost: 565.8269298265482\niteration: 85001 w: [ 0.2418445  0.2235995 -0.715522  -1.5590905] b: -0.04392638143800761 cost: 565.8129031461231\niteration: 85101 w: [ 0.2418475  0.2237995 -0.716322  -1.5591905] b: -0.043917979696702414 cost: 565.7990126545548\niteration: 85201 w: [ 0.24185    0.2239995 -0.717122  -1.5592905] b: -0.04390955183724068 cost: 565.7851244516952\niteration: 85301 w: [ 0.2418525  0.2241995 -0.717922  -1.5593905] b: -0.04390111097634154 cost: 565.7712382959821\niteration: 85401 w: [ 0.2418555  0.2243995 -0.718722  -1.5594905] b: -0.04389263306963594 cost: 565.7573535235814\niteration: 85501 w: [ 0.241858   0.2245995 -0.719522  -1.5595905] b: -0.04388415163630646 cost: 565.7434712064734\niteration: 85601 w: [ 0.241861   0.2247995 -0.720322  -1.5596905] b: -0.04387563206629608 cost: 565.7295903627551\niteration: 85701 w: [ 0.2418635  0.2249995 -0.721122  -1.5597905] b: -0.04386710751443344 cost: 565.7157118836956\niteration: 85801 w: [ 0.2418665  0.2251995 -0.721922  -1.5598905] b: -0.04385854482794359 cost: 565.7018349683426\niteration: 85901 w: [ 0.241869   0.2253995 -0.722722  -1.5599905] b: -0.043849974247310256 cost: 565.687960326702\niteration: 86001 w: [ 0.2418715  0.2255995 -0.723522  -1.5600905] b: -0.04384137172776742 cost: 565.674087728679\niteration: 86101 w: [ 0.2418745  0.2257995 -0.724322  -1.5601905] b: -0.043832752208085725 cost: 565.6602165353485\niteration: 86201 w: [ 0.241877   0.2259995 -0.725122  -1.5602905] b: -0.043824110588573835 cost: 565.6463477757137\niteration: 86301 w: [ 0.24188    0.2261995 -0.725922  -1.5603905] b: -0.04381544213421699 cost: 565.6324805095624\niteration: 86401 w: [ 0.2418825  0.2263995 -0.726722  -1.5604905] b: -0.04380676178345182 cost: 565.6186155882541\niteration: 86501 w: [ 0.2418855  0.2265995 -0.727522  -1.5605905] b: -0.04379804039122569 cost: 565.6047522485059\niteration: 86601 w: [ 0.241888   0.2267995 -0.728322  -1.5606905] b: -0.04378931985650645 cost: 565.590891165143\niteration: 86701 w: [ 0.241891   0.2269995 -0.729122  -1.5607905] b: -0.043780556825052705 cost: 565.5770317536927\niteration: 86801 w: [ 0.2418935  0.2271995 -0.729922  -1.5608905] b: -0.043771791738631514 cost: 565.5631745073835\niteration: 86901 w: [ 0.241897   0.2273995 -0.730722  -1.561019 ] b: -0.04376298611426308 cost: 565.5492380857785\niteration: 87001 w: [ 0.241901   0.2275995 -0.731522  -1.561169 ] b: -0.04375415960676601 cost: 565.5352436938665\niteration: 87101 w: [ 0.241905   0.2277995 -0.732322  -1.561319 ] b: -0.04374530682982238 cost: 565.5212527184391\niteration: 87201 w: [ 0.241909   0.2279995 -0.733122  -1.561469 ] b: -0.04373641721903922 cost: 565.5072651575699\niteration: 87301 w: [ 0.2419125  0.2281995 -0.733922  -1.561619 ] b: -0.043727502434556184 cost: 565.4932814473848\niteration: 87401 w: [ 0.241917   0.2283995 -0.734722  -1.561769 ] b: -0.04371855774177547 cost: 565.4793002846758\niteration: 87501 w: [ 0.241921   0.2285995 -0.735522  -1.561919 ] b: -0.043709577312652216 cost: 565.4653229706638\niteration: 87601 w: [ 0.2419245  0.2287995 -0.736322  -1.562069 ] b: -0.04370057390044967 cost: 565.4513494259093\niteration: 87701 w: [ 0.241929   0.2289995 -0.737122  -1.562219 ] b: -0.04369153621240533 cost: 565.4373785910419\niteration: 87801 w: [ 0.241933   0.2291995 -0.737922  -1.562367 ] b: -0.04368246893742625 cost: 565.4234169022783\niteration: 87901 w: [ 0.2419355  0.2293995 -0.738722  -1.5624745] b: -0.04367338175751114 cost: 565.4095669425142\niteration: 88001 w: [ 0.2419385  0.2295995 -0.739522  -1.5625745] b: -0.04366426680660982 cost: 565.3957387879867\niteration: 88101 w: [ 0.241941   0.2297995 -0.740322  -1.5626745] b: -0.043655130509352665 cost: 565.3819127979477\niteration: 88201 w: [ 0.241944   0.2299995 -0.741122  -1.5627745] b: -0.04364597541703816 cost: 565.3680885713421\niteration: 88301 w: [ 0.2419465  0.2301995 -0.741922  -1.5628745] b: -0.04363678732185613 cost: 565.354266415744\niteration: 88401 w: [ 0.2419495  0.2303995 -0.742722  -1.5629745] b: -0.043627592821355386 cost: 565.3404461171352\niteration: 88501 w: [ 0.2419515  0.2305995 -0.743522  -1.5630745] b: -0.043618356211724836 cost: 565.3266282390144\niteration: 88601 w: [ 0.241955   0.2307995 -0.744322  -1.5631745] b: -0.04360912485760081 cost: 565.3128114262324\niteration: 88701 w: [ 0.2419575  0.2309995 -0.745122  -1.5632745] b: -0.043599848481717315 cost: 565.2989969419219\niteration: 88801 w: [ 0.24196    0.2311995 -0.745922  -1.5633745] b: -0.04359056789094995 cost: 565.2851845049095\niteration: 88901 w: [ 0.241963   0.2313995 -0.746722  -1.5634745] b: -0.04358125102465927 cost: 565.271373849579\niteration: 89001 w: [ 0.2419655  0.2315995 -0.747522  -1.5635745] b: -0.043571922294626135 cost: 565.2575652471947\niteration: 89101 w: [ 0.2419685  0.2317995 -0.748322  -1.5636745] b: -0.04356256603531824 cost: 565.2437585196113\niteration: 89201 w: [ 0.241971   0.2319995 -0.749122  -1.5637745] b: -0.04355318698437483 cost: 565.2299537513082\niteration: 89301 w: [ 0.241974   0.2321995 -0.749922  -1.5638745] b: -0.04354379097217741 cost: 565.2161509513413\niteration: 89401 w: [ 0.2419765  0.2323995 -0.750722  -1.5639745] b: -0.04353436124041771 cost: 565.2023500169087\niteration: 89501 w: [ 0.241979   0.2325995 -0.751522  -1.5640745] b: -0.043524926208496365 cost: 565.1885511293257\niteration: 89601 w: [ 0.2419815  0.2327995 -0.752322  -1.5641745] b: -0.04351545162973985 cost: 565.1747542820754\niteration: 89701 w: [ 0.2419845  0.2329995 -0.753122  -1.5642745] b: -0.04350597357477985 cost: 565.1609589925495\niteration: 89801 w: [ 0.241987   0.2331995 -0.753922  -1.5643745] b: -0.04349645743238784 cost: 565.1471659805871\niteration: 89901 w: [ 0.24199    0.2333995 -0.754722  -1.5644745] b: -0.04348693235091929 cost: 565.1333746170727\niteration: 90001 w: [ 0.2419925  0.2335995 -0.755522  -1.5645745] b: -0.04347737574231354 cost: 565.1195854404783\niteration: 90101 w: [ 0.241994   0.2337995 -0.756322  -1.5646285] b: -0.04346779962834007 cost: 565.1059085405564\niteration: 90201 w: [ 0.2419955  0.2339995 -0.757122  -1.5646785] b: -0.04345821872708318 cost: 565.092242110591\niteration: 90301 w: [ 0.241997   0.2341995 -0.757922  -1.5647285] b: -0.04344863015546345 cost: 565.0785765921421\niteration: 90401 w: [ 0.241998   0.2343995 -0.758722  -1.5647785] b: -0.04343900768222378 cost: 565.0649124937082\niteration: 90501 w: [ 0.242      0.2345995 -0.759522  -1.5648285] b: -0.04342937608349749 cost: 565.0512482793849\niteration: 90601 w: [ 0.2420015  0.2347995 -0.760322  -1.5648785] b: -0.04341974556066009 cost: 565.0375854917914\niteration: 90701 w: [ 0.242003   0.2349995 -0.761122  -1.5649285] b: -0.0434100887893934 cost: 565.0239236121013\niteration: 90801 w: [ 0.242004   0.2351995 -0.761922  -1.5649785] b: -0.043400418886425436 cost: 565.0102628991568\niteration: 90901 w: [ 0.242006   0.2353995 -0.762722  -1.5650285] b: -0.043390718365287996 cost: 564.9966025805952\niteration: 91001 w: [ 0.2420075  0.2355995 -0.763522  -1.5650785] b: -0.043381034953649915 cost: 564.9829434346113\niteration: 91101 w: [ 0.2420085  0.2357995 -0.764322  -1.5651285] b: -0.043371319466822544 cost: 564.9692852586478\niteration: 91201 w: [ 0.24201    0.2359995 -0.765122  -1.5651785] b: -0.04336159595208973 cost: 564.9556278664095\niteration: 91301 w: [ 0.2420115  0.2361995 -0.765922  -1.5652285] b: -0.04335184473633115 cost: 564.9419713817019\niteration: 91401 w: [ 0.242013   0.2363995 -0.766722  -1.5652785] b: -0.04334209678837021 cost: 564.9283158105529\niteration: 91501 w: [ 0.242014   0.2365995 -0.767522  -1.5653285] b: -0.043332332069927 cost: 564.9146615513002\niteration: 91601 w: [ 0.242016   0.2367995 -0.768322  -1.5653785] b: -0.043322543660289384 cost: 564.9010073958249\niteration: 91701 w: [ 0.2420175  0.2369995 -0.769122  -1.56543  ] b: -0.04331274571329043 cost: 564.8873511457244\niteration: 91801 w: [ 0.2420195  0.2371995 -0.769922  -1.565495 ] b: -0.043302932070080125 cost: 564.8736653039914\niteration: 91901 w: [ 0.2420215  0.2373995 -0.770722  -1.565595 ] b: -0.04329310011885063 cost: 564.8599021408312\niteration: 92001 w: [ 0.242025   0.2375995 -0.771522  -1.565695 ] b: -0.04328324612871905 cost: 564.8461402767304\niteration: 92101 w: [ 0.242027   0.2377995 -0.772322  -1.565795 ] b: -0.043273371921822575 cost: 564.8323808552801\niteration: 92201 w: [ 0.2420305  0.2379995 -0.773122  -1.565895 ] b: -0.04326347021339266 cost: 564.8186230087731\niteration: 92301 w: [ 0.2420325  0.2381995 -0.773922  -1.565995 ] b: -0.04325355266235703 cost: 564.8048673286175\niteration: 92401 w: [ 0.2420355  0.2383995 -0.774722  -1.566095 ] b: -0.04324360688320746 cost: 564.791113333957\niteration: 92501 w: [ 0.242038   0.2385995 -0.775522  -1.566195 ] b: -0.04323364526379469 cost: 564.7773615612219\niteration: 92601 w: [ 0.242041   0.2387995 -0.776322  -1.566295 ] b: -0.04322365614693717 cost: 564.7636114925101\niteration: 92701 w: [ 0.2420435  0.2389995 -0.777122  -1.566395 ] b: -0.04321365155651646 cost: 564.7498635532531\niteration: 92801 w: [ 0.242046   0.2391995 -0.777922  -1.566495 ] b: -0.04320362092799316 cost: 564.7361176557522\niteration: 92901 w: [ 0.242049   0.2393995 -0.778722  -1.566595 ] b: -0.04319357154936389 cost: 564.7223733045179\niteration: 93001 w: [ 0.2420515  0.2395995 -0.779522  -1.566695 ] b: -0.043183500870779504 cost: 564.7086312408707\niteration: 93101 w: [ 0.2420545  0.2397995 -0.780322  -1.566795 ] b: -0.04317339796419578 cost: 564.6948908133479\niteration: 93201 w: [ 0.242057   0.2399995 -0.781122  -1.566895 ] b: -0.04316328468997496 cost: 564.6811525829081\niteration: 93301 w: [ 0.24206    0.2401995 -0.781922  -1.566995 ] b: -0.043153139554223306 cost: 564.6674160813033\niteration: 93401 w: [ 0.2420625  0.2403995 -0.782722  -1.567095 ] b: -0.04314298368862032 cost: 564.6536816839442\niteration: 93501 w: [ 0.2420655  0.2405995 -0.783522  -1.567195 ] b: -0.043132794141889294 cost: 564.6399491077445\niteration: 93601 w: [ 0.2420675  0.2407995 -0.784322  -1.567295 ] b: -0.04312259277473159 cost: 564.6262190132941\niteration: 93701 w: [ 0.2420705  0.2409995 -0.785122  -1.567395 ] b: -0.04311236355779528 cost: 564.6124900187275\niteration: 93801 w: [ 0.2420735  0.2411995 -0.785922  -1.567495 ] b: -0.04310211560043256 cost: 564.598763159841\niteration: 93901 w: [ 0.242076   0.2413995 -0.786722  -1.567595 ] b: -0.04309184598897121 cost: 564.5850384690128\niteration: 94001 w: [ 0.242079   0.2415995 -0.787522  -1.567695 ] b: -0.043081551810192964 cost: 564.57131553498\niteration: 94101 w: [ 0.2420815  0.2417995 -0.788322  -1.567795 ] b: -0.04307124180852551 cost: 564.5575946772075\niteration: 94201 w: [ 0.2420845  0.2419995 -0.789122  -1.567895 ] b: -0.04306089521888538 cost: 564.5438756666762\niteration: 94301 w: [ 0.2420865  0.2421995 -0.789922  -1.567995 ] b: -0.0430505437387101 cost: 564.5301590850338\niteration: 94401 w: [ 0.24209    0.2423995 -0.790722  -1.568095 ] b: -0.04304015567263773 cost: 564.5164435567576\niteration: 94501 w: [ 0.242092   0.2425995 -0.791522  -1.568195 ] b: -0.0430297605322172 cost: 564.5027307141337\niteration: 94601 w: [ 0.242095   0.2427995 -0.792322  -1.568295 ] b: -0.04301932917231236 cost: 564.4890192097065\niteration: 94701 w: [ 0.242097   0.2429995 -0.793122  -1.5683745] b: -0.043008888920415836 cost: 564.4753490068462\niteration: 94801 w: [ 0.242099   0.2431995 -0.793922  -1.5684245] b: -0.0429984286252257 cost: 564.4617356426851\niteration: 94901 w: [ 0.2421     0.2433995 -0.794722  -1.5684745] b: -0.04298794394435063 cost: 564.4481235257192\niteration: 95001 w: [ 0.2421015  0.2435995 -0.795522  -1.5685245] b: -0.04297745418878878 cost: 564.4345121504667\niteration: 95101 w: [ 0.242103   0.2437995 -0.796322  -1.5685745] b: -0.04296696664485826 cost: 564.4209016883001\niteration: 95201 w: [ 0.2421045  0.2439995 -0.797122  -1.5686245] b: -0.04295644123694536 cost: 564.4072921307824\niteration: 95301 w: [ 0.2421055  0.2441995 -0.797922  -1.5686745] b: -0.04294590747705121 cost: 564.3936839257761\niteration: 95401 w: [ 0.2421075  0.2443995 -0.798722  -1.5687245] b: -0.04293535844378935 cost: 564.3800757468179\niteration: 95501 w: [ 0.242109   0.2445995 -0.799522  -1.5687745] b: -0.0429248105322959 cost: 564.3664689223577\niteration: 95601 w: [ 0.24211    0.2447995 -0.800322  -1.5688245] b: -0.042914231317410435 cost: 564.3528632506319\niteration: 95701 w: [ 0.2421115  0.2449995 -0.801122  -1.5688745] b: -0.0429036386530198 cost: 564.339258176895\niteration: 95801 w: [ 0.2421135  0.2451995 -0.801922  -1.5689245] b: -0.042893040190413546 cost: 564.3256538977456\niteration: 95901 w: [ 0.242115   0.2453995 -0.802722  -1.5689745] b: -0.04288242463570142 cost: 564.3120507093068\niteration: 96001 w: [ 0.242116   0.2455995 -0.803522  -1.5690245] b: -0.04287179818325529 cost: 564.2984484178504\niteration: 96101 w: [ 0.2421175  0.2457995 -0.804322  -1.5690745] b: -0.04286114589667203 cost: 564.2848469817073\niteration: 96201 w: [ 0.242119   0.2459995 -0.805122  -1.5691245] b: -0.04285049546565869 cost: 564.2712464585142\niteration: 96301 w: [ 0.2421205  0.2461995 -0.805922  -1.5691745] b: -0.04283982175149266 cost: 564.2576468429056\niteration: 96401 w: [ 0.242122   0.2463995 -0.806722  -1.5692245] b: -0.042829138600040825 cost: 564.2440481378233\niteration: 96501 w: [ 0.242123   0.2465995 -0.807522  -1.5692745] b: -0.042818422330508775 cost: 564.2304505961251\niteration: 96601 w: [ 0.242125   0.2467995 -0.808322  -1.5693245] b: -0.04280771447778303 cost: 564.2168534529061\niteration: 96701 w: [ 0.242126   0.2469995 -0.809122  -1.5693745] b: -0.042796993181550815 cost: 564.2032576050162\niteration: 96801 w: [ 0.2421275  0.2471995 -0.809922  -1.5694245] b: -0.0427862413197053 cost: 564.1896624702285\niteration: 96901 w: [ 0.242129   0.2473995 -0.810722  -1.5694745] b: -0.04277547747391657 cost: 564.1760682453745\niteration: 97001 w: [ 0.2421305  0.2475995 -0.811522  -1.5695245] b: -0.042764709295739556 cost: 564.1624749320912\niteration: 97101 w: [ 0.242132   0.2477995 -0.812322  -1.5695745] b: -0.04275393496385171 cost: 564.1488825299868\niteration: 97201 w: [ 0.2421335  0.2479995 -0.813122  -1.5696245] b: -0.0427431202319462 cost: 564.1352910316616\niteration: 97301 w: [ 0.2421355  0.248207  -0.8139145 -1.5696955] b: -0.04273230551772971 cost: 564.1217399040174\niteration: 97401 w: [ 0.2421365  0.2484555 -0.814666  -1.569747 ] b: -0.0427214804078523 cost: 564.1086323393199\niteration: 97501 w: [ 0.242138   0.2487055 -0.815416  -1.569797 ] b: -0.04271063782359111 cost: 564.0955428435747\niteration: 97601 w: [ 0.242139   0.2489555 -0.816166  -1.569847 ] b: -0.04269976614457776 cost: 564.0824545437958\niteration: 97701 w: [ 0.2421405  0.2492055 -0.816916  -1.569897 ] b: -0.04268888759656745 cost: 564.0693667172809\niteration: 97801 w: [ 0.242142   0.2494555 -0.817666  -1.569947 ] b: -0.042678017481067865 cost: 564.0562798564747\niteration: 97901 w: [ 0.242143   0.2497055 -0.818416  -1.569997 ] b: -0.04266710843574821 cost: 564.0431940338291\niteration: 98001 w: [ 0.242144   0.2499555 -0.819166  -1.570047 ] b: -0.042656191429996985 cost: 564.0301092683413\niteration: 98101 w: [ 0.2421455  0.2502055 -0.819916  -1.570097 ] b: -0.04264526209258483 cost: 564.0170247847808\niteration: 98201 w: [ 0.242147   0.2504555 -0.820666  -1.570147 ] b: -0.042634326617456024 cost: 564.003941263751\niteration: 98301 w: [ 0.242148   0.2507055 -0.821416  -1.570197 ] b: -0.042623371524695766 cost: 563.9908589750465\niteration: 98401 w: [ 0.2421495  0.2509555 -0.822166  -1.570247 ] b: -0.04261239280825489 cost: 563.9777771215216\niteration: 98501 w: [ 0.242151   0.2512055 -0.822916  -1.570297 ] b: -0.042601414514422384 cost: 563.9646962319683\niteration: 98601 w: [ 0.242152   0.2514555 -0.823666  -1.570347 ] b: -0.042590415147771406 cost: 563.9516164185434\niteration: 98701 w: [ 0.2421535  0.2517055 -0.824416  -1.570397 ] b: -0.04257941146856852 cost: 563.9385372003757\niteration: 98801 w: [ 0.2421545  0.2519555 -0.825166  -1.570447 ] b: -0.04256836886616425 cost: 563.9254591492567\niteration: 98901 w: [ 0.2421565  0.2522055 -0.825916  -1.570497 ] b: -0.04255733434055153 cost: 563.9123816403068\niteration: 99001 w: [ 0.242157   0.2524555 -0.826666  -1.570547 ] b: -0.0425462896746042 cost: 563.8993053214343\niteration: 99101 w: [ 0.2421585  0.2527055 -0.827416  -1.570597 ] b: -0.04253521993207394 cost: 563.8862294425503\niteration: 99201 w: [ 0.2421595  0.2529555 -0.828166  -1.570647 ] b: -0.04252413312891034 cost: 563.873154925134\niteration: 99301 w: [ 0.242161   0.2532055 -0.828916  -1.570697 ] b: -0.04251303764582283 cost: 563.8600807174487\niteration: 99401 w: [ 0.2421625  0.2534555 -0.829666  -1.570747 ] b: -0.04250194477709257 cost: 563.8470074742296\niteration: 99501 w: [ 0.2421635  0.2537055 -0.830416  -1.570797 ] b: -0.04249081189654357 cost: 563.8339354312387\niteration: 99601 w: [ 0.242165   0.2539555 -0.831166  -1.570847 ] b: -0.04247967325288792 cost: 563.8208638547225\niteration: 99701 w: [ 0.2421665  0.2542055 -0.831916  -1.570897 ] b: -0.04246853139665554 cost: 563.8077932413789\niteration: 99801 w: [ 0.242168   0.2544555 -0.832666  -1.570947 ] b: -0.042457368840363024 cost: 563.7947235873087\niteration: 99901 w: [ 0.2421685  0.2547055 -0.833416  -1.570997 ] b: -0.042446192506948185 cost: 563.7816551667245\niteration: 100000 w: [ 0.24217    0.254953  -0.8341585 -1.5710465] b: -0.04243509994948715 cost: 563.768717592519\n\n\n\n\n\n\nprint(\"After 100000 iterations\")\nprint(\"cost:\", cost)\n\nAfter 100000 iterations\ncost: 563.768717592519\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nplt.plot(np.arange(1,100001,1), cost_arr)\nplt.title(\"Cost vs Number of iterations\")\nplt.xlabel(\"Number of iterations\")\nplt.ylabel(\"Cost\")\nplt.show()"
  },
  {
    "objectID": "posts/mlalgos/gradient_descent_multivariable_vectorized_from_scratch.html#gradient-descent-for-multiple-features",
    "href": "posts/mlalgos/gradient_descent_multivariable_vectorized_from_scratch.html#gradient-descent-for-multiple-features",
    "title": "Vectorized Multivariable Linear Regression from Scratch",
    "section": "",
    "text": "The following describes in detail the vectorized imiplementation of the algorithm of gradient descent for mltiple features represented by the vector x.\n\n\n\ndef cost_func(x,y,w,b):\n  cost = 0\n  n_points = y.shape[0]\n  for i in range(n_points):\n    f = np.dot(w,x[i])+b\n    cost += (f -y[i])**2\n  cost = cost/(2*n_points)\n  return cost\n\n\n\n\n\ndef derivative_func(x,y,w,b):\n  n_points = y.shape[0]\n  n_features = x[0].shape[0]\n  dw = np.array([0]*n_features)\n  db = 0\n  for i in range(n_points):\n    f = np.dot(w,x[i]) + b\n    for j in range(n_features):\n      dw[j] += (f - y[i])*x[i][j]/n_points\n    db += (f - y[i])/n_points\n  return dw, db\n\n\n\n\n\ndef grad_desc(x,y,w,b,a,n,derivative_func, cost_func):\n  cost_arr = []\n  for i in range(n):\n    dw,db = derivative_func(x,y,w,b)\n    w = w - a * dw\n    b = b - a * db\n    cost = cost_func(x,y,w,b)\n    cost_arr.append(cost)\n    if(i%100==0 or i==n-1):\n      print(\"iteration:\",i+1,\"w:\", w,\"b:\", b, \"cost:\", cost)\n  return w, b, cost, cost_arr\n\n\n\n\n\nimport numpy as np\nx_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\ny_train = np.array([460, 232, 178])\nw = np.array([0,0,0,0])\nb = 0\na = 0.0000005\nn = 100000\n\n\nw,b,cost,cost_arr = grad_desc(x_train, y_train, w, b, a, n, derivative_func, cost_func)\n\niteration: 1 w: [2.413345e-01 5.580000e-04 1.830000e-04 6.034500e-03] b: 0.000145 cost: 2529.445907451492\niteration: 101 w: [ 0.2023515  0.0007205 -0.000998  -0.0021815] b: -0.00011984671618528004 cost: 695.9930856678557\niteration: 201 w: [ 0.2025335  0.0009705 -0.002148  -0.009355 ] b: -0.00035963602268744473 cost: 694.9289935685856\niteration: 301 w: [ 0.202715   0.0012205 -0.003298  -0.016518 ] b: -0.0005982983702994033 cost: 693.8710622039179\niteration: 401 w: [ 0.202896  0.001445 -0.004448 -0.023628] b: -0.0008358359588454846 cost: 692.8254981266464\niteration: 501 w: [ 0.2030755  0.001675  -0.005568  -0.0307165] b: -0.0010722555533169366 cost: 691.7882133976278\niteration: 601 w: [ 0.2032545  0.001925  -0.006668  -0.037777 ] b: -0.001307562678074441 cost: 690.7597559872951\niteration: 701 w: [ 0.203433   0.002175  -0.007768  -0.0447935] b: -0.001541761106294673 cost: 689.7419977161079\niteration: 801 w: [ 0.203611   0.002425  -0.008868  -0.0517985] b: -0.0017748583098112362 cost: 688.7303041602801\niteration: 901 w: [ 0.203787  0.002675 -0.009968 -0.05875 ] b: -0.002006856060060288 cost: 687.7305146449748\niteration: 1001 w: [ 0.203963   0.002925  -0.011068  -0.0656845] b: -0.0022377620291096846 cost: 686.7374556983464\niteration: 1101 w: [ 0.204138  0.003175 -0.012168 -0.072588] b: -0.0024675777448512647 cost: 685.7530337759337\niteration: 1201 w: [ 0.204312   0.003425  -0.013268  -0.0794485] b: -0.0026963111772917827 cost: 684.7788423460347\niteration: 1301 w: [ 0.204486  0.003675 -0.014368 -0.086298] b: -0.0029239663570966323 cost: 683.810415310279\niteration: 1401 w: [ 0.2046585  0.0039135 -0.015468  -0.0930935] b: -0.0031505485097711224 cost: 682.8536764534834\niteration: 1501 w: [ 0.20483    0.0041135 -0.016568  -0.0998695] b: -0.0033760631581245247 cost: 681.9040178588283\niteration: 1601 w: [ 0.205002   0.004328  -0.0176535 -0.106625 ] b: -0.0036005085376852666 cost: 680.9615129263806\niteration: 1701 w: [ 0.205172   0.004578  -0.0187035 -0.1133365] b: -0.003823897765948446 cost: 680.0296293812801\niteration: 1801 w: [ 0.2053415  0.004828  -0.0197535 -0.120036 ] b: -0.004046235485751484 cost: 679.1034286909584\niteration: 1901 w: [ 0.205511   0.005078  -0.0208035 -0.1266875] b: -0.004267518384910261 cost: 678.187686912896\niteration: 2001 w: [ 0.205679   0.005328  -0.0218535 -0.133312 ] b: -0.004487759825311172 cost: 677.2795368977503\niteration: 2101 w: [ 0.2058465  0.005596  -0.0229035 -0.1399215] b: -0.004706960723743415 cost: 676.3772234638445\niteration: 2201 w: [ 0.206013   0.005896  -0.0239535 -0.1464775] b: -0.004925125624179499 cost: 675.4856970809177\niteration: 2301 w: [ 0.206179   0.006196  -0.0250035 -0.153025 ] b: -0.005142256578274909 cost: 674.5991745576366\niteration: 2401 w: [ 0.206344   0.006496  -0.0260535 -0.15954  ] b: -0.005358360241940179 cost: 673.7207726269572\niteration: 2501 w: [ 0.2065085  0.0067745 -0.027125  -0.1660135] b: -0.0055734435777244845 cost: 672.8512633960598\niteration: 2601 w: [ 0.206672   0.0070245 -0.028225  -0.1724735] b: -0.0057875101964774515 cost: 671.9868482788764\niteration: 2701 w: [ 0.206835   0.0072745 -0.029325  -0.1788865] b: -0.006000561348086028 cost: 671.1322623266645\niteration: 2801 w: [ 0.2069975  0.0075245 -0.030425  -0.185284 ] b: -0.006212606472340106 cost: 670.2833886731445\niteration: 2901 w: [ 0.207159   0.0077745 -0.031525  -0.191667 ] b: -0.00642364451026305 cost: 669.4400745262191\niteration: 3001 w: [ 0.2073195  0.0080245 -0.032625  -0.197989 ] b: -0.0066336814964713135 cost: 668.6081996289139\niteration: 3101 w: [ 0.207479   0.0082745 -0.033725  -0.2042985] b: -0.006842726206046504 cost: 667.7815296096983\niteration: 3201 w: [ 0.207639   0.0085245 -0.034825  -0.2105845] b: -0.007050775211079575 cost: 666.9614207743449\niteration: 3301 w: [ 0.2077975  0.0087745 -0.035925  -0.2168315] b: -0.007257836447118926 cost: 666.1497838167132\niteration: 3401 w: [ 0.207956   0.0090245 -0.037025  -0.223065 ] b: -0.007463916293745738 cost: 665.3433667135956\niteration: 3501 w: [ 0.2081135  0.0092745 -0.038125  -0.229261 ] b: -0.0076690160776340536 cost: 664.5451307273105\niteration: 3601 w: [ 0.2082695  0.0095245 -0.039225  -0.2354185] b: -0.007873146832570208 cost: 663.7551382261652\niteration: 3701 w: [ 0.2084255  0.0097745 -0.040325  -0.2415725] b: -0.008076305656634936 cost: 662.9690130199062\niteration: 3801 w: [ 0.208581   0.0100245 -0.041425  -0.2476855] b: -0.008278497331775565 cost: 662.1913455782793\niteration: 3901 w: [ 0.208735   0.010317  -0.0424825 -0.2537675] b: -0.008479729273751815 cost: 661.4215053114478\niteration: 4001 w: [ 0.2088885  0.010617  -0.0435325 -0.259838 ] b: -0.008680002444220802 cost: 660.6565300223458\niteration: 4101 w: [ 0.209042   0.010917  -0.0445825 -0.265858 ] b: -0.008879322789329899 cost: 659.9009983958877\niteration: 4201 w: [ 0.209194   0.011217  -0.0456325 -0.271867 ] b: -0.009077697385785134 cost: 659.1500791236687\niteration: 4301 w: [ 0.209346   0.011517  -0.0466825 -0.2778635] b: -0.009275126539940259 cost: 658.4039296265429\niteration: 4401 w: [ 0.209496   0.011817  -0.0477325 -0.283797 ] b: -0.00947161669159306 cost: 657.6685605755812\niteration: 4501 w: [ 0.2096465  0.012117  -0.0487825 -0.289721 ] b: -0.00966717154421513 cost: 656.9375086290565\niteration: 4601 w: [ 0.2097965  0.012417  -0.0498325 -0.2956425] b: -0.009861794300524621 cost: 656.2099347808465\niteration: 4701 w: [ 0.209945   0.012717  -0.0508825 -0.3015015] b: -0.010055491971623896 cost: 655.4929004964749\niteration: 4801 w: [ 0.2100935  0.013008  -0.0519325 -0.3073465] b: -0.010248264069244464 cost: 654.7806761471173\niteration: 4901 w: [ 0.2102415  0.013258  -0.0529825 -0.3131835] b: -0.010440117224497688 cost: 654.0727457961598\niteration: 5001 w: [ 0.210388   0.013508  -0.0540325 -0.3189695] b: -0.010631055309118353 cost: 653.373832197853\niteration: 5101 w: [ 0.2105345  0.013758  -0.0550825 -0.3247415] b: -0.010821084725986743 cost: 652.6795723436072\niteration: 5201 w: [ 0.21068    0.014008  -0.0561325 -0.3304895] b: -0.011010203536947682 cost: 651.9910995793263\niteration: 5301 w: [ 0.210825   0.0142615 -0.0571825 -0.336195 ] b: -0.011198420491851319 cost: 651.3104716657031\niteration: 5401 w: [ 0.21097    0.0145615 -0.0582325 -0.341889 ] b: -0.011385740109903942 cost: 650.6338118623238\niteration: 5501 w: [ 0.2111135  0.0148615 -0.0592825 -0.3475665] b: -0.011572162149229777 cost: 649.9619720741375\niteration: 5601 w: [ 0.2112565  0.0151615 -0.0603325 -0.353189 ] b: -0.011757693805196872 cost: 649.2992767667502\niteration: 5701 w: [ 0.211399   0.0154615 -0.0613825 -0.3588115] b: -0.011942339549135714 cost: 648.6394511780375\niteration: 5801 w: [ 0.211541   0.015804  -0.0624325 -0.36442  ] b: -0.012126100113712401 cost: 647.9837941514609\niteration: 5901 w: [ 0.211682   0.016154  -0.0634825 -0.3699655] b: -0.012308980410778101 cost: 647.3379762581632\niteration: 6001 w: [ 0.211822   0.016504  -0.0645325 -0.3755155] b: -0.012490987986764828 cost: 646.6944523563071\niteration: 6101 w: [ 0.2119625  0.016825  -0.0655825 -0.3810445] b: -0.012672123409456514 cost: 646.0562288339962\niteration: 6201 w: [ 0.2121005  0.017125  -0.0666325 -0.3865165] b: -0.012852392093039107 cost: 645.4271963544116\niteration: 6301 w: [ 0.212239   0.017425  -0.0676825 -0.391982 ] b: -0.013031797225555575 cost: 644.8015924971579\niteration: 6401 w: [ 0.212378   0.017725  -0.0687325 -0.39744  ] b: -0.013210340216809507 cost: 644.1795163791699\niteration: 6501 w: [ 0.212515   0.018025  -0.0697825 -0.402849 ] b: -0.013388026728848293 cost: 643.5654740729954\niteration: 6601 w: [ 0.212652   0.018325  -0.0708325 -0.4082395] b: -0.013564858977700602 cost: 642.9560887832505\niteration: 6701 w: [ 0.2127885  0.018625  -0.0718825 -0.4136275] b: -0.01374084259258523 cost: 642.3496126278868\niteration: 6801 w: [ 0.212924   0.018925  -0.0729325 -0.418967 ] b: -0.01391598386789509 cost: 641.750973972813\niteration: 6901 w: [ 0.2130585  0.019225  -0.0739825 -0.4242755] b: -0.014090284070279208 cost: 641.1582356076469\niteration: 7001 w: [ 0.213193   0.019525  -0.0750325 -0.4295835] b: -0.014263746409743042 cost: 640.568109942329\niteration: 7101 w: [ 0.213327   0.019825  -0.0760825 -0.4348655] b: -0.014436375252410026 cost: 639.9832945934037\niteration: 7201 w: [ 0.2134595  0.020125  -0.0771325 -0.4401045] b: -0.014608172944214843 cost: 639.4055458161864\niteration: 7301 w: [ 0.2135925  0.020425  -0.0781825 -0.445343 ] b: -0.01477914443959268 cost: 638.8303412585573\niteration: 7401 w: [ 0.213725   0.020712  -0.0792325 -0.450556 ] b: -0.014949292091960484 cost: 638.2603743385561\niteration: 7501 w: [ 0.213856   0.020962  -0.0802345 -0.455718 ] b: -0.015118622477034573 cost: 637.6994142844079\niteration: 7601 w: [ 0.213987   0.021212  -0.0812345 -0.460876 ] b: -0.015287140198373884 cost: 637.141328724567\niteration: 7701 w: [ 0.2141175  0.021462  -0.0822345 -0.4660195] b: -0.015454844998592595 cost: 636.5871528919143\niteration: 7801 w: [ 0.214247   0.021717  -0.0832295 -0.4711345] b: -0.015621740978801963 cost: 636.0383759096808\niteration: 7901 w: [ 0.214376   0.022017  -0.0841795 -0.4762155] b: -0.01578783423794423 cost: 635.4960978953343\niteration: 8001 w: [ 0.214504   0.022317  -0.0851295 -0.481276 ] b: -0.015953127487535804 cost: 634.9582480210778\niteration: 8101 w: [ 0.2146325  0.022617  -0.0860795 -0.4863355] b: -0.016117622136583992 cost: 634.4228215205202\niteration: 8201 w: [ 0.214759   0.022917  -0.0870295 -0.4913435] b: -0.01628132562817647 cost: 633.8949096425367\niteration: 8301 w: [ 0.214886   0.023217  -0.0879795 -0.4963405] b: -0.016444239940155186 cost: 633.3703751730842\niteration: 8401 w: [ 0.2150125  0.023517  -0.0889295 -0.501336 ] b: -0.016606366647562168 cost: 632.8482581924086\niteration: 8501 w: [ 0.2151375  0.023817  -0.0898795 -0.506274 ] b: -0.016767709530766145 cost: 632.3341237830863\niteration: 8601 w: [ 0.215262   0.024117  -0.0908295 -0.5111985] b: -0.016928273313438692 cost: 631.8235400798213\niteration: 8701 w: [ 0.215387   0.024417  -0.0917795 -0.516123 ] b: -0.01708806319070792 cost: 631.3151561998055\niteration: 8801 w: [ 0.215511   0.024713  -0.0927335 -0.521015 ] b: -0.01724707952602148 cost: 630.8121060432607\niteration: 8901 w: [ 0.2156345  0.024963  -0.0937335 -0.525869 ] b: -0.017405325612922583 cost: 630.3142816967135\niteration: 9001 w: [ 0.2157575  0.025213  -0.0947335 -0.5307105] b: -0.017562809157736888 cost: 629.8198151695871\niteration: 9101 w: [ 0.21588    0.025463  -0.0957335 -0.5355455] b: -0.017719526577670713 cost: 629.3281094657372\niteration: 9201 w: [ 0.216002   0.025713  -0.0967335 -0.5403505] b: -0.017875484390204293 cost: 628.8414195222083\niteration: 9301 w: [ 0.2161225  0.025963  -0.0977335 -0.545109 ] b: -0.018030688326887326 cost: 628.3612912918138\niteration: 9401 w: [ 0.2162435  0.026213  -0.0987335 -0.5498765] b: -0.01818514099529271 cost: 627.8823616070082\niteration: 9501 w: [ 0.2163645  0.026463  -0.0997335 -0.55463  ] b: -0.01833884113036277 cost: 627.406829126374\niteration: 9601 w: [ 0.2164835  0.026713  -0.1007335 -0.55933  ] b: -0.018491796348226376 cost: 626.9384182136115\niteration: 9701 w: [ 0.2166025  0.026963  -0.1017335 -0.5640205] b: -0.0186440142509367 cost: 626.4729114274149\niteration: 9801 w: [ 0.2167215  0.0272455 -0.1027335 -0.5686955] b: -0.018795488411498525 cost: 626.0106572418074\niteration: 9901 w: [ 0.2168395  0.0275455 -0.1037335 -0.5733615] b: -0.018946223421659778 cost: 625.5511251713053\niteration: 10001 w: [ 0.2169565  0.0278455 -0.1047335 -0.5779825] b: -0.019096231305416184 cost: 625.0977588586564\niteration: 10101 w: [ 0.217073   0.0281555 -0.1057335 -0.582588 ] b: -0.019245509266673105 cost: 624.6477093062289\niteration: 10201 w: [ 0.2171905  0.0285055 -0.1067335 -0.5871965] b: -0.019394061233485523 cost: 624.1990628401919\niteration: 10301 w: [ 0.2173055  0.0288555 -0.1077335 -0.591755 ] b: -0.019541889810656344 cost: 623.756940828035\niteration: 10401 w: [ 0.217421   0.0291565 -0.1087335 -0.596304 ] b: -0.019688998492371303 cost: 623.3178810248277\niteration: 10501 w: [ 0.2175355  0.0294565 -0.1097335 -0.600827 ] b: -0.01983539125795989 cost: 622.883076942993\niteration: 10601 w: [ 0.2176505  0.0297565 -0.1107335 -0.60536  ] b: -0.019981068756584914 cost: 622.4492294594884\niteration: 10701 w: [ 0.217764   0.0300565 -0.1117335 -0.609837 ] b: -0.020126033527300156 cost: 622.0223009308603\niteration: 10801 w: [ 0.217877   0.0303565 -0.1127335 -0.614291 ] b: -0.020270294817821766 cost: 621.5992617284829\niteration: 10901 w: [ 0.21799    0.0306565 -0.1137335 -0.618751 ] b: -0.02041385061047308 cost: 621.1774937822787\niteration: 11001 w: [ 0.2181025  0.0309565 -0.1147335 -0.623193 ] b: -0.020556707946261358 cost: 620.7591386769445\niteration: 11101 w: [ 0.218214   0.0312565 -0.1157335 -0.627589 ] b: -0.02069886562388622 cost: 620.346653647179\niteration: 11201 w: [ 0.2183255  0.0315565 -0.1167335 -0.631963 ] b: -0.02084033257743231 cost: 619.9378677314106\niteration: 11301 w: [ 0.218436   0.0318565 -0.1177335 -0.6363535] b: -0.020981104941769094 cost: 619.5293800742135\niteration: 11401 w: [ 0.2185465  0.0321565 -0.1187335 -0.6407095] b: -0.02112118541533878 cost: 619.1256610414683\niteration: 11501 w: [ 0.218656   0.0324565 -0.1197335 -0.645023 ] b: -0.021260582653791514 cost: 618.7273677374055\niteration: 11601 w: [ 0.218765   0.0327565 -0.1207335 -0.6493335] b: -0.021399300515166655 cost: 618.3310315353523\niteration: 11701 w: [ 0.2188745  0.0330565 -0.1217335 -0.6536335] b: -0.021537336933569844 cost: 617.937292002899\niteration: 11801 w: [ 0.218983   0.0333565 -0.1227335 -0.657916 ] b: -0.021674693962655475 cost: 617.5467414903138\niteration: 11901 w: [ 0.2190905  0.0336565 -0.1237335 -0.6621515] b: -0.0218113813877972 cost: 617.1618783685964\niteration: 12001 w: [ 0.2191975  0.0339175 -0.1247335 -0.6663905] b: -0.021947397814013597 cost: 616.7785953545737\niteration: 12101 w: [ 0.2193045  0.0341675 -0.1257335 -0.6706005] b: -0.022082747048806018 cost: 616.3994721755062\niteration: 12201 w: [ 0.219412   0.0344175 -0.1267335 -0.6748215] b: -0.02221742695768332 cost: 616.0210374030172\niteration: 12301 w: [ 0.2195175  0.0346675 -0.1277335 -0.6789725] b: -0.02235145143549944 cost: 615.6501030944033\niteration: 12401 w: [ 0.2196225  0.0349175 -0.1287335 -0.6831335] b: -0.0224848178827133 cost: 615.2799070611549\niteration: 12501 w: [ 0.219728   0.0351675 -0.1297335 -0.6872765] b: -0.022617527643841392 cost: 614.9127850177091\niteration: 12601 w: [ 0.219833   0.0354175 -0.1307335 -0.6914245] b: -0.0227495818960631 cost: 614.5468166786753\niteration: 12701 w: [ 0.219937   0.0357035 -0.1316975 -0.695498 ] b: -0.022880989178960497 cost: 614.1890482651543\niteration: 12801 w: [ 0.2200405  0.0360035 -0.1326475 -0.69959  ] b: -0.023011749972109922 cost: 613.8314674589074\niteration: 12901 w: [ 0.2201435  0.0363035 -0.1335975 -0.703653 ] b: -0.023141865970558107 cost: 613.4777823447339\niteration: 13001 w: [ 0.220247   0.0366035 -0.1345475 -0.707723 ] b: -0.02327133873010081 cost: 613.1250323657525\niteration: 13101 w: [ 0.2203485  0.0369035 -0.1354975 -0.711743 ] b: -0.02340017468047057 cost: 612.7778432097031\niteration: 13201 w: [ 0.2204495  0.0372035 -0.1364475 -0.715743 ] b: -0.023528377632724386 cost: 612.4337408013147\niteration: 13301 w: [ 0.220551   0.0375035 -0.1373975 -0.719737 ] b: -0.023655947599212898 cost: 612.0915776312164\niteration: 13401 w: [ 0.220652   0.0378035 -0.1383475 -0.723737 ] b: -0.023782887371431118 cost: 611.7503919077939\niteration: 13501 w: [ 0.2207525  0.0381035 -0.1392975 -0.727682 ] b: -0.0239091993245801 cost: 611.4150384762629\niteration: 13601 w: [ 0.220852   0.0384035 -0.1402475 -0.731619 ] b: -0.02403488506694832 cost: 611.0817408338819\niteration: 13701 w: [ 0.2209515  0.0386875 -0.1411975 -0.735535 ] b: -0.024159953743669894 cost: 610.7516098278452\niteration: 13801 w: [ 0.2210505  0.0389375 -0.1421475 -0.739453 ] b: -0.024284399432392856 cost: 610.4229305640621\niteration: 13901 w: [ 0.2211495  0.0391875 -0.1430975 -0.7433555] b: -0.02440823061268634 cost: 610.0968620724638\niteration: 14001 w: [ 0.2212475  0.0394375 -0.1440475 -0.7472055] b: -0.024531448498309936 cost: 609.7762667281536\niteration: 14101 w: [ 0.221345   0.0396875 -0.1449975 -0.75105  ] b: -0.024654058484439773 cost: 609.4574515351217\niteration: 14201 w: [ 0.221443   0.0399375 -0.1459475 -0.7549   ] b: -0.024776058627795857 cost: 609.1395605427014\niteration: 14301 w: [ 0.2215395  0.0401875 -0.1468975 -0.7587235] b: -0.02489745463311686 cost: 608.8250590471938\niteration: 14401 w: [ 0.2216365  0.0404375 -0.1478475 -0.7625305] b: -0.025018246447289245 cost: 608.513150635224\niteration: 14501 w: [ 0.221732   0.0406875 -0.1487975 -0.7662865] b: -0.0251384419363099 cost: 608.2064466004118\niteration: 14601 w: [ 0.221828   0.0409375 -0.1497475 -0.7700675] b: -0.02525803949500909 cost: 607.899137813877\niteration: 14701 w: [ 0.2219235  0.0411875 -0.1506975 -0.773818 ] b: -0.025377043527862346 cost: 607.5954341258337\niteration: 14801 w: [ 0.222018   0.0414375 -0.1516475 -0.77757  ] b: -0.025495452376246567 cost: 607.2929046248208\niteration: 14901 w: [ 0.222112   0.0416875 -0.1525975 -0.78127  ] b: -0.025613273671796284 cost: 606.9955455849491\niteration: 15001 w: [ 0.222206   0.0419375 -0.1535475 -0.7849655] b: -0.025730508955738533 cost: 606.6997721135755\niteration: 15101 w: [ 0.222299   0.0421875 -0.1544975 -0.7886515] b: -0.02584716140403884 cost: 606.4059523084273\niteration: 15201 w: [ 0.2223935  0.0424375 -0.1554475 -0.7923395] b: -0.025963231464783777 cost: 606.113223033845\niteration: 15301 w: [ 0.222485   0.0427205 -0.1563645 -0.7959785] b: -0.026078719669841502 cost: 605.825774379996\niteration: 15401 w: [ 0.2225775  0.0430205 -0.1572645 -0.799608 ] b: -0.026193634847617844 cost: 605.5404515207609\niteration: 15501 w: [ 0.2226685  0.0433205 -0.1581645 -0.803218 ] b: -0.02630797824718566 cost: 605.2577528413842\niteration: 15601 w: [ 0.22276    0.0436465 -0.1590645 -0.8068355] b: -0.02642174994469171 cost: 604.975542649424\niteration: 15701 w: [ 0.2228515  0.0439465 -0.1599645 -0.8104355] b: -0.026534950684120408 cost: 604.6959461327008\niteration: 15801 w: [ 0.222942   0.0442465 -0.1608645 -0.813985 ] b: -0.02664758577218656 cost: 604.4211587364466\niteration: 15901 w: [ 0.223032   0.0445465 -0.1617645 -0.817535 ] b: -0.026759661095677168 cost: 604.1474876756603\niteration: 16001 w: [ 0.223121   0.0448465 -0.1626645 -0.82106  ] b: -0.02687117796326525 cost: 603.8767475121662\niteration: 16101 w: [ 0.223211   0.0451465 -0.1635645 -0.8246095] b: -0.02698213300753872 cost: 603.6054067327044\niteration: 16201 w: [ 0.2232995  0.0454465 -0.1644645 -0.828118 ] b: -0.02709253179619542 cost: 603.338109411341\niteration: 16301 w: [ 0.2233885  0.0457465 -0.1653645 -0.831605 ] b: -0.02720237706930822 cost: 603.0734448329539\niteration: 16401 w: [ 0.2234755  0.0460465 -0.1662645 -0.8350605] b: -0.02731167200178289 cost: 602.8120955019917\niteration: 16501 w: [ 0.2235635  0.0463465 -0.1671645 -0.838531 ] b: -0.027420418152392365 cost: 602.5507931241274\niteration: 16601 w: [ 0.2236505  0.0466465 -0.1680645 -0.841981 ] b: -0.027528617355603375 cost: 602.2920132344569\niteration: 16701 w: [ 0.223738   0.0469465 -0.1689645 -0.845419 ] b: -0.02763626984870761 cost: 602.0351472733996\niteration: 16801 w: [ 0.2238245  0.0472465 -0.1698645 -0.848819 ] b: -0.02774338257091851 cost: 601.781971731565\niteration: 16901 w: [ 0.2239095  0.0475465 -0.1707645 -0.852201 ] b: -0.027849960070771652 cost: 601.5310860833888\niteration: 17001 w: [ 0.2239955  0.0478465 -0.1716645 -0.855594 ] b: -0.027956000095674418 cost: 601.280496791055\niteration: 17101 w: [ 0.2240815  0.0481465 -0.1725645 -0.858969 ] b: -0.028061503217908484 cost: 601.0321791382336\niteration: 17201 w: [ 0.2241665  0.0484465 -0.1734645 -0.862335 ] b: -0.02816647496249229 cost: 600.785512290176\niteration: 17301 w: [ 0.2242505  0.0487465 -0.1743645 -0.8656545] b: -0.028270915757273776 cost: 600.5430041094243\niteration: 17401 w: [ 0.2243345  0.0490465 -0.1752645 -0.8689625] b: -0.028374833813324698 cost: 600.302272665091\niteration: 17501 w: [ 0.2244185  0.0493465 -0.1761645 -0.872278 ] b: -0.028478226503474452 cost: 600.0620443034203\niteration: 17601 w: [ 0.224502   0.0496465 -0.1770645 -0.875578 ] b: -0.028581094737951146 cost: 599.8238493606705\niteration: 17701 w: [ 0.2245855  0.0499075 -0.1780035 -0.878873 ] b: -0.028683441628782586 cost: 599.5864780048678\niteration: 17801 w: [ 0.2246685  0.0501575 -0.1789535 -0.882123 ] b: -0.028785270033620674 cost: 599.3529134291119\niteration: 17901 w: [ 0.2247505  0.0504075 -0.1799035 -0.885349 ] b: -0.028886586852719803 cost: 599.1218874398852\niteration: 18001 w: [ 0.2248325  0.0506575 -0.1808535 -0.888599 ] b: -0.0289873872507459 cost: 598.8902552467946\niteration: 18101 w: [ 0.224914   0.0509075 -0.1818035 -0.891805 ] b: -0.029087676851767906 cost: 598.6624413867178\niteration: 18201 w: [ 0.224996   0.0511575 -0.1827535 -0.895052 ] b: -0.029187455423271568 cost: 598.4329274248081\niteration: 18301 w: [ 0.225077   0.0514075 -0.1837035 -0.898213 ] b: -0.029286726831094834 cost: 598.2098997223087\niteration: 18401 w: [ 0.225157   0.0516575 -0.1846535 -0.9013855] b: -0.029385497944038615 cost: 597.9870568240138\niteration: 18501 w: [ 0.2252375  0.0519075 -0.1856035 -0.90454  ] b: -0.029483766806903622 cost: 597.7662813098792\niteration: 18601 w: [ 0.2253175  0.0521575 -0.1865535 -0.90769  ] b: -0.029581535070724117 cost: 597.5467052503907\niteration: 18701 w: [ 0.225397   0.0524075 -0.1875035 -0.9108425] b: -0.029678803236851573 cost: 597.3278836386268\niteration: 18801 w: [ 0.2254765  0.0526575 -0.1884535 -0.913966 ] b: -0.029775573873902755 cost: 597.1117936514489\niteration: 18901 w: [ 0.2255545  0.0529525 -0.1893585 -0.917044 ] b: -0.029871854940054024 cost: 596.9000182722007\niteration: 19001 w: [ 0.2256335  0.0532525 -0.1902585 -0.920144 ] b: -0.02996764381481427 cost: 596.6878057834251\niteration: 19101 w: [ 0.2257105  0.0535525 -0.1911585 -0.923202 ] b: -0.030062947666483684 cost: 596.4790787932457\niteration: 19201 w: [ 0.2257895  0.0538525 -0.1920585 -0.926298 ] b: -0.030157760385747162 cost: 596.268863960318\niteration: 19301 w: [ 0.2258665  0.0541525 -0.1929585 -0.9293565] b: -0.03025208707109722 cost: 596.0618328563316\niteration: 19401 w: [ 0.2259435  0.0544525 -0.1938585 -0.932372 ] b: -0.030345931795073677 cost: 595.8582870482653\niteration: 19501 w: [ 0.22602    0.0547525 -0.1947585 -0.935387 ] b: -0.03043929869763318 cost: 595.6556073043978\niteration: 19601 w: [ 0.2260955  0.0550525 -0.1956585 -0.938387 ] b: -0.030532190708298445 cost: 595.4546716581052\niteration: 19701 w: [ 0.2261715  0.0553525 -0.1965585 -0.9413945] b: -0.030624605111184526 cost: 595.2541097034198\niteration: 19801 w: [ 0.226248   0.0556525 -0.1974585 -0.9443945] b: -0.03071654391543985 cost: 595.0548272177073\niteration: 19901 w: [ 0.226323   0.055948  -0.1983585 -0.947353 ] b: -0.030808008508034515 cost: 594.8588797489125\niteration: 20001 w: [ 0.2263975  0.056198  -0.1992585 -0.950303 ] b: -0.03089900821608077 cost: 594.6645143205441\niteration: 20101 w: [ 0.2264715  0.056448  -0.2001585 -0.953236 ] b: -0.030989540724734088 cost: 594.4719559722831\niteration: 20201 w: [ 0.226546   0.056698  -0.2010585 -0.9561575] b: -0.031079610285199125 cost: 594.2808641987467\niteration: 20301 w: [ 0.226621   0.056948  -0.2019585 -0.959099 ] b: -0.031169212585408033 cost: 594.089382321483\niteration: 20401 w: [ 0.2266945  0.057198  -0.2028585 -0.961999 ] b: -0.03125835336460141 cost: 593.9011220856247\niteration: 20501 w: [ 0.226768   0.057448  -0.2037585 -0.9648995] b: -0.031347034380156076 cost: 593.7136055893023\niteration: 20601 w: [ 0.2268405  0.057698  -0.2046585 -0.96775  ] b: -0.03143525829466008 cost: 593.5297588774491\niteration: 20701 w: [ 0.226913   0.057948  -0.2055585 -0.9706005] b: -0.031523030256172514 cost: 593.3466593044674\niteration: 20801 w: [ 0.2269855  0.058198  -0.2064585 -0.973461 ] b: -0.03161034795099629 cost: 593.1637328043008\niteration: 20901 w: [ 0.2270575  0.058448  -0.2073585 -0.976311 ] b: -0.03169721273547193 cost: 592.9821592767\niteration: 21001 w: [ 0.2271295  0.058698  -0.2082585 -0.9791445] b: -0.031783627973511856 cost: 592.8022709309206\niteration: 21101 w: [ 0.2272005  0.058948  -0.2091585 -0.981954 ] b: -0.03186959593254691 cost: 592.6244816666643\niteration: 21201 w: [ 0.227271   0.059198  -0.2100585 -0.9847385] b: -0.031955118891547736 cost: 592.4488285694205\niteration: 21301 w: [ 0.2273415  0.059448  -0.2109585 -0.9875095] b: -0.03204020318925374 cost: 592.2746461288074\niteration: 21401 w: [ 0.227413   0.059698  -0.2118585 -0.990308 ] b: -0.03212484466369454 cost: 592.0996322811992\niteration: 21501 w: [ 0.2274825  0.059948  -0.2127585 -0.993058 ] b: -0.032209045856998644 cost: 591.9280399144474\niteration: 21601 w: [ 0.227553   0.060198  -0.2136585 -0.9958385] b: -0.03229280796740813 cost: 591.7554532343294\niteration: 21701 w: [ 0.227622   0.060448  -0.2145585 -0.9985625] b: -0.03237613245359281 cost: 591.5866948102239\niteration: 21801 w: [ 0.227691   0.060698  -0.2154585 -1.0012625] b: -0.032459028825831876 cost: 591.4199363990265\niteration: 21901 w: [ 0.2277595  0.060948  -0.2163585 -1.003978 ] b: -0.03254149309225528 cost: 591.2530038709505\niteration: 22001 w: [ 0.2278285  0.061198  -0.2172585 -1.006678 ] b: -0.032623529803074224 cost: 591.0875922861928\niteration: 22101 w: [ 0.227897   0.061448  -0.2181585 -1.0093755] b: -0.032705134098729445 cost: 590.922988556733\niteration: 22201 w: [ 0.227965   0.061698  -0.2190585 -1.0120665] b: -0.032786313600360585 cost: 590.759406288281\niteration: 22301 w: [ 0.228033   0.061948  -0.2199585 -1.0147375] b: -0.03286706473063845 cost: 590.5975642687405\niteration: 22401 w: [ 0.228099   0.062198  -0.2208585 -1.0173525] b: -0.03294739682079655 cost: 590.4393748598166\niteration: 22501 w: [ 0.2281665  0.062428  -0.2217585 -1.019995 ] b: -0.03302731262759392 cost: 590.2804691124735\niteration: 22601 w: [ 0.228233   0.062678  -0.2226585 -1.0226305] b: -0.033106810102979764 cost: 590.1224610798586\niteration: 22701 w: [ 0.228299   0.062932  -0.2235585 -1.0252305] b: -0.033185890804676224 cost: 589.9669416345338\niteration: 22801 w: [ 0.228366   0.063232  -0.2244585 -1.0278745] b: -0.03326455330480152 cost: 589.8094626462408\niteration: 22901 w: [ 0.228432   0.063532  -0.2253585 -1.030469 ] b: -0.033342801129786825 cost: 589.6552155308925\niteration: 23001 w: [ 0.2284965  0.063832  -0.2262585 -1.033019 ] b: -0.03342064024019282 cost: 589.5039065495638\niteration: 23101 w: [ 0.2285615  0.064132  -0.2271585 -1.035591 ] b: -0.03349807166603075 cost: 589.3520580402729\niteration: 23201 w: [ 0.2286265  0.064432  -0.2280585 -1.038141 ] b: -0.03357509639770777 cost: 589.2019537850598\niteration: 23301 w: [ 0.2286915  0.064732  -0.2289585 -1.040691 ] b: -0.03365171702513824 cost: 589.0524502184331\niteration: 23401 w: [ 0.2287555  0.065032  -0.2298585 -1.043234 ] b: -0.03372793269256857 cost: 588.9039059229541\niteration: 23501 w: [ 0.22882    0.065332  -0.2307585 -1.045784 ] b: -0.03380374387021662 cost: 588.7556020886944\niteration: 23601 w: [ 0.2288835  0.065632  -0.2316585 -1.048277 ] b: -0.03387915359030647 cost: 588.6107860098186\niteration: 23701 w: [ 0.2289465  0.065932  -0.2325585 -1.0507635] b: -0.03395416925930713 cost: 588.4668727039144\niteration: 23801 w: [ 0.2290095  0.066232  -0.2334585 -1.0532635] b: -0.034028788166007955 cost: 588.3228533495923\niteration: 23901 w: [ 0.229072   0.066532  -0.2343585 -1.0557175] b: -0.034103011966940044 cost: 588.1817101748717\niteration: 24001 w: [ 0.2291345  0.066832  -0.2352585 -1.058194 ] b: -0.03417684460499529 cost: 588.0400048215535\niteration: 24101 w: [ 0.2291975  0.067132  -0.2361585 -1.0606755] b: -0.03425027975225192 cost: 587.8986178886387\niteration: 24201 w: [ 0.2292595  0.067432  -0.2370585 -1.0631255] b: -0.03432332467844226 cost: 587.7593542571486\niteration: 24301 w: [ 0.229321   0.067732  -0.2379585 -1.065549 ] b: -0.03439598191029202 cost: 587.6219467202673\niteration: 24401 w: [ 0.229382   0.068032  -0.2388585 -1.067961 ] b: -0.03446825314321018 cost: 587.4856451352441\niteration: 24501 w: [ 0.229443   0.068332  -0.2397585 -1.070361 ] b: -0.03454014753791199 cost: 587.3504653415265\niteration: 24601 w: [ 0.2295035  0.068632  -0.2406585 -1.072761 ] b: -0.03461165826637223 cost: 587.215819525676\niteration: 24701 w: [ 0.2295645  0.068932  -0.2415585 -1.075161 ] b: -0.03468278609644376 cost: 587.0817057127721\niteration: 24801 w: [ 0.2296255  0.069232  -0.2424585 -1.077561 ] b: -0.03475352957996196 cost: 586.9481249879263\niteration: 24901 w: [ 0.2296855  0.069532  -0.2433585 -1.079932 ] b: -0.034823891889103224 cost: 586.816464124715\niteration: 25001 w: [ 0.2297445  0.069832  -0.2442585 -1.082273 ] b: -0.03489388267399602 cost: 586.6867521192315\niteration: 25101 w: [ 0.2298045  0.070132  -0.2451585 -1.084623 ] b: -0.03496349871261061 cost: 586.5571197797434\niteration: 25201 w: [ 0.229863   0.070432  -0.2460585 -1.0869325] b: -0.0350327430143417 cost: 586.4299103100594\niteration: 25301 w: [ 0.229922   0.070704  -0.2469585 -1.0892605] b: -0.03510162088340405 cost: 586.3024909515779\niteration: 25401 w: [ 0.2299815  0.070954  -0.2478585 -1.0916055] b: -0.035170124101316244 cost: 586.1749080756\niteration: 25501 w: [ 0.23004    0.071204  -0.2487585 -1.0939055] b: -0.035238260838501914 cost: 586.0499285070129\niteration: 25601 w: [ 0.2300985  0.071454  -0.2496585 -1.096214 ] b: -0.03530603098095956 cost: 585.9250456315397\niteration: 25701 w: [ 0.230157   0.071704  -0.2505585 -1.0984995] b: -0.035373432904286434 cost: 585.801716711953\niteration: 25801 w: [ 0.2302135  0.071954  -0.2514585 -1.1007495] b: -0.03544047941469967 cost: 585.6805035609458\niteration: 25901 w: [ 0.230271   0.072204  -0.2523585 -1.1029995] b: -0.03550716566380518 cost: 585.5597585295052\niteration: 26001 w: [ 0.2303285  0.072454  -0.2532585 -1.105257 ] b: -0.035573490160500036 cost: 585.4391421194335\niteration: 26101 w: [ 0.230385   0.072704  -0.2541585 -1.107507 ] b: -0.035639455878560135 cost: 585.3193393114123\niteration: 26201 w: [ 0.2304425  0.072954  -0.2550585 -1.109757 ] b: -0.03570506138918167 cost: 585.2000043863312\niteration: 26301 w: [ 0.2304985  0.073204  -0.2559585 -1.111981 ] b: -0.03577031283924514 cost: 585.0823081462393\niteration: 26401 w: [ 0.2305555  0.073454  -0.2568585 -1.114202 ] b: -0.035835206768040535 cost: 584.9652029548472\niteration: 26501 w: [ 0.230611   0.073704  -0.2577585 -1.116402 ] b: -0.035899750630749204 cost: 584.8494917920449\niteration: 26601 w: [ 0.230666   0.073954  -0.2586585 -1.1185575] b: -0.035963951829464914 cost: 584.736200649114\niteration: 26701 w: [ 0.2307215  0.074204  -0.2595585 -1.120743 ] b: -0.03602780596903525 cost: 584.6220183015874\niteration: 26801 w: [ 0.230777   0.074454  -0.2604585 -1.122943 ] b: -0.036091307838234124 cost: 584.50764343916\niteration: 26901 w: [ 0.230832   0.074704  -0.2613585 -1.1250935] b: -0.03615446516129961 cost: 584.3958804264032\niteration: 27001 w: [ 0.2308865  0.074954  -0.2622585 -1.127249 ] b: -0.036217276007025186 cost: 584.2843306559529\niteration: 27101 w: [ 0.230942   0.075204  -0.2631575 -1.129444 ] b: -0.036279741540073546 cost: 584.1715191658969\niteration: 27201 w: [ 0.2309955  0.075454  -0.2640075 -1.131544 ] b: -0.03634186722265422 cost: 584.0641202463933\niteration: 27301 w: [ 0.231049   0.075704  -0.2648575 -1.133644 ] b: -0.03640365750762085 cost: 583.9571307072051\niteration: 27401 w: [ 0.2311025  0.0759575 -0.265704  -1.1357625] b: -0.03646511194130591 cost: 583.8498018988405\niteration: 27501 w: [ 0.231156   0.0762575 -0.266504  -1.1378625] b: -0.036526229855296806 cost: 583.7442272701054\niteration: 27601 w: [ 0.231209   0.0765575 -0.267304  -1.1399625] b: -0.03658701315868626 cost: 583.6390618390426\niteration: 27701 w: [ 0.231262   0.0768575 -0.268104  -1.142059 ] b: -0.03664746317113464 cost: 583.5344523286376\niteration: 27801 w: [ 0.2313145  0.0771575 -0.268904  -1.144146 ] b: -0.03670758274728636 cost: 583.4306493396557\niteration: 27901 w: [ 0.231368   0.0774575 -0.269704  -1.1462385] b: -0.03676736434747774 cost: 583.327018319911\niteration: 28001 w: [ 0.2314195  0.0777575 -0.270504  -1.148287 ] b: -0.03682681910070681 cost: 583.2256253025431\niteration: 28101 w: [ 0.23147    0.0780575 -0.271304  -1.150287 ] b: -0.03688595444246288 cost: 583.1266304988022\niteration: 28201 w: [ 0.231522   0.0783575 -0.272104  -1.1523345] b: -0.03694476554783443 cost: 583.0260468945207\niteration: 28301 w: [ 0.231574   0.0786575 -0.272904  -1.1543845] b: -0.037003248744037834 cost: 582.9257494374963\niteration: 28401 w: [ 0.2316245  0.0789575 -0.273704  -1.156388 ] b: -0.03706140981977542 cost: 582.8277429568899\niteration: 28501 w: [ 0.2316755  0.0792525 -0.274504  -1.158393 ] b: -0.03711925348814608 cost: 582.7300763452338\niteration: 28601 w: [ 0.2317275  0.0795025 -0.275304  -1.160443 ] b: -0.03717677158621253 cost: 582.6312222039475\niteration: 28701 w: [ 0.231778   0.0797525 -0.276104  -1.1624535] b: -0.03723396624406065 cost: 582.5343513982098\niteration: 28801 w: [ 0.2318275  0.0800025 -0.276904  -1.1644035] b: -0.037290847107994476 cost: 582.4402837453388\niteration: 28901 w: [ 0.2318775  0.0802525 -0.277704  -1.1663635] b: -0.03734741736588498 cost: 582.3461689874248\niteration: 29001 w: [ 0.231928   0.0805025 -0.278504  -1.168334 ] b: -0.03740367254773504 cost: 582.2519930753316\niteration: 29101 w: [ 0.231977   0.0807525 -0.279304  -1.170284 ] b: -0.03745961774146073 cost: 582.1589906539273\niteration: 29201 w: [ 0.2320265  0.0810025 -0.280104  -1.172234 ] b: -0.03751524991403125 cost: 582.0663409135265\niteration: 29301 w: [ 0.2320755  0.0812525 -0.280904  -1.174177 ] b: -0.037570570510696205 cost: 581.9743196583163\niteration: 29401 w: [ 0.2321255  0.0815025 -0.281704  -1.176127 ] b: -0.03762558376528412 cost: 581.8823742526064\niteration: 29501 w: [ 0.2321745  0.0817525 -0.282504  -1.178077 ] b: -0.03768028659555024 cost: 581.790783140823\niteration: 29601 w: [ 0.2322235  0.0820025 -0.283304  -1.179993 ] b: -0.03773467896340583 cost: 581.7008612963946\niteration: 29701 w: [ 0.2322705  0.0822525 -0.284104  -1.1818505] b: -0.037788772464048104 cost: 581.6135375926061\niteration: 29801 w: [ 0.2323185  0.0825025 -0.284904  -1.1837505] b: -0.03784256607677312 cost: 581.524902996663\niteration: 29901 w: [ 0.2323665  0.0827525 -0.285704  -1.1856505] b: -0.03789605642095031 cost: 581.4366040876556\niteration: 30001 w: [ 0.232414   0.0830025 -0.286504  -1.1875125] b: -0.03794924505114637 cost: 581.3500862598542\niteration: 30101 w: [ 0.232461   0.0832525 -0.287304  -1.189364 ] b: -0.038002138879864575 cost: 581.2642892822115\niteration: 30201 w: [ 0.232509   0.0835025 -0.288104  -1.191264 ] b: -0.038054734023587715 cost: 581.1769821691576\niteration: 30301 w: [ 0.232557   0.0837525 -0.288904  -1.193145 ] b: -0.03810702570206265 cost: 581.0907234453425\niteration: 30401 w: [ 0.2326035  0.0840025 -0.289704  -1.194995 ] b: -0.038159019239984805 cost: 581.0059534748533\niteration: 30501 w: [ 0.23265    0.0842525 -0.290504  -1.196802 ] b: -0.038210721202811014 cost: 580.9231010096198\niteration: 30601 w: [ 0.232696   0.0845025 -0.291304  -1.1986445] b: -0.03826213457334974 cost: 580.8392390338167\niteration: 30701 w: [ 0.2327425  0.0847525 -0.292104  -1.2004475] b: -0.03831325491083526 cost: 580.7571483266508\niteration: 30801 w: [ 0.2327875  0.0850025 -0.292904  -1.2022475] b: -0.038364088279058836 cost: 580.6754721971766\niteration: 30901 w: [ 0.232833   0.0852525 -0.293704  -1.2040475] b: -0.03841463635969496 cost: 580.5940976853171\niteration: 31001 w: [ 0.232879   0.0855025 -0.294504  -1.205854 ] b: -0.03846489687908244 cost: 580.5127879100409\niteration: 31101 w: [ 0.232925   0.0857525 -0.295304  -1.207654 ] b: -0.03851486725696284 cost: 580.4320176154398\niteration: 31201 w: [ 0.2329705  0.0860025 -0.296104  -1.209454 ] b: -0.038564549111440846 cost: 580.3515498560345\niteration: 31301 w: [ 0.233016   0.0862525 -0.296904  -1.211254 ] b: -0.03861394464289773 cost: 580.271384083231\niteration: 31401 w: [ 0.23306    0.0865025 -0.297704  -1.212981 ] b: -0.03866306022889402 cost: 580.1941290797067\niteration: 31501 w: [ 0.2331045  0.0867525 -0.298504  -1.214731 ] b: -0.03871189940458871 cost: 580.1163341865199\niteration: 31601 w: [ 0.2331485  0.0870025 -0.299304  -1.216481 ] b: -0.03876045611532014 cost: 580.0388257038768\niteration: 31701 w: [ 0.2331925  0.0872525 -0.300104  -1.218215 ] b: -0.03880873314917972 cost: 579.9621667934034\niteration: 31801 w: [ 0.233236   0.0875025 -0.300904  -1.219915 ] b: -0.03885673724861454 cost: 579.8869820571664\niteration: 31901 w: [ 0.2332795  0.0877525 -0.301704  -1.221651 ] b: -0.0389044692318585 cost: 579.810810084755\niteration: 32001 w: [ 0.2333245  0.0880025 -0.302504  -1.223401 ] b: -0.038951917774882945 cost: 579.7344305934936\niteration: 32101 w: [ 0.233368   0.0882525 -0.303304  -1.225117 ] b: -0.038999089943017894 cost: 579.6595155814375\niteration: 32201 w: [ 0.233411   0.0885025 -0.304104  -1.226817 ] b: -0.03904599065257062 cost: 579.5854275156081\niteration: 32301 w: [ 0.233454   0.0887405 -0.304916  -1.228515 ] b: -0.03909262220892757 cost: 579.5115402500659\niteration: 32401 w: [ 0.233497   0.0889405 -0.305766  -1.230215 ] b: -0.03913898019763201 cost: 579.4374179785354\niteration: 32501 w: [ 0.2335395  0.0891405 -0.306616  -1.231867 ] b: -0.03918506809116004 cost: 579.3651987236226\niteration: 32601 w: [ 0.2335815  0.0893405 -0.307466  -1.233517 ] b: -0.03923089502544056 cost: 579.293303926633\niteration: 32701 w: [ 0.2336235  0.0895405 -0.308316  -1.235167 ] b: -0.039276454919974196 cost: 579.2216646028121\niteration: 32801 w: [ 0.233666   0.0897405 -0.309166  -1.236841 ] b: -0.039321748947745525 cost: 579.1494749983648\niteration: 32901 w: [ 0.233708   0.0899405 -0.310016  -1.238491 ] b: -0.03936677744506123 cost: 579.0783502171353\niteration: 33001 w: [ 0.23375    0.0901825 -0.310866  -1.240141 ] b: -0.03941154076535484 cost: 579.0072381661133\niteration: 33101 w: [ 0.2337915  0.0904325 -0.311716  -1.241791 ] b: -0.03945604130048666 cost: 578.9363361525034\niteration: 33201 w: [ 0.233833   0.0906825 -0.312566  -1.243424 ] b: -0.039500276418159834 cost: 578.8662496264814\niteration: 33301 w: [ 0.2338745  0.0909325 -0.313416  -1.245044 ] b: -0.039544253036813914 cost: 578.7968398683205\niteration: 33401 w: [ 0.233915   0.0912015 -0.314266  -1.246644 ] b: -0.03958797167206071 cost: 578.7282213140238\niteration: 33501 w: [ 0.2339555  0.0915015 -0.315116  -1.248244 ] b: -0.039631433847373415 cost: 578.6596645429348\niteration: 33601 w: [ 0.2339955  0.0918015 -0.315966  -1.2498235] b: -0.03967463920262613 cost: 578.5920126118914\niteration: 33701 w: [ 0.2340345  0.0921015 -0.316816  -1.251378 ] b: -0.03971759518180093 cost: 578.5254024140827\niteration: 33801 w: [ 0.2340755  0.0924015 -0.317666  -1.252978 ] b: -0.03976029560045821 cost: 578.4575577119253\niteration: 33901 w: [ 0.234116   0.0927015 -0.318516  -1.254578 ] b: -0.03980274014542608 cost: 578.3899545900872\niteration: 34001 w: [ 0.234156   0.0930015 -0.319366  -1.256156 ] b: -0.0398449291811528 cost: 578.3232921326702\niteration: 34101 w: [ 0.234195   0.0933015 -0.320216  -1.257706 ] b: -0.03988686953426681 cost: 578.2577512513948\niteration: 34201 w: [ 0.2342345  0.0936015 -0.321066  -1.2592665] b: -0.03992855968521551 cost: 578.192105442392\niteration: 34301 w: [ 0.2342745  0.0939015 -0.321916  -1.260846 ] b: -0.039969997145264925 cost: 578.1260926471763\niteration: 34401 w: [ 0.2343135  0.0942015 -0.322766  -1.262376 ] b: -0.0400111836306711 cost: 578.0618610106209\niteration: 34501 w: [ 0.234351   0.0945015 -0.323616  -1.263876 ] b: -0.04005213058034522 cost: 577.9987844177801\niteration: 34601 w: [ 0.234389   0.0948015 -0.324466  -1.265376 ] b: -0.040092835569364216 cost: 577.9359195552594\niteration: 34701 w: [ 0.2344275  0.0951015 -0.325316  -1.266885 ] b: -0.04013329949006035 cost: 577.8729892010455\niteration: 34801 w: [ 0.2344665  0.0954015 -0.326166  -1.2684235] b: -0.0401735179249774 cost: 577.8093686928104\niteration: 34901 w: [ 0.2345045  0.0957015 -0.327016  -1.2699235] b: -0.04021349173072119 cost: 577.7471477518251\niteration: 35001 w: [ 0.2345425  0.0960015 -0.327866  -1.2714235] b: -0.0402532243530567 cost: 577.6851392670904\niteration: 35101 w: [ 0.23458    0.0963015 -0.328716  -1.2729235] b: -0.04029271580407977 cost: 577.6233440632799\niteration: 35201 w: [ 0.234618   0.0966015 -0.329566  -1.2744265] b: -0.04033196703313952 cost: 577.561670194767\niteration: 35301 w: [ 0.2346565  0.0969015 -0.330416  -1.2759265] b: -0.040370974913990804 cost: 577.5002985706521\niteration: 35401 w: [ 0.2346935  0.0972015 -0.331266  -1.2773845] b: -0.04040974820161255 cost: 577.4403948883896\niteration: 35501 w: [ 0.23473    0.0975015 -0.332116  -1.2788345] b: -0.04044828782236295 cost: 577.3809305612007\niteration: 35601 w: [ 0.2347665  0.0978015 -0.332966  -1.2802845] b: -0.04048659269761744 cost: 577.3216652853081\niteration: 35701 w: [ 0.2348025  0.0981015 -0.333816  -1.281697 ] b: -0.04052466800032149 cost: 577.2637036243619\niteration: 35801 w: [ 0.234839   0.0984015 -0.334666  -1.2831375] b: -0.04056251456725318 cost: 577.2051096373219\niteration: 35901 w: [ 0.234876   0.0987015 -0.335516  -1.2845875] b: -0.04060012693596323 cost: 577.1464342581274\niteration: 36001 w: [ 0.2349125  0.0990015 -0.336366  -1.2860375] b: -0.040637507884454825 cost: 577.0879582969292\niteration: 36101 w: [ 0.2349485  0.0993015 -0.337216  -1.287469 ] b: -0.04067465497604283 cost: 577.0302170243261\niteration: 36201 w: [ 0.2349845  0.0996015 -0.338066  -1.288869 ] b: -0.04071157545788297 cost: 576.9735767128163\niteration: 36301 w: [ 0.2350205  0.0999015 -0.338916  -1.290291 ] b: -0.04074827073290848 cost: 576.9164916057211\niteration: 36401 w: [ 0.235057   0.1002015 -0.339766  -1.291741 ] b: -0.04078473294128609 cost: 576.8587984629943\niteration: 36501 w: [ 0.235093   0.1005015 -0.340616  -1.29315  ] b: -0.04082096442696895 cost: 576.8024695150779\niteration: 36601 w: [ 0.235127   0.1008015 -0.341466  -1.294503 ] b: -0.040856975545559504 cost: 576.7479141739208\niteration: 36701 w: [ 0.235161   0.1011015 -0.342316  -1.295853 ] b: -0.040892768210689545 cost: 576.693617432196\niteration: 36801 w: [ 0.2351955  0.1014015 -0.343166  -1.297203 ] b: -0.0409283433718738 cost: 576.6394934582967\niteration: 36901 w: [ 0.2352305  0.101653  -0.344016  -1.2986015] b: -0.04096369792004398 cost: 576.5844670090263\niteration: 37001 w: [ 0.2352655  0.101903  -0.344866  -1.2999845] b: -0.04099882822106821 cost: 576.5300651668858\niteration: 37101 w: [ 0.2353     0.102153  -0.345716  -1.3013345] b: -0.04103373899600653 cost: 576.4767575590865\niteration: 37201 w: [ 0.235335   0.102403  -0.346566  -1.3026845] b: -0.04106843126447955 cost: 576.4236229323824\niteration: 37301 w: [ 0.2353685  0.102653  -0.347416  -1.3040345] b: -0.041102906494701615 cost: 576.3706626628178\niteration: 37401 w: [ 0.2354035  0.102903  -0.348266  -1.305402 ] b: -0.041137162685250535 cost: 576.3173965234547\niteration: 37501 w: [ 0.235438   0.103153  -0.349116  -1.3067665] b: -0.04117119741448527 cost: 576.264390047907\niteration: 37601 w: [ 0.235472   0.103403  -0.349966  -1.3081055] b: -0.04120501502001705 cost: 576.2122501390596\niteration: 37701 w: [ 0.2355055  0.103653  -0.350816  -1.3094055] b: -0.04123862059829533 cost: 576.1613307696916\niteration: 37801 w: [ 0.235538   0.103903  -0.351666  -1.3107055] b: -0.04127201738045683 cost: 576.1105734746174\niteration: 37901 w: [ 0.235571   0.104153  -0.352516  -1.3120055] b: -0.041305203555341244 cost: 576.0599765863802\niteration: 38001 w: [ 0.235604  0.104403 -0.353366 -1.313289] b: -0.04133818302203708 cost: 576.0099787025753\niteration: 38101 w: [ 0.235637  0.104653 -0.354216 -1.314589] b: -0.04137095150264688 cost: 575.9597017287678\niteration: 38201 w: [ 0.23567   0.104903 -0.355066 -1.315889] b: -0.04140351122899681 cost: 575.9095857080565\niteration: 38301 w: [ 0.235703  0.105153 -0.355916 -1.317189] b: -0.0414358603899377 cost: 575.8596306389535\niteration: 38401 w: [ 0.235736  0.105403 -0.356766 -1.318484] b: -0.04146799908161879 cost: 575.8099669237896\niteration: 38501 w: [ 0.235768  0.105653 -0.357616 -1.319734] b: -0.04149993308611568 cost: 575.7616325257842\niteration: 38601 w: [ 0.2358005  0.105903  -0.358466  -1.3210175] b: -0.04153166516286352 cost: 575.7125806755608\niteration: 38701 w: [ 0.2358335  0.106153  -0.359316  -1.3223175] b: -0.04156318692053569 cost: 575.6632608230395\niteration: 38801 w: [ 0.2358665  0.106403  -0.360166  -1.323613 ] b: -0.04159449877467235 cost: 575.6142172033825\niteration: 38901 w: [ 0.235898  0.106653 -0.361016 -1.324855] b: -0.04162560638728071 cost: 575.5666988942295\niteration: 39001 w: [ 0.235928  0.106903 -0.361866 -1.326055] b: -0.041656520251694945 cost: 575.5203958072647\niteration: 39101 w: [ 0.235959  0.107153 -0.362716 -1.327255] b: -0.04168723964423964 cost: 575.4742294167376\niteration: 39201 w: [ 0.23599   0.107403 -0.363566 -1.328485] b: -0.0417177610879797 cost: 575.4274461198178\niteration: 39301 w: [ 0.2360215  0.107653  -0.364416  -1.329735 ] b: -0.04174807861468223 cost: 575.3803061381557\niteration: 39401 w: [ 0.2360535  0.107903  -0.365266  -1.330982 ] b: -0.04177819475450038 cost: 575.333389350813\niteration: 39501 w: [ 0.236084  0.108153 -0.366116 -1.332182] b: -0.0418081177183429 cost: 575.2877893616308\niteration: 39601 w: [ 0.236115  0.108403 -0.366966 -1.333382] b: -0.041837842252267825 cost: 575.2423270333826\niteration: 39701 w: [ 0.236145  0.108653 -0.367816 -1.334582] b: -0.04186737091647987 cost: 575.1970031746131\niteration: 39801 w: [ 0.2361755  0.108903  -0.368666  -1.3357985] b: -0.04189670787333269 cost: 575.1514131145881\niteration: 39901 w: [ 0.2362075  0.109153  -0.369516  -1.3370485] b: -0.041925844302299675 cost: 575.1051460350462\niteration: 40001 w: [ 0.2362385  0.109403  -0.370366  -1.3382535] b: -0.04195478432736597 cost: 575.0601211514692\niteration: 40101 w: [ 0.2362685  0.109653  -0.371216  -1.3394385] b: -0.041983530345845586 cost: 575.0157194179855\niteration: 40201 w: [ 0.236298   0.109903  -0.372066  -1.3405885] b: -0.04201208589575054 cost: 574.9722946301621\niteration: 40301 w: [ 0.236327   0.110153  -0.372916  -1.3417385] b: -0.042040457465445595 cost: 574.9289972297917\niteration: 40401 w: [ 0.236356  0.110403 -0.373766 -1.342889] b: -0.042068641775232674 cost: 574.885815128229\niteration: 40501 w: [ 0.236386  0.110653 -0.374616 -1.344063] b: -0.04209663862340553 cost: 574.8422014578688\niteration: 40601 w: [ 0.236415  0.110903 -0.375466 -1.345213] b: -0.042124446106123335 cost: 574.7992880765408\niteration: 40701 w: [ 0.2364445  0.111153  -0.376316  -1.346363 ] b: -0.042152066367081305 cost: 574.756501115953\niteration: 40801 w: [ 0.236474  0.111403 -0.377166 -1.347513] b: -0.04217950014430178 cost: 574.7138414023984\niteration: 40901 w: [ 0.236503  0.111653 -0.378016 -1.348663] b: -0.04220674817577341 cost: 574.6713090778348\niteration: 41001 w: [ 0.236532  0.111903 -0.378866 -1.349809] b: -0.0422338089291292 cost: 574.6289967207371\niteration: 41101 w: [ 0.2365605  0.112153  -0.379716  -1.3509265] b: -0.042260689109262006 cost: 574.5874693017602\niteration: 41201 w: [ 0.2365895  0.112403  -0.380566  -1.3520765] b: -0.04228738300700726 cost: 574.5453144606171\niteration: 41301 w: [ 0.236619   0.112653  -0.381416  -1.3532265] b: -0.04231389119597559 cost: 574.5032860694122\niteration: 41401 w: [ 0.236648   0.112903  -0.382266  -1.3543765] b: -0.04234021404981142 cost: 574.4613853337008\niteration: 41501 w: [ 0.236677  0.113153 -0.383116 -1.355483] b: -0.04236635242007891 cost: 574.4205985054763\niteration: 41601 w: [ 0.2367035  0.1134285 -0.3839405 -1.3565575] b: -0.04239231620439948 cost: 574.3809331566257\niteration: 41701 w: [ 0.23673    0.1137285 -0.3847405 -1.3576075] b: -0.04241810721118105 cost: 574.3421989955024\niteration: 41801 w: [ 0.2367575  0.1140285 -0.3855405 -1.3586855] b: -0.04244372624968999 cost: 574.3029434223896\niteration: 41901 w: [ 0.2367855  0.1143285 -0.3863405 -1.3597855] b: -0.04246916793194147 cost: 574.2633090579128\niteration: 42001 w: [ 0.236813   0.1146285 -0.3871405 -1.3608855] b: -0.04249443216381352 cost: 574.223791155423\niteration: 42101 w: [ 0.236841   0.1149285 -0.3879405 -1.3619855] b: -0.042519517132565306 cost: 574.1843891544167\niteration: 42201 w: [ 0.236868   0.1152285 -0.3887405 -1.3630495] b: -0.04254442545416703 cost: 574.1458955543695\niteration: 42301 w: [ 0.236894   0.1155285 -0.3895405 -1.3640995] b: -0.04256916521620936 cost: 574.1078181774484\niteration: 42401 w: [ 0.236921   0.1158285 -0.3903405 -1.3651495] b: -0.042593732836203727 cost: 574.0698461567857\niteration: 42501 w: [ 0.236948   0.1160995 -0.3911405 -1.3662285] b: -0.04261813000249716 cost: 574.0315164935558\niteration: 42601 w: [ 0.236976   0.1163495 -0.3919405 -1.3673285] b: -0.04264234802749705 cost: 573.9929640236277\niteration: 42701 w: [ 0.237004   0.1165995 -0.3927405 -1.3684285] b: -0.04266638980262582 cost: 573.9545276857469\niteration: 42801 w: [ 0.237031   0.1168495 -0.3935405 -1.3694835] b: -0.04269025604872505 cost: 573.9171708014238\niteration: 42901 w: [ 0.237057   0.1170995 -0.3943405 -1.370503 ] b: -0.04271395374938623 cost: 573.8806778818463\niteration: 43001 w: [ 0.237082   0.1173495 -0.3951405 -1.371503 ] b: -0.042737487833860124 cost: 573.844699437578\niteration: 43101 w: [ 0.237108   0.1175995 -0.3959405 -1.372503 ] b: -0.04276085952795905 cost: 573.8088172913805\niteration: 43201 w: [ 0.2371335  0.1178495 -0.3967405 -1.373518 ] b: -0.0427840687217941 cost: 573.7727164630452\niteration: 43301 w: [ 0.23716    0.1180995 -0.3975405 -1.374568 ] b: -0.042807108662038476 cost: 573.7359822518579\niteration: 43401 w: [ 0.237186   0.1183495 -0.3983405 -1.3755805] b: -0.04282998068979318 cost: 573.7001357958096\niteration: 43501 w: [ 0.2372105  0.1185995 -0.3991405 -1.3765805] b: -0.042852690359312814 cost: 573.6646487781856\niteration: 43601 w: [ 0.237236   0.1188495 -0.3999405 -1.3775805] b: -0.04287523804404365 cost: 573.6292573797967\niteration: 43701 w: [ 0.237262   0.1190995 -0.4007405 -1.3785805] b: -0.04289762083747462 cost: 573.5939622494903\niteration: 43801 w: [ 0.237287   0.1193495 -0.4015405 -1.3795805] b: -0.04291984494137986 cost: 573.5587648026735\niteration: 43901 w: [ 0.2373125  0.1195995 -0.4023405 -1.3805805] b: -0.04294190198420261 cost: 573.5236636218713\niteration: 44001 w: [ 0.2373385  0.1198495 -0.4031405 -1.381598 ] b: -0.04296379505920361 cost: 573.4883040313749\niteration: 44101 w: [ 0.237364   0.1200995 -0.4039405 -1.382598 ] b: -0.04298552599591126 cost: 573.4533979540172\niteration: 44201 w: [ 0.237389   0.1203495 -0.4047405 -1.383598 ] b: -0.04300709426809053 cost: 573.418588964991\niteration: 44301 w: [ 0.2374145  0.1205995 -0.4055405 -1.384598 ] b: -0.0430284987909089 cost: 573.3838762225417\niteration: 44401 w: [ 0.237439   0.1208495 -0.4063405 -1.3855765] b: -0.043049742568824316 cost: 573.3496891408569\niteration: 44501 w: [ 0.2374635  0.1210995 -0.4071405 -1.3865265] b: -0.043070830870778606 cost: 573.3161590382939\niteration: 44601 w: [ 0.2374875  0.1213495 -0.4079405 -1.3874685] b: -0.043091764106472634 cost: 573.2828750504999\niteration: 44701 w: [ 0.2375105  0.1215995 -0.4087405 -1.388377 ] b: -0.04311254849486624 cost: 573.2503361312698\niteration: 44801 w: [ 0.237535   0.1218495 -0.4095405 -1.389327 ] b: -0.0431331810381253 cost: 573.2170650067814\niteration: 44901 w: [ 0.237559   0.1220995 -0.4103405 -1.390277 ] b: -0.043153660252572196 cost: 573.1838817771294\niteration: 45001 w: [ 0.2375825  0.1223495 -0.4111405 -1.391227 ] b: -0.04317398468854931 cost: 573.1507868905456\niteration: 45101 w: [ 0.237607   0.1225995 -0.4119405 -1.392177 ] b: -0.043194153625139675 cost: 573.1177784209367\niteration: 45201 w: [ 0.237631   0.1228495 -0.4127405 -1.393127 ] b: -0.0432141645198454 cost: 573.0848582926463\niteration: 45301 w: [ 0.2376545  0.1230995 -0.4135405 -1.394052 ] b: -0.04323402453976285 cost: 573.0525040747171\niteration: 45401 w: [ 0.2376775  0.1233495 -0.4143405 -1.394952 ] b: -0.04325373872034051 cost: 573.0207093344746\niteration: 45501 w: [ 0.2377005  0.1235995 -0.4151405 -1.395861 ] b: -0.043273303211227625 cost: 572.988823184883\niteration: 45601 w: [ 0.2377245  0.1238495 -0.4159405 -1.396811 ] b: -0.04329271654528412 cost: 572.9562434849137\niteration: 45701 w: [ 0.2377485  0.1240995 -0.4167405 -1.397761 ] b: -0.043311973696829716 cost: 572.9237514765476\niteration: 45801 w: [ 0.2377735  0.1243495 -0.4175405 -1.398711 ] b: -0.043331079045608255 cost: 572.891346316377\niteration: 45901 w: [ 0.237797   0.1245995 -0.4183405 -1.399661 ] b: -0.043350028227333595 cost: 572.859029658229\niteration: 46001 w: [ 0.23782    0.1248495 -0.4191405 -1.4005535] b: -0.04336882459814166 cost: 572.8278657280372\niteration: 46101 w: [ 0.2378415  0.1250995 -0.4199405 -1.4014035] b: -0.04338748200757183 cost: 572.797564157023\niteration: 46201 w: [ 0.2378625  0.1253495 -0.4207405 -1.4022535] b: -0.04340600146954301 cost: 572.767334223718\niteration: 46301 w: [ 0.237885   0.1255995 -0.4215405 -1.4031035] b: -0.043424378619056715 cost: 572.7371736295078\niteration: 46401 w: [ 0.237907   0.1258495 -0.4223405 -1.403997 ] b: -0.043442615939793455 cost: 572.706292852243\niteration: 46501 w: [ 0.23793    0.1260995 -0.4231405 -1.404897 ] b: -0.04346070460299656 cost: 572.6753715643873\niteration: 46601 w: [ 0.237953   0.1263495 -0.4239405 -1.405797 ] b: -0.043478646795048044 cost: 572.6445293814484\niteration: 46701 w: [ 0.2379755  0.1265995 -0.4247405 -1.406663 ] b: -0.04349644417190212 cost: 572.6143770761721\niteration: 46801 w: [ 0.2379965  0.1268495 -0.4255405 -1.407513 ] b: -0.04351410419364293 cost: 572.5845847100355\niteration: 46901 w: [ 0.238018   0.1270995 -0.4263405 -1.408363 ] b: -0.04353162267454058 cost: 572.5548631748914\niteration: 47001 w: [ 0.23804    0.1273495 -0.4271405 -1.409213 ] b: -0.04354900290062445 cost: 572.5252121354614\niteration: 47101 w: [ 0.2380615  0.1275995 -0.4279405 -1.410063 ] b: -0.043566245243081 cost: 572.4956323522135\niteration: 47201 w: [ 0.2380835  0.1278495 -0.4287405 -1.410928 ] b: -0.04358334701491288 cost: 572.4658598890682\niteration: 47301 w: [ 0.2381065  0.1280995 -0.4295405 -1.411828 ] b: -0.043600302970251276 cost: 572.4355486724738\niteration: 47401 w: [ 0.2381285  0.1283495 -0.4303405 -1.412707 ] b: -0.04361711456308684 cost: 572.4056821494374\niteration: 47501 w: [ 0.23815    0.1285995 -0.4311405 -1.413557 ] b: -0.04363378394255007 cost: 572.3763934986426\niteration: 47601 w: [ 0.238172   0.1288495 -0.4319405 -1.414407 ] b: -0.043650315473948806 cost: 572.3471754141245\niteration: 47701 w: [ 0.2381935  0.1290995 -0.4327405 -1.415254 ] b: -0.04366670694392344 cost: 572.318079981069\niteration: 47801 w: [ 0.2382135  0.1293495 -0.4335405 -1.4160545] b: -0.04368296499597868 cost: 572.2898498190917\niteration: 47901 w: [ 0.238234   0.1295995 -0.4343405 -1.4168545] b: -0.04369909069808017 cost: 572.2616911211487\niteration: 48001 w: [ 0.238254   0.1298495 -0.4351405 -1.4176545] b: -0.04371508640276859 cost: 572.233596067335\niteration: 48101 w: [ 0.238275   0.1300995 -0.4359405 -1.418478 ] b: -0.04373094847758602 cost: 572.2051674540347\niteration: 48201 w: [ 0.238296   0.1303495 -0.4367405 -1.4192845] b: -0.04374667758125656 cost: 572.1770908405823\niteration: 48301 w: [ 0.238316   0.1305995 -0.4375405 -1.4200845] b: -0.04376227470442577 cost: 572.1491874302409\niteration: 48401 w: [ 0.238336   0.1308495 -0.4383405 -1.4208845] b: -0.043777738942043114 cost: 572.1213474737636\niteration: 48501 w: [ 0.238357   0.1310995 -0.4391405 -1.4216845] b: -0.04379307066504745 cost: 572.0935698922207\niteration: 48601 w: [ 0.238377   0.1313495 -0.4399405 -1.4224845] b: -0.043808273158978506 cost: 572.0658560601026\niteration: 48701 w: [ 0.2383975  0.1315995 -0.4407405 -1.4232845] b: -0.043823344244381245 cost: 572.0382052554959\niteration: 48801 w: [ 0.238417   0.1318495 -0.4415405 -1.4240845] b: -0.043838285385110934 cost: 572.0106187440057\niteration: 48901 w: [ 0.238438   0.1320995 -0.4423405 -1.42488  ] b: -0.04385309403304935 cost: 571.9831666008774\niteration: 49001 w: [ 0.238457   0.1323495 -0.4431405 -1.4256415] b: -0.043867776888403244 cost: 571.956328861757\niteration: 49101 w: [ 0.238478   0.1325995 -0.4439405 -1.4264415] b: -0.043882330073129965 cost: 571.9289274425673\niteration: 49201 w: [ 0.238498   0.1328495 -0.4447405 -1.4272415] b: -0.043896752974754163 cost: 571.9015894529904\niteration: 49301 w: [ 0.2385185  0.1330995 -0.4455405 -1.4280415] b: -0.043911045235483885 cost: 571.8743146421923\niteration: 49401 w: [ 0.2385385  0.1333495 -0.4463405 -1.4288415] b: -0.04392520212560996 cost: 571.8471031499669\niteration: 49501 w: [ 0.238559   0.1335995 -0.4471405 -1.4296415] b: -0.04393922802379701 cost: 571.8199546539037\niteration: 49601 w: [ 0.238578   0.1338495 -0.4479405 -1.430406 ] b: -0.043953126590839296 cost: 571.7934288668474\niteration: 49701 w: [ 0.238597   0.1340995 -0.4487405 -1.4311495] b: -0.04396690250434432 cost: 571.7672898843078\niteration: 49801 w: [ 0.238615   0.1343495 -0.4495405 -1.4318495] b: -0.04398056015238576 cost: 571.741885052648\niteration: 49901 w: [ 0.2386325  0.1345995 -0.4503405 -1.4325495] b: -0.043994098472817704 cost: 571.7165297840289\niteration: 50001 w: [ 0.2386515  0.1348205 -0.4511405 -1.4332785] b: -0.0440075227404304 cost: 571.6909376475869\niteration: 50101 w: [ 0.23867    0.1350205 -0.4519405 -1.4340285] b: -0.04402082510182937 cost: 571.6651943734959\niteration: 50201 w: [ 0.2386895  0.1352205 -0.4527405 -1.4347785] b: -0.04403400219790913 cost: 571.6395057232006\niteration: 50301 w: [ 0.2387085  0.1354205 -0.4535405 -1.4355285] b: -0.044047058042500566 cost: 571.6138736786957\niteration: 50401 w: [ 0.238728   0.1356205 -0.4543405 -1.4362785] b: -0.04405998754110906 cost: 571.5882968889058\niteration: 50501 w: [ 0.2387475  0.1358205 -0.4551405 -1.4370285] b: -0.04407279288595525 cost: 571.5627762223476\niteration: 50601 w: [ 0.238766   0.1360205 -0.4559405 -1.4377445] b: -0.044085477553486095 cost: 571.5378236616749\niteration: 50701 w: [ 0.2387835  0.1362205 -0.4567405 -1.4384445] b: -0.044098046452340896 cost: 571.5131622069822\niteration: 50801 w: [ 0.238801   0.1364205 -0.4575405 -1.4391445] b: -0.04411050086059633 cost: 571.4885502132248\niteration: 50901 w: [ 0.2388195  0.1366205 -0.4583405 -1.4398445] b: -0.04412283969101553 cost: 571.4639863819477\niteration: 51001 w: [ 0.2388375  0.1368205 -0.4591405 -1.4405675] b: -0.044135060898999365 cost: 571.4391320743304\niteration: 51101 w: [ 0.238857   0.1370205 -0.4599405 -1.4413175] b: -0.04414716053130088 cost: 571.4139316192799\niteration: 51201 w: [ 0.238876   0.1372205 -0.4607405 -1.4420675] b: -0.04415913495838772 cost: 571.3887874409287\niteration: 51301 w: [ 0.2388955  0.1374205 -0.4615405 -1.4428175] b: -0.0441709867368011 cost: 571.363698831815\niteration: 51401 w: [ 0.2389145  0.1376205 -0.4623405 -1.443562 ] b: -0.04418271572393476 cost: 571.3387461016308\niteration: 51501 w: [ 0.238932   0.137821  -0.46314   -1.4442615] b: -0.04419432737805757 cost: 571.3145042195908\niteration: 51601 w: [ 0.238949   0.138063  -0.463898  -1.4449195] b: -0.044205827799104404 cost: 571.2913476147653\niteration: 51701 w: [ 0.2389655  0.138313  -0.464648  -1.4455695] b: -0.04421721681972525 cost: 571.2684340461198\niteration: 51801 w: [ 0.2389815  0.138563  -0.465398  -1.4462195] b: -0.04422850434069259 cost: 571.2455633956558\niteration: 51901 w: [ 0.2389985  0.138813  -0.466148  -1.4468695] b: -0.044239678708783134 cost: 571.2227341420393\niteration: 52001 w: [ 0.2390145  0.139063  -0.466898  -1.4475195] b: -0.044250746851745126 cost: 571.1999482838279\niteration: 52101 w: [ 0.2390325  0.139313  -0.467648  -1.4482155] b: -0.04426170401705745 cost: 571.1765562495957\niteration: 52201 w: [ 0.2390505  0.139563  -0.468398  -1.4489155] b: -0.04427254465004354 cost: 571.1531566804923\niteration: 52301 w: [ 0.239068   0.139813  -0.469148  -1.4496155] b: -0.04428327161545885 cost: 571.1298060747328\niteration: 52401 w: [ 0.2390845  0.140063  -0.469898  -1.450274 ] b: -0.04429389017660648 cost: 571.1070804803992\niteration: 52501 w: [ 0.239101  0.140313 -0.470648 -1.450924] b: -0.04430439662860817 cost: 571.0845156320821\niteration: 52601 w: [ 0.239118  0.140563 -0.471398 -1.451574] b: -0.044314795067265496 cost: 571.0619929125285\niteration: 52701 w: [ 0.239134  0.140813 -0.472148 -1.452224] b: -0.04432508804827828 cost: 571.0395131527289\niteration: 52801 w: [ 0.239151  0.141063 -0.472898 -1.452874] b: -0.04433527193365774 cost: 571.0170752746512\niteration: 52901 w: [ 0.239167  0.141313 -0.473648 -1.453524] b: -0.04434534745751853 cost: 570.9946802925166\niteration: 53001 w: [ 0.2391835  0.141563  -0.474398  -1.454174 ] b: -0.04435531863289233 cost: 570.9723274671761\niteration: 53101 w: [ 0.2392    0.141813 -0.475148 -1.454825] b: -0.04436518053874599 cost: 570.9500036134561\niteration: 53201 w: [ 0.2392175  0.142063  -0.475898  -1.4555225] b: -0.04437493125918676 cost: 570.9270996986912\niteration: 53301 w: [ 0.239235  0.142313 -0.476648 -1.456201] b: -0.04438456818399991 cost: 570.9044968378925\niteration: 53401 w: [ 0.2392515  0.142563  -0.477398  -1.456851 ] b: -0.044394092257732165 cost: 570.8823183350595\niteration: 53501 w: [ 0.239268  0.142813 -0.478148 -1.457501] b: -0.04440351456080184 cost: 570.860182230946\niteration: 53601 w: [ 0.2392845  0.143063  -0.478898  -1.458151 ] b: -0.04441282671879517 cost: 570.8380885264514\niteration: 53701 w: [ 0.2393005  0.143313  -0.479648  -1.458793 ] b: -0.04442203232312543 cost: 570.8161421960624\niteration: 53801 w: [ 0.239316  0.143563 -0.480398 -1.459393] b: -0.04443113542144272 cost: 570.7947837673903\niteration: 53901 w: [ 0.239331  0.143813 -0.481148 -1.459993] b: -0.04444013876439281 cost: 570.7734623473094\niteration: 54001 w: [ 0.2393465  0.144063  -0.481898  -1.460593 ] b: -0.04444904417862618 cost: 570.7521769537893\niteration: 54101 w: [ 0.2393615  0.144313  -0.482648  -1.461193 ] b: -0.044457845839779735 cost: 570.7309284984201\niteration: 54201 w: [ 0.239377  0.144563 -0.483398 -1.461793] b: -0.04446655213257856 cost: 570.7097161381254\niteration: 54301 w: [ 0.239392  0.144813 -0.484148 -1.462391] b: -0.044475154939276416 cost: 570.6885660860677\niteration: 54401 w: [ 0.2394075  0.145063  -0.484898  -1.462991 ] b: -0.04448366212289125 cost: 570.6674266770738\niteration: 54501 w: [ 0.239422  0.145313 -0.485648 -1.463591] b: -0.044492065937663396 cost: 570.646324651441\niteration: 54601 w: [ 0.2394375  0.145563  -0.486398  -1.464191 ] b: -0.04450037403963673 cost: 570.6252579376106\niteration: 54701 w: [ 0.239453  0.145813 -0.487148 -1.464791] b: -0.04450857951135688 cost: 570.6042279047145\niteration: 54801 w: [ 0.239468   0.1460755 -0.487898  -1.465391 ] b: -0.04451668734511934 cost: 570.5831645555419\niteration: 54901 w: [ 0.2394835  0.1463755 -0.488648  -1.465991 ] b: -0.044524691593873233 cost: 570.5619267333468\niteration: 55001 w: [ 0.2394985  0.1466755 -0.489398  -1.466591 ] b: -0.044532600467770384 cost: 570.5407256843602\niteration: 55101 w: [ 0.2395135  0.1469755 -0.490148  -1.467191 ] b: -0.044540406320805144 cost: 570.5195611994791\niteration: 55201 w: [ 0.2395285  0.1472755 -0.490898  -1.467791 ] b: -0.04454811571589193 cost: 570.4984332770302\niteration: 55301 w: [ 0.2395435  0.1475755 -0.491648  -1.4683875] b: -0.04455572340303722 cost: 570.4773844482463\niteration: 55401 w: [ 0.2395575  0.1478755 -0.492398  -1.4689375] b: -0.04456323919682102 cost: 570.4569353087255\niteration: 55501 w: [ 0.2395715  0.1481755 -0.493148  -1.469496 ] b: -0.04457065797154219 cost: 570.4364147788573\niteration: 55601 w: [ 0.239587   0.1484755 -0.493898  -1.470096 ] b: -0.04457798249458192 cost: 570.4154274737182\niteration: 55701 w: [ 0.239602   0.1487755 -0.494648  -1.470696 ] b: -0.04458520803349047 cost: 570.3944767350475\niteration: 55801 w: [ 0.2396165  0.1490755 -0.495398  -1.471296 ] b: -0.04459233313599394 cost: 570.3735629365909\niteration: 55901 w: [ 0.239632   0.1493755 -0.496148  -1.471896 ] b: -0.044599356714052685 cost: 570.3526849354695\niteration: 56001 w: [ 0.239647   0.1496755 -0.496898  -1.472496 ] b: -0.04460628387337297 cost: 570.331843872712\niteration: 56101 w: [ 0.239661   0.1499755 -0.497648  -1.4730585] b: -0.04461310856722972 cost: 570.3114793660353\niteration: 56201 w: [ 0.239675   0.1502755 -0.498398  -1.4736085] b: -0.04461984425898379 cost: 570.2912929971058\niteration: 56301 w: [ 0.239689   0.1505755 -0.499148  -1.4741585] b: -0.04462648736223333 cost: 570.2711377636118\niteration: 56401 w: [ 0.2397025  0.1508755 -0.499898  -1.4746925] b: -0.044633039926924044 cost: 570.2511989012149\niteration: 56501 w: [ 0.239715   0.1511755 -0.500648  -1.4751925] b: -0.044639506135810456 cost: 570.2316814574103\niteration: 56601 w: [ 0.2397275  0.1514755 -0.501398  -1.4756925] b: -0.0446458883217852 cost: 570.2121900848757\niteration: 56701 w: [ 0.23974    0.1517755 -0.502148  -1.4761925] b: -0.04465218576046076 cost: 570.192724783249\niteration: 56801 w: [ 0.239753   0.1520755 -0.502898  -1.476706 ] b: -0.04465840011527091 cost: 570.1731317458647\niteration: 56901 w: [ 0.2397665  0.1523755 -0.503648  -1.477256 ] b: -0.044664523709915506 cost: 570.1531526954597\niteration: 57001 w: [ 0.239781   0.1526755 -0.504398  -1.477806 ] b: -0.044670555839731285 cost: 570.1332041366994\niteration: 57101 w: [ 0.239794   0.1529755 -0.505148  -1.478356 ] b: -0.04467649505190555 cost: 570.1132874964165\niteration: 57201 w: [ 0.2398085  0.1532755 -0.505898  -1.478906 ] b: -0.04468234135110249 cost: 570.0934007972817\niteration: 57301 w: [ 0.239822   0.1535755 -0.506648  -1.479456 ] b: -0.04468809692781731 cost: 570.0735459058966\niteration: 57401 w: [ 0.239836   0.1538755 -0.507398  -1.480006 ] b: -0.044693758872060584 cost: 570.0537218065622\niteration: 57501 w: [ 0.23985    0.1541755 -0.508148  -1.480556 ] b: -0.04469933228908955 cost: 570.0339288367919\niteration: 57601 w: [ 0.2398625  0.1544755 -0.508898  -1.4810705] b: -0.04470481296680544 cost: 570.0145566313226\niteration: 57701 w: [ 0.2398755  0.1547755 -0.509648  -1.4815705] b: -0.04471020815553974 cost: 569.9953696812341\niteration: 57801 w: [ 0.239888   0.1550755 -0.510398  -1.4820705] b: -0.04471552192522094 cost: 569.9762090808101\niteration: 57901 w: [ 0.2399005  0.1553755 -0.511148  -1.4825705] b: -0.04472075027235149 cost: 569.9570745457365\niteration: 58001 w: [ 0.239913   0.1556755 -0.511898  -1.4830705] b: -0.04472589320116645 cost: 569.9379660755556\niteration: 58101 w: [ 0.239926   0.1559755 -0.512648  -1.4835705] b: -0.04473095472348089 cost: 569.9188835482136\niteration: 58201 w: [ 0.239939   0.1562755 -0.513398  -1.484102 ] b: -0.04473592974685667 cost: 569.8994907263377\niteration: 58301 w: [ 0.239953   0.1565755 -0.514148  -1.484652 ] b: -0.04474081251747856 cost: 569.879930360033\niteration: 58401 w: [ 0.2399665  0.1568755 -0.514898  -1.485202 ] b: -0.04474560643547577 cost: 569.8604012945524\niteration: 58501 w: [ 0.2399805  0.1571755 -0.515648  -1.485752 ] b: -0.04475030349020866 cost: 569.8409029863919\niteration: 58601 w: [ 0.239994   0.1574755 -0.516398  -1.486298 ] b: -0.044754911697236444 cost: 569.8214779421842\niteration: 58701 w: [ 0.2400065  0.1577755 -0.517148  -1.486798 ] b: -0.04475943173374751 cost: 569.8025627717394\niteration: 58801 w: [ 0.2400185  0.1580755 -0.517898  -1.4872595] b: -0.044763870528273 cost: 569.7840727218494\niteration: 58901 w: [ 0.24003    0.1583755 -0.518648  -1.4877095] b: -0.04476823554538529 cost: 569.765724311177\niteration: 59001 w: [ 0.240041   0.1586755 -0.519398  -1.4881595] b: -0.0447725226465158 cost: 569.7473974735639\niteration: 59101 w: [ 0.240052   0.1589755 -0.520148  -1.4886095] b: -0.04477673219981719 cost: 569.729092338658\niteration: 59201 w: [ 0.2400635  0.1592755 -0.520898  -1.4890595] b: -0.04478086785247668 cost: 569.7108081686955\niteration: 59301 w: [ 0.240075   0.1595755 -0.521648  -1.4895095] b: -0.044784926693603455 cost: 569.6925455654234\niteration: 59401 w: [ 0.2400865  0.1598755 -0.522398  -1.4899595] b: -0.04478891127734012 cost: 569.6743045283498\niteration: 59501 w: [ 0.240098   0.1601755 -0.523148  -1.4904095] b: -0.044792816871129626 cost: 569.6560850575707\niteration: 59601 w: [ 0.240109   0.1604755 -0.523898  -1.4908645] b: -0.0447966461507861 cost: 569.6378368637107\niteration: 59701 w: [ 0.2401215  0.1607755 -0.524648  -1.4913645] b: -0.044800393298342706 cost: 569.619161347444\niteration: 59801 w: [ 0.240134   0.1610755 -0.525398  -1.4918645] b: -0.04480405801626756 cost: 569.6005118880126\niteration: 59901 w: [ 0.240147   0.1613755 -0.526148  -1.4923645] b: -0.04480763520812887 cost: 569.5818882639572\niteration: 60001 w: [ 0.2401595  0.1616755 -0.526898  -1.4928645] b: -0.04481113070740227 cost: 569.5632908317524\niteration: 60101 w: [ 0.240171   0.1619755 -0.527648  -1.493334 ] b: -0.04481454605901117 cost: 569.5450182054849\niteration: 60201 w: [ 0.2401825  0.1622755 -0.528398  -1.493784 ] b: -0.044817882300208826 cost: 569.526958672386\niteration: 60301 w: [ 0.2401935  0.1625755 -0.529148  -1.494234 ] b: -0.044821140312463585 cost: 569.5089209046606\niteration: 60401 w: [ 0.240205   0.1628755 -0.529898  -1.494684 ] b: -0.044824322649991324 cost: 569.490904266945\niteration: 60501 w: [ 0.2402165  0.1631755 -0.530648  -1.495134 ] b: -0.044827431866871645 cost: 569.4729091918731\niteration: 60601 w: [ 0.2402275  0.1634755 -0.531398  -1.495584 ] b: -0.04483046541656892 cost: 569.4549357833702\niteration: 60701 w: [ 0.240239   0.1637755 -0.532148  -1.496034 ] b: -0.044833418566627974 cost: 569.4369836018276\niteration: 60801 w: [ 0.24025    0.1640755 -0.532898  -1.496484 ] b: -0.04483629569279948 cost: 569.4190532196315\niteration: 60901 w: [ 0.2402615  0.1643755 -0.533648  -1.496934 ] b: -0.04483909898493552 cost: 569.4011439298841\niteration: 61001 w: [ 0.240273   0.1646755 -0.534398  -1.497384 ] b: -0.04484182589650491 cost: 569.3832562014524\niteration: 61101 w: [ 0.240284   0.16497   -0.535148  -1.4978395] b: -0.04484447610915759 cost: 569.3653696146876\niteration: 61201 w: [ 0.2402965  0.1652235 -0.535898  -1.498336 ] b: -0.044847041533183925 cost: 569.3473522030133\niteration: 61301 w: [ 0.24031    0.1654735 -0.536648  -1.498836 ] b: -0.04484952532626583 cost: 569.3293470960521\niteration: 61401 w: [ 0.240322   0.1657235 -0.537398  -1.4993245] b: -0.04485192752755638 cost: 569.3114743426181\niteration: 61501 w: [ 0.2403325  0.1659735 -0.538148  -1.4997305] b: -0.04485425073040904 cost: 569.2943831870945\niteration: 61601 w: [ 0.240342   0.1662235 -0.538898  -1.5001305] b: -0.044856510479960324 cost: 569.2773651819281\niteration: 61701 w: [ 0.2403525  0.1664735 -0.539648  -1.5005305] b: -0.04485869992424291 cost: 569.2603635876148\niteration: 61801 w: [ 0.2403625  0.1667235 -0.540398  -1.5009305] b: -0.0448608179737973 cost: 569.2433798392283\niteration: 61901 w: [ 0.240373   0.1669735 -0.541148  -1.5013305] b: -0.044862869732760276 cost: 569.226413020541\niteration: 62001 w: [ 0.240383   0.1672235 -0.541898  -1.5017305] b: -0.044864851925541994 cost: 569.2094638208309\niteration: 62101 w: [ 0.2403935  0.1674735 -0.542648  -1.5021305] b: -0.044866771113509694 cost: 569.1925317766621\niteration: 62201 w: [ 0.2404035  0.1677235 -0.543398  -1.5025305] b: -0.044868615641347094 cost: 569.1756171246001\niteration: 62301 w: [ 0.2404135  0.1679735 -0.544148  -1.5029305] b: -0.04487039680683701 cost: 569.158719838163\niteration: 62401 w: [ 0.2404235  0.1682235 -0.544898  -1.5033305] b: -0.04487210514081172 cost: 569.1418399173649\niteration: 62501 w: [ 0.2404335  0.1684735 -0.545648  -1.5037305] b: -0.04487374428999055 cost: 569.1249773617413\niteration: 62601 w: [ 0.240444   0.1687235 -0.546398  -1.5041305] b: -0.044875319358474726 cost: 569.1081316326905\niteration: 62701 w: [ 0.240454   0.1689735 -0.547148  -1.5045305] b: -0.04487682342724546 cost: 569.0913036216986\niteration: 62801 w: [ 0.2404645  0.1692235 -0.547898  -1.5049485] b: -0.04487826062547549 cost: 569.0743368942079\niteration: 62901 w: [ 0.240476   0.1694735 -0.548648  -1.5053945] b: -0.04487962312379703 cost: 569.0571471180903\niteration: 63001 w: [ 0.2404865  0.1697235 -0.549398  -1.5057945] b: -0.044880911909810785 cost: 569.0403733284305\niteration: 63101 w: [ 0.240496   0.1699735 -0.550148  -1.5061945] b: -0.04488213517528422 cost: 569.0236173541431\niteration: 63201 w: [ 0.2405065  0.1702235 -0.550898  -1.5065945] b: -0.044883287094389254 cost: 569.0068779218115\niteration: 63301 w: [ 0.2405165  0.1704735 -0.551648  -1.5069945] b: -0.044884370585207375 cost: 568.9901562626421\niteration: 63401 w: [ 0.240527   0.1707235 -0.552398  -1.5073945] b: -0.04488538637984364 cost: 568.9734515987129\niteration: 63501 w: [ 0.240537   0.1709735 -0.553148  -1.5077945] b: -0.04488633484604603 cost: 568.9567644795526\niteration: 63601 w: [ 0.240547   0.1712235 -0.553898  -1.5081945] b: -0.04488721562285334 cost: 568.9400947220805\niteration: 63701 w: [ 0.2405575  0.1714735 -0.554648  -1.5085945] b: -0.0448880254347744 cost: 568.9234420029147\niteration: 63801 w: [ 0.240567   0.1717235 -0.555398  -1.5089945] b: -0.044888768292820215 cost: 568.9068072909803\niteration: 63901 w: [ 0.2405775  0.1719735 -0.556148  -1.5093945] b: -0.04488944019289172 cost: 568.8901889254711\niteration: 64001 w: [ 0.2405875  0.1722235 -0.556898  -1.5097945] b: -0.04489004186700648 cost: 568.8735884276006\niteration: 64101 w: [ 0.2405975  0.1724735 -0.557648  -1.5101945] b: -0.044890578783624895 cost: 568.857005289795\niteration: 64201 w: [ 0.240608   0.1727235 -0.558398  -1.5105945] b: -0.04489104584543653 cost: 568.8404388652581\niteration: 64301 w: [ 0.2406185  0.1729735 -0.559148  -1.5109945] b: -0.04489144633489671 cost: 568.8238900289779\niteration: 64401 w: [ 0.2406285  0.1732235 -0.559898  -1.5113945] b: -0.0448917766120924 cost: 568.8073586019825\niteration: 64501 w: [ 0.240638   0.1734735 -0.560648  -1.5117685] b: -0.04489204068447237 cost: 568.7910525097269\niteration: 64601 w: [ 0.240648   0.1737235 -0.561398  -1.512156 ] b: -0.044892240358248015 cost: 568.7746542129256\niteration: 64701 w: [ 0.2406565  0.1739735 -0.562148  -1.512506 ] b: -0.04489237466835711 cost: 568.75856970587\niteration: 64801 w: [ 0.2406655  0.1742235 -0.562898  -1.512856 ] b: -0.04489244696729769 cost: 568.7424986328639\niteration: 64901 w: [ 0.2406745  0.1744735 -0.563648  -1.513206 ] b: -0.04489246527326841 cost: 568.726441212959\niteration: 65001 w: [ 0.240683   0.1747235 -0.564398  -1.513556 ] b: -0.04489241428738068 cost: 568.7103977298219\niteration: 65101 w: [ 0.240692   0.1749735 -0.565148  -1.513906 ] b: -0.04489231004334036 cost: 568.6943674824792\niteration: 65201 w: [ 0.240701   0.1752235 -0.565898  -1.514256 ] b: -0.04489214270710021 cost: 568.6783508876156\niteration: 65301 w: [ 0.24071    0.1754735 -0.566648  -1.514606 ] b: -0.04489191483203941 cost: 568.6623479450496\niteration: 65401 w: [ 0.2407185  0.1757235 -0.567398  -1.514956 ] b: -0.04489162970012087 cost: 568.6463590023856\niteration: 65501 w: [ 0.240728   0.1759735 -0.568148  -1.515306 ] b: -0.044891274927176844 cost: 568.630383016019\niteration: 65601 w: [ 0.2407365  0.1762235 -0.568898  -1.515656 ] b: -0.04489086618262559 cost: 568.6144211104248\niteration: 65701 w: [ 0.240745   0.1764735 -0.569648  -1.516006 ] b: -0.04489039508976024 cost: 568.5984731861477\niteration: 65801 w: [ 0.2407545  0.1767235 -0.570398  -1.516356 ] b: -0.044889863108977156 cost: 568.582537824422\niteration: 65901 w: [ 0.240763   0.1769735 -0.571148  -1.516706 ] b: -0.04488927206488172 cost: 568.5666169357443\niteration: 66001 w: [ 0.240772   0.1772235 -0.571898  -1.517056 ] b: -0.04488861649553792 cost: 568.5507092868971\niteration: 66101 w: [ 0.240781   0.1774735 -0.572648  -1.517406 ] b: -0.04488790514804882 cost: 568.5348152887956\niteration: 66201 w: [ 0.240789   0.1777235 -0.573398  -1.517728 ] b: -0.044887133771344435 cost: 568.5191427128302\niteration: 66301 w: [ 0.2407965  0.1779735 -0.574148  -1.518028 ] b: -0.044886305983297584 cost: 568.503644320882\niteration: 66401 w: [ 0.2408045  0.1782235 -0.574898  -1.518328 ] b: -0.044885425877233175 cost: 568.4881558335743\niteration: 66501 w: [ 0.240813   0.1784735 -0.575648  -1.5186625] b: -0.04488449506208519 cost: 568.4724254399188\niteration: 66601 w: [ 0.2408215  0.1787235 -0.576398  -1.5190125] b: -0.04488350076372432 cost: 568.4565950135748\niteration: 66701 w: [ 0.240831   0.1789735 -0.577148  -1.5193625] b: -0.044882445239250175 cost: 568.4407774799153\niteration: 66801 w: [ 0.2408395  0.1792235 -0.577898  -1.5197125] b: -0.04488133286362339 cost: 568.4249740850212\niteration: 66901 w: [ 0.2408485  0.1794735 -0.578648  -1.5200625] b: -0.04488015270993369 cost: 568.4091840944394\niteration: 67001 w: [ 0.2408575  0.1797235 -0.579398  -1.5204125] b: -0.04487891971888266 cost: 568.3934077530002\niteration: 67101 w: [ 0.240866   0.1799735 -0.580148  -1.5207625] b: -0.044877620413157765 cost: 568.3776453658315\niteration: 67201 w: [ 0.240875   0.1802235 -0.580898  -1.5211125] b: -0.0448762588036135 cost: 568.3618961875501\niteration: 67301 w: [ 0.240884   0.1804735 -0.581648  -1.5214625] b: -0.04487484254413159 cost: 568.3461606577763\niteration: 67401 w: [ 0.2408925  0.1807235 -0.582398  -1.5218125] b: -0.044873359614753236 cost: 568.3304392760039\niteration: 67501 w: [ 0.2409015  0.1809735 -0.583148  -1.5221625] b: -0.04487182021994447 cost: 568.3147309081618\niteration: 67601 w: [ 0.2409105  0.1812235 -0.583898  -1.5225125] b: -0.04487021671168406 cost: 568.2990361879282\niteration: 67701 w: [ 0.2409195  0.1814735 -0.584648  -1.5228625] b: -0.044868554558079185 cost: 568.2833551152875\niteration: 67801 w: [ 0.240928   0.1817235 -0.585398  -1.5232125] b: -0.0448668326690262 cost: 568.2676882497838\niteration: 67901 w: [ 0.240937   0.1819735 -0.586148  -1.5235625] b: -0.04486504813285369 cost: 568.2520343372929\niteration: 68001 w: [ 0.2409455  0.1822235 -0.586898  -1.523876 ] b: -0.04486320764871655 cost: 568.2366421404846\niteration: 68101 w: [ 0.2409525  0.1824735 -0.587648  -1.5241685] b: -0.04486131698491053 cost: 568.2214041689945\niteration: 68201 w: [ 0.240959   0.1827235 -0.588398  -1.5244185] b: -0.04485937758432829 cost: 568.2064625147353\niteration: 68301 w: [ 0.2409655  0.1829735 -0.589148  -1.5246685] b: -0.04485739010157927 cost: 568.191528501538\niteration: 68401 w: [ 0.240972   0.1832235 -0.589898  -1.5249185] b: -0.04485535453896185 cost: 568.1766021292979\niteration: 68501 w: [ 0.2409785  0.1834735 -0.590648  -1.525183 ] b: -0.044853275964410515 cost: 568.1615867385609\niteration: 68601 w: [ 0.240986   0.1837235 -0.591398  -1.525483 ] b: -0.04485114663100071 cost: 568.1463438544681\niteration: 68701 w: [ 0.2409935  0.1839735 -0.592148  -1.525783 ] b: -0.04484896431598089 cost: 568.1311113877174\niteration: 68801 w: [ 0.2410015  0.1842235 -0.592898  -1.526083 ] b: -0.04484673230101014 cost: 568.115888847355\niteration: 68901 w: [ 0.241009   0.1844735 -0.593648  -1.526383 ] b: -0.044844449131341994 cost: 568.1006770696602\niteration: 69001 w: [ 0.2410165  0.1847235 -0.594398  -1.526683 ] b: -0.04484211189488024 cost: 568.0854757087199\niteration: 69101 w: [ 0.241024   0.1849735 -0.595148  -1.526983 ] b: -0.044839717679744914 cost: 568.0702847641898\niteration: 69201 w: [ 0.241032   0.1852235 -0.595898  -1.527283 ] b: -0.044837270860516566 cost: 568.0551037607585\niteration: 69301 w: [ 0.24104    0.1854735 -0.596648  -1.527583 ] b: -0.04483477617612403 cost: 568.0399334800187\niteration: 69401 w: [ 0.2410475  0.1857235 -0.597398  -1.527883 ] b: -0.04483222634257584 cost: 568.0247734941025\niteration: 69501 w: [ 0.241055   0.1859735 -0.598148  -1.528183 ] b: -0.04482962245553335 cost: 568.0096239240279\niteration: 69601 w: [ 0.2410625  0.1862235 -0.598898  -1.528483 ] b: -0.044826965974957146 cost: 567.9944847696721\niteration: 69701 w: [ 0.24107    0.1864735 -0.599648  -1.528783 ] b: -0.044824255810442755 cost: 567.9793560307802\niteration: 69801 w: [ 0.2410775  0.1867235 -0.600398  -1.529083 ] b: -0.044821494514963575 cost: 567.964237707296\niteration: 69901 w: [ 0.2410855  0.1869735 -0.601148  -1.529383 ] b: -0.04481868209123726 cost: 567.9491295015928\niteration: 70001 w: [ 0.241093   0.1872235 -0.601898  -1.5296815] b: -0.0448158157771353 cost: 567.9340411849798\niteration: 70101 w: [ 0.241099   0.1874735 -0.602648  -1.5299325] b: -0.044812903178222785 cost: 567.9192575548324\niteration: 70201 w: [ 0.2411055  0.1877235 -0.603398  -1.5301825] b: -0.04480994178728949 cost: 567.9044873510401\niteration: 70301 w: [ 0.241112   0.1879735 -0.604148  -1.5304325] b: -0.04480693491556161 cost: 567.8897247860717\niteration: 70401 w: [ 0.2411185  0.1882235 -0.604898  -1.5306825] b: -0.044803880743585434 cost: 567.8749698597245\niteration: 70501 w: [ 0.2411245  0.1884735 -0.605648  -1.5309325] b: -0.04480078546725343 cost: 567.8602227370592\niteration: 70601 w: [ 0.2411305  0.1887235 -0.606398  -1.5311825] b: -0.04479764544539645 cost: 567.8454835057097\niteration: 70701 w: [ 0.241137   0.1889735 -0.607148  -1.5314325] b: -0.04479446068042328 cost: 567.8307511515355\niteration: 70801 w: [ 0.241144   0.1892235 -0.607898  -1.5317035] b: -0.04479122732824677 cost: 567.8159001742825\niteration: 70901 w: [ 0.241152   0.1894735 -0.608648  -1.5320035] b: -0.04478794283028032 cost: 567.8008842338573\niteration: 71001 w: [ 0.2411595  0.1897235 -0.609398  -1.5323035] b: -0.04478460978039001 cost: 567.7858789900508\niteration: 71101 w: [ 0.2411675  0.1899735 -0.610148  -1.5326035] b: -0.04478121797998786 cost: 567.7708840388574\niteration: 71201 w: [ 0.241175   0.1902235 -0.610898  -1.5329035] b: -0.044777775811414816 cost: 567.7558994763973\niteration: 71301 w: [ 0.2411825  0.1904735 -0.611648  -1.5332035] b: -0.04477427817658842 cost: 567.740925326507\niteration: 71401 w: [ 0.24119    0.1907235 -0.612398  -1.5335035] b: -0.044770729450173415 cost: 567.7259615892932\niteration: 71501 w: [ 0.241198   0.1909735 -0.613148  -1.5338035] b: -0.044767129634804455 cost: 567.7110081634311\niteration: 71601 w: [ 0.241205   0.1912235 -0.613898  -1.5341035] b: -0.0447634783687217 cost: 567.696065352114\niteration: 71701 w: [ 0.241213   0.1914735 -0.614648  -1.5344035] b: -0.0447597716469623 cost: 567.6811324617726\niteration: 71801 w: [ 0.24122    0.1917235 -0.615398  -1.5347035] b: -0.04475601056515604 cost: 567.6662107624259\niteration: 71901 w: [ 0.241228   0.1919735 -0.616148  -1.5350035] b: -0.0447521984050765 cost: 567.6512984070831\niteration: 72001 w: [ 0.241236   0.1922235 -0.616898  -1.5353035] b: -0.04474833662653165 cost: 567.6363967715947\niteration: 72101 w: [ 0.2412425  0.1924735 -0.617648  -1.5355705] b: -0.04474442339446299 cost: 567.6216912480177\niteration: 72201 w: [ 0.2412485  0.1927235 -0.618398  -1.535797 ] b: -0.044740460851290756 cost: 567.6072208608837\niteration: 72301 w: [ 0.2412535  0.1929735 -0.619148  -1.535997 ] b: -0.044736458378039246 cost: 567.5929048009751\niteration: 72401 w: [ 0.2412585  0.1932235 -0.619898  -1.536197 ] b: -0.04473242230707027 cost: 567.5785940011182\niteration: 72501 w: [ 0.2412635  0.1934735 -0.620648  -1.536397 ] b: -0.04472835227582463 cost: 567.5642884611924\niteration: 72601 w: [ 0.241269   0.1937235 -0.621398  -1.536597 ] b: -0.04472424027081226 cost: 567.5499880663137\niteration: 72701 w: [ 0.2412735  0.1939735 -0.622148  -1.536797 ] b: -0.044720095037857566 cost: 567.535693159498\niteration: 72801 w: [ 0.241279   0.1942235 -0.622898  -1.536997 ] b: -0.044715912935543264 cost: 567.5214030691868\niteration: 72901 w: [ 0.2412845  0.1944735 -0.623648  -1.537197 ] b: -0.04471168740771142 cost: 567.507118619557\niteration: 73001 w: [ 0.241289   0.1947235 -0.624398  -1.537397 ] b: -0.044707426471463016 cost: 567.492839108406\niteration: 73101 w: [ 0.2412935  0.1949735 -0.625148  -1.537597 ] b: -0.044703133043405736 cost: 567.4785656677225\niteration: 73201 w: [ 0.241299   0.1952235 -0.625898  -1.537797 ] b: -0.04469880421081756 cost: 567.4642961851722\niteration: 73301 w: [ 0.2413045  0.1954735 -0.626648  -1.538007 ] b: -0.044694428422869165 cost: 567.449978736396\niteration: 73401 w: [ 0.241311   0.1957235 -0.627398  -1.538257 ] b: -0.04469001138652787 cost: 567.4354533251985\niteration: 73501 w: [ 0.2413175  0.1959735 -0.628148  -1.538507 ] b: -0.04468555477135992 cost: 567.4209355500349\niteration: 73601 w: [ 0.2413235  0.1962235 -0.628898  -1.538757 ] b: -0.04468105384316036 cost: 567.4064254621868\niteration: 73701 w: [ 0.24133    0.1964735 -0.629648  -1.539007 ] b: -0.04467650605396739 cost: 567.3919227848545\niteration: 73801 w: [ 0.2413365  0.1967235 -0.630398  -1.539257 ] b: -0.044671908491398724 cost: 567.3774277423294\niteration: 73901 w: [ 0.241343   0.1969735 -0.631148  -1.539507 ] b: -0.044667265529727836 cost: 567.3629403349152\niteration: 74001 w: [ 0.241349   0.1972235 -0.631898  -1.539757 ] b: -0.0446625800858231 cost: 567.3484605192311\niteration: 74101 w: [ 0.241355   0.1974735 -0.632648  -1.540007 ] b: -0.04465785216191007 cost: 567.3339885891546\niteration: 74201 w: [ 0.2413615  0.1977235 -0.633398  -1.540257 ] b: -0.04465308103157559 cost: 567.3195237411394\niteration: 74301 w: [ 0.241368   0.1979735 -0.634148  -1.540507 ] b: -0.04464826268935859 cost: 567.3050665276254\niteration: 74401 w: [ 0.2413745  0.1982235 -0.634898  -1.540757 ] b: -0.04464339531594874 cost: 567.2906169483437\niteration: 74501 w: [ 0.241381   0.1984735 -0.635648  -1.541007 ] b: -0.04463848729315726 cost: 567.2761750040089\niteration: 74601 w: [ 0.241387   0.1987235 -0.636398  -1.541257 ] b: -0.044633532065139724 cost: 567.2617408045877\niteration: 74701 w: [ 0.241393   0.1989735 -0.637148  -1.541507 ] b: -0.04462853473495631 cost: 567.2473144898116\niteration: 74801 w: [ 0.241399   0.1992235 -0.637898  -1.5417395] b: -0.044623494392478716 cost: 567.2329823641644\niteration: 74901 w: [ 0.2414045  0.1994735 -0.638648  -1.5419395] b: -0.044618411746205615 cost: 567.2188178909146\niteration: 75001 w: [ 0.2414095  0.1997235 -0.639398  -1.5421395] b: -0.04461328717659812 cost: 567.2046591030679\niteration: 75101 w: [ 0.2414145  0.1999735 -0.640148  -1.5423395] b: -0.0446081312513026 cost: 567.190505572774\niteration: 75201 w: [ 0.2414195  0.2002235 -0.640898  -1.5425395] b: -0.04460293887134891 cost: 567.1763572994224\niteration: 75301 w: [ 0.241425   0.2004735 -0.641648  -1.5427395] b: -0.04459770858128816 cost: 567.1622139174234\niteration: 75401 w: [ 0.24143    0.2007235 -0.642398  -1.5429395] b: -0.04459244038298292 cost: 567.1480760508351\niteration: 75501 w: [ 0.241435   0.2009735 -0.643148  -1.5431395] b: -0.04458714047201242 cost: 567.1339434414232\niteration: 75601 w: [ 0.2414405  0.2012235 -0.643898  -1.5433395] b: -0.044581796098599 cost: 567.1198159994211\niteration: 75701 w: [ 0.2414455  0.2014735 -0.644648  -1.5435395] b: -0.04457640981484352 cost: 567.1056937951935\niteration: 75801 w: [ 0.2414505  0.2017235 -0.645398  -1.5437395] b: -0.0445709929170357 cost: 567.0915768479932\niteration: 75901 w: [ 0.241455   0.2019735 -0.646148  -1.5439395] b: -0.044565543221060815 cost: 567.0774655650389\niteration: 76001 w: [ 0.2414605  0.2022235 -0.646898  -1.5441405] b: -0.04456004932449957 cost: 567.0633540064424\niteration: 76101 w: [ 0.2414665  0.2024735 -0.647648  -1.544374 ] b: -0.04455451390285402 cost: 567.0490951615458\niteration: 76201 w: [ 0.241473   0.2027235 -0.648398  -1.544624 ] b: -0.04454893986455376 cost: 567.0347658845934\niteration: 76301 w: [ 0.241479   0.2029735 -0.649148  -1.544874 ] b: -0.04454332156907549 cost: 567.0204443621012\niteration: 76401 w: [ 0.2414855  0.2032235 -0.649898  -1.545124 ] b: -0.044537656468473706 cost: 567.0061301770573\niteration: 76501 w: [ 0.241492   0.2034735 -0.650648  -1.545374 ] b: -0.04453194456494381 cost: 566.9918236245034\niteration: 76601 w: [ 0.2414985  0.2037235 -0.651398  -1.545624 ] b: -0.04452618549644902 cost: 566.9775247043\niteration: 76701 w: [ 0.2415035  0.2039735 -0.652148  -1.545833 ] b: -0.044520385995847525 cost: 566.9634200128506\niteration: 76801 w: [ 0.2415085  0.2042235 -0.652898  -1.546033 ] b: -0.044514554075105975 cost: 566.9493617327123\niteration: 76901 w: [ 0.241514   0.2044735 -0.653648  -1.546233 ] b: -0.0445086817259632 cost: 566.9353085171541\niteration: 77001 w: [ 0.2415185  0.2047235 -0.654398  -1.546433 ] b: -0.0445027689502574 cost: 566.9212609367133\niteration: 77101 w: [ 0.2415235  0.2049735 -0.655148  -1.546633 ] b: -0.044496824129579116 cost: 566.9072184216469\niteration: 77201 w: [ 0.241529   0.2052235 -0.655898  -1.546833 ] b: -0.04449084471558758 cost: 566.8931806534987\niteration: 77301 w: [ 0.241534   0.2054735 -0.656648  -1.547033 ] b: -0.0444848186871729 cost: 566.879148541308\niteration: 77401 w: [ 0.241539   0.2057235 -0.657398  -1.547233 ] b: -0.04447875515444184 cost: 566.8651216837847\niteration: 77501 w: [ 0.2415445  0.2059735 -0.658148  -1.547433 ] b: -0.04447266104162603 cost: 566.8510998520667\niteration: 77601 w: [ 0.241549   0.2062235 -0.658898  -1.547619 ] b: -0.0444665281574926 cost: 566.8371445071833\niteration: 77701 w: [ 0.241553   0.2064735 -0.659648  -1.547769 ] b: -0.04446036404281835 cost: 566.8233501539166\niteration: 77801 w: [ 0.2415565  0.2067235 -0.660398  -1.547919 ] b: -0.044454171711708665 cost: 566.8095592967883\niteration: 77901 w: [ 0.2415605  0.2069735 -0.661148  -1.548069 ] b: -0.04444795080130273 cost: 566.7957715491276\niteration: 78001 w: [ 0.241564   0.2072235 -0.661898  -1.548219 ] b: -0.044441696576763716 cost: 566.7819874719131\niteration: 78101 w: [ 0.2415685  0.2074735 -0.662648  -1.548369 ] b: -0.044435414140217955 cost: 566.7682062768596\niteration: 78201 w: [ 0.241572   0.2077235 -0.663398  -1.548519 ] b: -0.04442910640779916 cost: 566.7544285946202\niteration: 78301 w: [ 0.2415755  0.2079735 -0.664148  -1.548669 ] b: -0.04442276609430268 cost: 566.7406544947395\niteration: 78401 w: [ 0.2415795  0.2082235 -0.664898  -1.548819 ] b: -0.04441639392987798 cost: 566.726883364453\niteration: 78501 w: [ 0.2415835  0.2084735 -0.665648  -1.548969 ] b: -0.04440999574532973 cost: 566.7131156423799\niteration: 78601 w: [ 0.2415875  0.2087235 -0.666398  -1.549119 ] b: -0.0444035693561241 cost: 566.6993513282141\niteration: 78701 w: [ 0.241591   0.2089735 -0.667148  -1.549269 ] b: -0.04439711112036111 cost: 566.6855904016103\niteration: 78801 w: [ 0.241595   0.2092235 -0.667898  -1.549419 ] b: -0.044390622496886606 cost: 566.6718326915116\niteration: 78901 w: [ 0.241599   0.2094735 -0.668648  -1.549569 ] b: -0.04438410749485907 cost: 566.6580783894475\niteration: 79001 w: [ 0.2416025  0.2097235 -0.669398  -1.549719 ] b: -0.04437756575125167 cost: 566.644327439185\niteration: 79101 w: [ 0.241606   0.2099735 -0.670148  -1.549869 ] b: -0.0443709903452587 cost: 566.6305800704831\niteration: 79201 w: [ 0.2416105  0.2102235 -0.670898  -1.550019 ] b: -0.044364386014641544 cost: 566.6168354506932\niteration: 79301 w: [ 0.241615   0.2104735 -0.671648  -1.550205 ] b: -0.04435775012600436 cost: 566.6029477281119\niteration: 79401 w: [ 0.24162    0.2107235 -0.672398  -1.550405 ] b: -0.044351078665761046 cost: 566.5890079620382\niteration: 79501 w: [ 0.241625   0.2109735 -0.673148  -1.550605 ] b: -0.04434437228402714 cost: 566.5750734493666\niteration: 79601 w: [ 0.2416305  0.2112235 -0.673898  -1.550805 ] b: -0.04433762260313563 cost: 566.5611440712677\niteration: 79701 w: [ 0.2416355  0.2114735 -0.674648  -1.551005 ] b: -0.04433083363246524 cost: 566.5472199579689\niteration: 79801 w: [ 0.2416405  0.2117235 -0.675398  -1.551205 ] b: -0.04432401083893338 cost: 566.5333010979483\niteration: 79901 w: [ 0.241645   0.2119735 -0.676148  -1.551405 ] b: -0.04431715495320546 cost: 566.5193879249645\niteration: 80001 w: [ 0.2416505  0.2122235 -0.676898  -1.551605 ] b: -0.044310255775840315 cost: 566.5054791362836\niteration: 80101 w: [ 0.2416555  0.2124735 -0.677648  -1.551805 ] b: -0.04430331950221026 cost: 566.4915760338886\niteration: 80201 w: [ 0.2416605  0.2127235 -0.678398  -1.552005 ] b: -0.0442963505062136 cost: 566.4776781845525\niteration: 80301 w: [ 0.241666   0.2129735 -0.679148  -1.552205 ] b: -0.044289341867313335 cost: 566.4637853288026\niteration: 80401 w: [ 0.241671   0.2132235 -0.679898  -1.552405 ] b: -0.04428229577335388 cost: 566.4498978784472\niteration: 80501 w: [ 0.2416755  0.2134735 -0.680648  -1.552605 ] b: -0.04427521222624583 cost: 566.4360161490459\niteration: 80601 w: [ 0.241681   0.2137235 -0.681398  -1.552805 ] b: -0.044268097057288236 cost: 566.4221387348672\niteration: 80701 w: [ 0.2416865  0.2139735 -0.682148  -1.553005 ] b: -0.04426093788097838 cost: 566.4082669578703\niteration: 80801 w: [ 0.2416915  0.2142235 -0.682898  -1.553205 ] b: -0.044253737613752384 cost: 566.3944004097254\niteration: 80901 w: [ 0.241696   0.2144735 -0.683648  -1.5534015] b: -0.04424650407418981 cost: 566.3805523811844\niteration: 81001 w: [ 0.2417     0.2147235 -0.684398  -1.5535535] b: -0.0442392484030999 cost: 566.366874485128\niteration: 81101 w: [ 0.241704   0.2149735 -0.685148  -1.5537035] b: -0.04423194912860855 cost: 566.3532074100382\niteration: 81201 w: [ 0.241708   0.2152235 -0.685898  -1.5538535] b: -0.04422462533743393 cost: 566.3395437422951\niteration: 81301 w: [ 0.2417115  0.2154735 -0.686648  -1.5540035] b: -0.044217278124250164 cost: 566.3258834800346\niteration: 81401 w: [ 0.2417155  0.2157235 -0.687398  -1.5541535] b: -0.04420989145970252 cost: 566.3122264133088\niteration: 81501 w: [ 0.241719   0.2159735 -0.688148  -1.5543035] b: -0.044202482468955556 cost: 566.2985729254839\niteration: 81601 w: [ 0.2417235  0.2162235 -0.688898  -1.5544535] b: -0.044195038402049726 cost: 566.2849225006541\niteration: 81701 w: [ 0.241727   0.2164735 -0.689648  -1.5546035] b: -0.04418756508952961 cost: 566.2712754017374\niteration: 81801 w: [ 0.2417305  0.2167235 -0.690398  -1.5547535] b: -0.044180069091013546 cost: 566.257631883448\niteration: 81901 w: [ 0.2417335  0.2169735 -0.691148  -1.554873 ] b: -0.04417253342569516 cost: 566.2441003949476\niteration: 82001 w: [ 0.241736   0.2172235 -0.691898  -1.554973 ] b: -0.04416499034453212 cost: 566.2306406449751\niteration: 82101 w: [ 0.241739   0.2174735 -0.692648  -1.555073 ] b: -0.04415741090929084 cost: 566.217182443745\niteration: 82201 w: [ 0.2417415  0.2177235 -0.693398  -1.555173 ] b: -0.04414982754621891 cost: 566.2037263877603\niteration: 82301 w: [ 0.241744   0.2179735 -0.694148  -1.555273 ] b: -0.04414220273050675 cost: 566.1902722096141\niteration: 82401 w: [ 0.2417465  0.2182235 -0.694898  -1.555373 ] b: -0.04413457872555968 cost: 566.1768199157001\niteration: 82501 w: [ 0.241749   0.2184735 -0.695648  -1.555473 ] b: -0.044126913634315756 cost: 566.1633694995927\niteration: 82601 w: [ 0.2417515  0.2187235 -0.696398  -1.555573 ] b: -0.04411924061215847 cost: 566.1499209663347\niteration: 82701 w: [ 0.241754   0.2189735 -0.697148  -1.555673 ] b: -0.04411153634278879 cost: 566.1364743123085\niteration: 82801 w: [ 0.2417575  0.2191995 -0.697922  -1.555797 ] b: -0.04410381153208676 cost: 566.1227044451356\niteration: 82901 w: [ 0.2417615  0.2193995 -0.698722  -1.555947 ] b: -0.04409605466964385 cost: 566.1085857709974\niteration: 83001 w: [ 0.2417655  0.2195995 -0.699522  -1.556097 ] b: -0.04408827476290967 cost: 566.0944705156153\niteration: 83101 w: [ 0.2417695  0.2197995 -0.700322  -1.556247 ] b: -0.044080464162358085 cost: 566.0803586777562\niteration: 83201 w: [ 0.241773   0.2199995 -0.701122  -1.556397 ] b: -0.044072620683480467 cost: 566.0662505793833\niteration: 83301 w: [ 0.2417775  0.2201995 -0.701922  -1.556547 ] b: -0.044064751250098316 cost: 566.0521452544546\niteration: 83401 w: [ 0.2417815  0.2203995 -0.702722  -1.556697 ] b: -0.044056852584561934 cost: 566.038043669485\niteration: 83501 w: [ 0.241785   0.2205995 -0.703522  -1.556847 ] b: -0.04404892104512502 cost: 566.0239457445279\niteration: 83601 w: [ 0.241789   0.2207995 -0.704322  -1.556997 ] b: -0.04404095845498934 cost: 566.0098509672554\niteration: 83701 w: [ 0.2417935  0.2209995 -0.705122  -1.557147 ] b: -0.044032968094685834 cost: 565.9957594175944\niteration: 83801 w: [ 0.241797   0.2211995 -0.705922  -1.557297 ] b: -0.04402494887260283 cost: 565.9816716654638\niteration: 83901 w: [ 0.241801   0.2213995 -0.706722  -1.557447 ] b: -0.04401689787563255 cost: 565.9675871402037\niteration: 84001 w: [ 0.241805   0.2215995 -0.707522  -1.557597 ] b: -0.044008814740949906 cost: 565.953506031749\niteration: 84101 w: [ 0.241809   0.2217995 -0.708322  -1.557747 ] b: -0.04400071112859554 cost: 565.9394283419414\niteration: 84201 w: [ 0.241813   0.2219995 -0.709122  -1.557897 ] b: -0.04399257246668448 cost: 565.9253540683698\niteration: 84301 w: [ 0.241817   0.2221995 -0.709922  -1.558047 ] b: -0.04398440057849661 cost: 565.9112832112706\niteration: 84401 w: [ 0.241821   0.2223995 -0.710722  -1.558197 ] b: -0.04397620202353111 cost: 565.8972157716677\niteration: 84501 w: [ 0.241825   0.2225995 -0.711522  -1.558347 ] b: -0.0439679753459165 cost: 565.883151749276\niteration: 84601 w: [ 0.241829   0.2227995 -0.712322  -1.558497 ] b: -0.04395971581078866 cost: 565.8690911432633\niteration: 84701 w: [ 0.241833   0.2229995 -0.713122  -1.558647 ] b: -0.043951425605690694 cost: 565.8550339539391\niteration: 84801 w: [ 0.241837   0.2231995 -0.713922  -1.558797 ] b: -0.043943108739772856 cost: 565.840980181919\niteration: 84901 w: [ 0.241841   0.2233995 -0.714722  -1.558947 ] b: -0.04393476157113743 cost: 565.8269298265482\niteration: 85001 w: [ 0.2418445  0.2235995 -0.715522  -1.5590905] b: -0.04392638143800761 cost: 565.8129031461231\niteration: 85101 w: [ 0.2418475  0.2237995 -0.716322  -1.5591905] b: -0.043917979696702414 cost: 565.7990126545548\niteration: 85201 w: [ 0.24185    0.2239995 -0.717122  -1.5592905] b: -0.04390955183724068 cost: 565.7851244516952\niteration: 85301 w: [ 0.2418525  0.2241995 -0.717922  -1.5593905] b: -0.04390111097634154 cost: 565.7712382959821\niteration: 85401 w: [ 0.2418555  0.2243995 -0.718722  -1.5594905] b: -0.04389263306963594 cost: 565.7573535235814\niteration: 85501 w: [ 0.241858   0.2245995 -0.719522  -1.5595905] b: -0.04388415163630646 cost: 565.7434712064734\niteration: 85601 w: [ 0.241861   0.2247995 -0.720322  -1.5596905] b: -0.04387563206629608 cost: 565.7295903627551\niteration: 85701 w: [ 0.2418635  0.2249995 -0.721122  -1.5597905] b: -0.04386710751443344 cost: 565.7157118836956\niteration: 85801 w: [ 0.2418665  0.2251995 -0.721922  -1.5598905] b: -0.04385854482794359 cost: 565.7018349683426\niteration: 85901 w: [ 0.241869   0.2253995 -0.722722  -1.5599905] b: -0.043849974247310256 cost: 565.687960326702\niteration: 86001 w: [ 0.2418715  0.2255995 -0.723522  -1.5600905] b: -0.04384137172776742 cost: 565.674087728679\niteration: 86101 w: [ 0.2418745  0.2257995 -0.724322  -1.5601905] b: -0.043832752208085725 cost: 565.6602165353485\niteration: 86201 w: [ 0.241877   0.2259995 -0.725122  -1.5602905] b: -0.043824110588573835 cost: 565.6463477757137\niteration: 86301 w: [ 0.24188    0.2261995 -0.725922  -1.5603905] b: -0.04381544213421699 cost: 565.6324805095624\niteration: 86401 w: [ 0.2418825  0.2263995 -0.726722  -1.5604905] b: -0.04380676178345182 cost: 565.6186155882541\niteration: 86501 w: [ 0.2418855  0.2265995 -0.727522  -1.5605905] b: -0.04379804039122569 cost: 565.6047522485059\niteration: 86601 w: [ 0.241888   0.2267995 -0.728322  -1.5606905] b: -0.04378931985650645 cost: 565.590891165143\niteration: 86701 w: [ 0.241891   0.2269995 -0.729122  -1.5607905] b: -0.043780556825052705 cost: 565.5770317536927\niteration: 86801 w: [ 0.2418935  0.2271995 -0.729922  -1.5608905] b: -0.043771791738631514 cost: 565.5631745073835\niteration: 86901 w: [ 0.241897   0.2273995 -0.730722  -1.561019 ] b: -0.04376298611426308 cost: 565.5492380857785\niteration: 87001 w: [ 0.241901   0.2275995 -0.731522  -1.561169 ] b: -0.04375415960676601 cost: 565.5352436938665\niteration: 87101 w: [ 0.241905   0.2277995 -0.732322  -1.561319 ] b: -0.04374530682982238 cost: 565.5212527184391\niteration: 87201 w: [ 0.241909   0.2279995 -0.733122  -1.561469 ] b: -0.04373641721903922 cost: 565.5072651575699\niteration: 87301 w: [ 0.2419125  0.2281995 -0.733922  -1.561619 ] b: -0.043727502434556184 cost: 565.4932814473848\niteration: 87401 w: [ 0.241917   0.2283995 -0.734722  -1.561769 ] b: -0.04371855774177547 cost: 565.4793002846758\niteration: 87501 w: [ 0.241921   0.2285995 -0.735522  -1.561919 ] b: -0.043709577312652216 cost: 565.4653229706638\niteration: 87601 w: [ 0.2419245  0.2287995 -0.736322  -1.562069 ] b: -0.04370057390044967 cost: 565.4513494259093\niteration: 87701 w: [ 0.241929   0.2289995 -0.737122  -1.562219 ] b: -0.04369153621240533 cost: 565.4373785910419\niteration: 87801 w: [ 0.241933   0.2291995 -0.737922  -1.562367 ] b: -0.04368246893742625 cost: 565.4234169022783\niteration: 87901 w: [ 0.2419355  0.2293995 -0.738722  -1.5624745] b: -0.04367338175751114 cost: 565.4095669425142\niteration: 88001 w: [ 0.2419385  0.2295995 -0.739522  -1.5625745] b: -0.04366426680660982 cost: 565.3957387879867\niteration: 88101 w: [ 0.241941   0.2297995 -0.740322  -1.5626745] b: -0.043655130509352665 cost: 565.3819127979477\niteration: 88201 w: [ 0.241944   0.2299995 -0.741122  -1.5627745] b: -0.04364597541703816 cost: 565.3680885713421\niteration: 88301 w: [ 0.2419465  0.2301995 -0.741922  -1.5628745] b: -0.04363678732185613 cost: 565.354266415744\niteration: 88401 w: [ 0.2419495  0.2303995 -0.742722  -1.5629745] b: -0.043627592821355386 cost: 565.3404461171352\niteration: 88501 w: [ 0.2419515  0.2305995 -0.743522  -1.5630745] b: -0.043618356211724836 cost: 565.3266282390144\niteration: 88601 w: [ 0.241955   0.2307995 -0.744322  -1.5631745] b: -0.04360912485760081 cost: 565.3128114262324\niteration: 88701 w: [ 0.2419575  0.2309995 -0.745122  -1.5632745] b: -0.043599848481717315 cost: 565.2989969419219\niteration: 88801 w: [ 0.24196    0.2311995 -0.745922  -1.5633745] b: -0.04359056789094995 cost: 565.2851845049095\niteration: 88901 w: [ 0.241963   0.2313995 -0.746722  -1.5634745] b: -0.04358125102465927 cost: 565.271373849579\niteration: 89001 w: [ 0.2419655  0.2315995 -0.747522  -1.5635745] b: -0.043571922294626135 cost: 565.2575652471947\niteration: 89101 w: [ 0.2419685  0.2317995 -0.748322  -1.5636745] b: -0.04356256603531824 cost: 565.2437585196113\niteration: 89201 w: [ 0.241971   0.2319995 -0.749122  -1.5637745] b: -0.04355318698437483 cost: 565.2299537513082\niteration: 89301 w: [ 0.241974   0.2321995 -0.749922  -1.5638745] b: -0.04354379097217741 cost: 565.2161509513413\niteration: 89401 w: [ 0.2419765  0.2323995 -0.750722  -1.5639745] b: -0.04353436124041771 cost: 565.2023500169087\niteration: 89501 w: [ 0.241979   0.2325995 -0.751522  -1.5640745] b: -0.043524926208496365 cost: 565.1885511293257\niteration: 89601 w: [ 0.2419815  0.2327995 -0.752322  -1.5641745] b: -0.04351545162973985 cost: 565.1747542820754\niteration: 89701 w: [ 0.2419845  0.2329995 -0.753122  -1.5642745] b: -0.04350597357477985 cost: 565.1609589925495\niteration: 89801 w: [ 0.241987   0.2331995 -0.753922  -1.5643745] b: -0.04349645743238784 cost: 565.1471659805871\niteration: 89901 w: [ 0.24199    0.2333995 -0.754722  -1.5644745] b: -0.04348693235091929 cost: 565.1333746170727\niteration: 90001 w: [ 0.2419925  0.2335995 -0.755522  -1.5645745] b: -0.04347737574231354 cost: 565.1195854404783\niteration: 90101 w: [ 0.241994   0.2337995 -0.756322  -1.5646285] b: -0.04346779962834007 cost: 565.1059085405564\niteration: 90201 w: [ 0.2419955  0.2339995 -0.757122  -1.5646785] b: -0.04345821872708318 cost: 565.092242110591\niteration: 90301 w: [ 0.241997   0.2341995 -0.757922  -1.5647285] b: -0.04344863015546345 cost: 565.0785765921421\niteration: 90401 w: [ 0.241998   0.2343995 -0.758722  -1.5647785] b: -0.04343900768222378 cost: 565.0649124937082\niteration: 90501 w: [ 0.242      0.2345995 -0.759522  -1.5648285] b: -0.04342937608349749 cost: 565.0512482793849\niteration: 90601 w: [ 0.2420015  0.2347995 -0.760322  -1.5648785] b: -0.04341974556066009 cost: 565.0375854917914\niteration: 90701 w: [ 0.242003   0.2349995 -0.761122  -1.5649285] b: -0.0434100887893934 cost: 565.0239236121013\niteration: 90801 w: [ 0.242004   0.2351995 -0.761922  -1.5649785] b: -0.043400418886425436 cost: 565.0102628991568\niteration: 90901 w: [ 0.242006   0.2353995 -0.762722  -1.5650285] b: -0.043390718365287996 cost: 564.9966025805952\niteration: 91001 w: [ 0.2420075  0.2355995 -0.763522  -1.5650785] b: -0.043381034953649915 cost: 564.9829434346113\niteration: 91101 w: [ 0.2420085  0.2357995 -0.764322  -1.5651285] b: -0.043371319466822544 cost: 564.9692852586478\niteration: 91201 w: [ 0.24201    0.2359995 -0.765122  -1.5651785] b: -0.04336159595208973 cost: 564.9556278664095\niteration: 91301 w: [ 0.2420115  0.2361995 -0.765922  -1.5652285] b: -0.04335184473633115 cost: 564.9419713817019\niteration: 91401 w: [ 0.242013   0.2363995 -0.766722  -1.5652785] b: -0.04334209678837021 cost: 564.9283158105529\niteration: 91501 w: [ 0.242014   0.2365995 -0.767522  -1.5653285] b: -0.043332332069927 cost: 564.9146615513002\niteration: 91601 w: [ 0.242016   0.2367995 -0.768322  -1.5653785] b: -0.043322543660289384 cost: 564.9010073958249\niteration: 91701 w: [ 0.2420175  0.2369995 -0.769122  -1.56543  ] b: -0.04331274571329043 cost: 564.8873511457244\niteration: 91801 w: [ 0.2420195  0.2371995 -0.769922  -1.565495 ] b: -0.043302932070080125 cost: 564.8736653039914\niteration: 91901 w: [ 0.2420215  0.2373995 -0.770722  -1.565595 ] b: -0.04329310011885063 cost: 564.8599021408312\niteration: 92001 w: [ 0.242025   0.2375995 -0.771522  -1.565695 ] b: -0.04328324612871905 cost: 564.8461402767304\niteration: 92101 w: [ 0.242027   0.2377995 -0.772322  -1.565795 ] b: -0.043273371921822575 cost: 564.8323808552801\niteration: 92201 w: [ 0.2420305  0.2379995 -0.773122  -1.565895 ] b: -0.04326347021339266 cost: 564.8186230087731\niteration: 92301 w: [ 0.2420325  0.2381995 -0.773922  -1.565995 ] b: -0.04325355266235703 cost: 564.8048673286175\niteration: 92401 w: [ 0.2420355  0.2383995 -0.774722  -1.566095 ] b: -0.04324360688320746 cost: 564.791113333957\niteration: 92501 w: [ 0.242038   0.2385995 -0.775522  -1.566195 ] b: -0.04323364526379469 cost: 564.7773615612219\niteration: 92601 w: [ 0.242041   0.2387995 -0.776322  -1.566295 ] b: -0.04322365614693717 cost: 564.7636114925101\niteration: 92701 w: [ 0.2420435  0.2389995 -0.777122  -1.566395 ] b: -0.04321365155651646 cost: 564.7498635532531\niteration: 92801 w: [ 0.242046   0.2391995 -0.777922  -1.566495 ] b: -0.04320362092799316 cost: 564.7361176557522\niteration: 92901 w: [ 0.242049   0.2393995 -0.778722  -1.566595 ] b: -0.04319357154936389 cost: 564.7223733045179\niteration: 93001 w: [ 0.2420515  0.2395995 -0.779522  -1.566695 ] b: -0.043183500870779504 cost: 564.7086312408707\niteration: 93101 w: [ 0.2420545  0.2397995 -0.780322  -1.566795 ] b: -0.04317339796419578 cost: 564.6948908133479\niteration: 93201 w: [ 0.242057   0.2399995 -0.781122  -1.566895 ] b: -0.04316328468997496 cost: 564.6811525829081\niteration: 93301 w: [ 0.24206    0.2401995 -0.781922  -1.566995 ] b: -0.043153139554223306 cost: 564.6674160813033\niteration: 93401 w: [ 0.2420625  0.2403995 -0.782722  -1.567095 ] b: -0.04314298368862032 cost: 564.6536816839442\niteration: 93501 w: [ 0.2420655  0.2405995 -0.783522  -1.567195 ] b: -0.043132794141889294 cost: 564.6399491077445\niteration: 93601 w: [ 0.2420675  0.2407995 -0.784322  -1.567295 ] b: -0.04312259277473159 cost: 564.6262190132941\niteration: 93701 w: [ 0.2420705  0.2409995 -0.785122  -1.567395 ] b: -0.04311236355779528 cost: 564.6124900187275\niteration: 93801 w: [ 0.2420735  0.2411995 -0.785922  -1.567495 ] b: -0.04310211560043256 cost: 564.598763159841\niteration: 93901 w: [ 0.242076   0.2413995 -0.786722  -1.567595 ] b: -0.04309184598897121 cost: 564.5850384690128\niteration: 94001 w: [ 0.242079   0.2415995 -0.787522  -1.567695 ] b: -0.043081551810192964 cost: 564.57131553498\niteration: 94101 w: [ 0.2420815  0.2417995 -0.788322  -1.567795 ] b: -0.04307124180852551 cost: 564.5575946772075\niteration: 94201 w: [ 0.2420845  0.2419995 -0.789122  -1.567895 ] b: -0.04306089521888538 cost: 564.5438756666762\niteration: 94301 w: [ 0.2420865  0.2421995 -0.789922  -1.567995 ] b: -0.0430505437387101 cost: 564.5301590850338\niteration: 94401 w: [ 0.24209    0.2423995 -0.790722  -1.568095 ] b: -0.04304015567263773 cost: 564.5164435567576\niteration: 94501 w: [ 0.242092   0.2425995 -0.791522  -1.568195 ] b: -0.0430297605322172 cost: 564.5027307141337\niteration: 94601 w: [ 0.242095   0.2427995 -0.792322  -1.568295 ] b: -0.04301932917231236 cost: 564.4890192097065\niteration: 94701 w: [ 0.242097   0.2429995 -0.793122  -1.5683745] b: -0.043008888920415836 cost: 564.4753490068462\niteration: 94801 w: [ 0.242099   0.2431995 -0.793922  -1.5684245] b: -0.0429984286252257 cost: 564.4617356426851\niteration: 94901 w: [ 0.2421     0.2433995 -0.794722  -1.5684745] b: -0.04298794394435063 cost: 564.4481235257192\niteration: 95001 w: [ 0.2421015  0.2435995 -0.795522  -1.5685245] b: -0.04297745418878878 cost: 564.4345121504667\niteration: 95101 w: [ 0.242103   0.2437995 -0.796322  -1.5685745] b: -0.04296696664485826 cost: 564.4209016883001\niteration: 95201 w: [ 0.2421045  0.2439995 -0.797122  -1.5686245] b: -0.04295644123694536 cost: 564.4072921307824\niteration: 95301 w: [ 0.2421055  0.2441995 -0.797922  -1.5686745] b: -0.04294590747705121 cost: 564.3936839257761\niteration: 95401 w: [ 0.2421075  0.2443995 -0.798722  -1.5687245] b: -0.04293535844378935 cost: 564.3800757468179\niteration: 95501 w: [ 0.242109   0.2445995 -0.799522  -1.5687745] b: -0.0429248105322959 cost: 564.3664689223577\niteration: 95601 w: [ 0.24211    0.2447995 -0.800322  -1.5688245] b: -0.042914231317410435 cost: 564.3528632506319\niteration: 95701 w: [ 0.2421115  0.2449995 -0.801122  -1.5688745] b: -0.0429036386530198 cost: 564.339258176895\niteration: 95801 w: [ 0.2421135  0.2451995 -0.801922  -1.5689245] b: -0.042893040190413546 cost: 564.3256538977456\niteration: 95901 w: [ 0.242115   0.2453995 -0.802722  -1.5689745] b: -0.04288242463570142 cost: 564.3120507093068\niteration: 96001 w: [ 0.242116   0.2455995 -0.803522  -1.5690245] b: -0.04287179818325529 cost: 564.2984484178504\niteration: 96101 w: [ 0.2421175  0.2457995 -0.804322  -1.5690745] b: -0.04286114589667203 cost: 564.2848469817073\niteration: 96201 w: [ 0.242119   0.2459995 -0.805122  -1.5691245] b: -0.04285049546565869 cost: 564.2712464585142\niteration: 96301 w: [ 0.2421205  0.2461995 -0.805922  -1.5691745] b: -0.04283982175149266 cost: 564.2576468429056\niteration: 96401 w: [ 0.242122   0.2463995 -0.806722  -1.5692245] b: -0.042829138600040825 cost: 564.2440481378233\niteration: 96501 w: [ 0.242123   0.2465995 -0.807522  -1.5692745] b: -0.042818422330508775 cost: 564.2304505961251\niteration: 96601 w: [ 0.242125   0.2467995 -0.808322  -1.5693245] b: -0.04280771447778303 cost: 564.2168534529061\niteration: 96701 w: [ 0.242126   0.2469995 -0.809122  -1.5693745] b: -0.042796993181550815 cost: 564.2032576050162\niteration: 96801 w: [ 0.2421275  0.2471995 -0.809922  -1.5694245] b: -0.0427862413197053 cost: 564.1896624702285\niteration: 96901 w: [ 0.242129   0.2473995 -0.810722  -1.5694745] b: -0.04277547747391657 cost: 564.1760682453745\niteration: 97001 w: [ 0.2421305  0.2475995 -0.811522  -1.5695245] b: -0.042764709295739556 cost: 564.1624749320912\niteration: 97101 w: [ 0.242132   0.2477995 -0.812322  -1.5695745] b: -0.04275393496385171 cost: 564.1488825299868\niteration: 97201 w: [ 0.2421335  0.2479995 -0.813122  -1.5696245] b: -0.0427431202319462 cost: 564.1352910316616\niteration: 97301 w: [ 0.2421355  0.248207  -0.8139145 -1.5696955] b: -0.04273230551772971 cost: 564.1217399040174\niteration: 97401 w: [ 0.2421365  0.2484555 -0.814666  -1.569747 ] b: -0.0427214804078523 cost: 564.1086323393199\niteration: 97501 w: [ 0.242138   0.2487055 -0.815416  -1.569797 ] b: -0.04271063782359111 cost: 564.0955428435747\niteration: 97601 w: [ 0.242139   0.2489555 -0.816166  -1.569847 ] b: -0.04269976614457776 cost: 564.0824545437958\niteration: 97701 w: [ 0.2421405  0.2492055 -0.816916  -1.569897 ] b: -0.04268888759656745 cost: 564.0693667172809\niteration: 97801 w: [ 0.242142   0.2494555 -0.817666  -1.569947 ] b: -0.042678017481067865 cost: 564.0562798564747\niteration: 97901 w: [ 0.242143   0.2497055 -0.818416  -1.569997 ] b: -0.04266710843574821 cost: 564.0431940338291\niteration: 98001 w: [ 0.242144   0.2499555 -0.819166  -1.570047 ] b: -0.042656191429996985 cost: 564.0301092683413\niteration: 98101 w: [ 0.2421455  0.2502055 -0.819916  -1.570097 ] b: -0.04264526209258483 cost: 564.0170247847808\niteration: 98201 w: [ 0.242147   0.2504555 -0.820666  -1.570147 ] b: -0.042634326617456024 cost: 564.003941263751\niteration: 98301 w: [ 0.242148   0.2507055 -0.821416  -1.570197 ] b: -0.042623371524695766 cost: 563.9908589750465\niteration: 98401 w: [ 0.2421495  0.2509555 -0.822166  -1.570247 ] b: -0.04261239280825489 cost: 563.9777771215216\niteration: 98501 w: [ 0.242151   0.2512055 -0.822916  -1.570297 ] b: -0.042601414514422384 cost: 563.9646962319683\niteration: 98601 w: [ 0.242152   0.2514555 -0.823666  -1.570347 ] b: -0.042590415147771406 cost: 563.9516164185434\niteration: 98701 w: [ 0.2421535  0.2517055 -0.824416  -1.570397 ] b: -0.04257941146856852 cost: 563.9385372003757\niteration: 98801 w: [ 0.2421545  0.2519555 -0.825166  -1.570447 ] b: -0.04256836886616425 cost: 563.9254591492567\niteration: 98901 w: [ 0.2421565  0.2522055 -0.825916  -1.570497 ] b: -0.04255733434055153 cost: 563.9123816403068\niteration: 99001 w: [ 0.242157   0.2524555 -0.826666  -1.570547 ] b: -0.0425462896746042 cost: 563.8993053214343\niteration: 99101 w: [ 0.2421585  0.2527055 -0.827416  -1.570597 ] b: -0.04253521993207394 cost: 563.8862294425503\niteration: 99201 w: [ 0.2421595  0.2529555 -0.828166  -1.570647 ] b: -0.04252413312891034 cost: 563.873154925134\niteration: 99301 w: [ 0.242161   0.2532055 -0.828916  -1.570697 ] b: -0.04251303764582283 cost: 563.8600807174487\niteration: 99401 w: [ 0.2421625  0.2534555 -0.829666  -1.570747 ] b: -0.04250194477709257 cost: 563.8470074742296\niteration: 99501 w: [ 0.2421635  0.2537055 -0.830416  -1.570797 ] b: -0.04249081189654357 cost: 563.8339354312387\niteration: 99601 w: [ 0.242165   0.2539555 -0.831166  -1.570847 ] b: -0.04247967325288792 cost: 563.8208638547225\niteration: 99701 w: [ 0.2421665  0.2542055 -0.831916  -1.570897 ] b: -0.04246853139665554 cost: 563.8077932413789\niteration: 99801 w: [ 0.242168   0.2544555 -0.832666  -1.570947 ] b: -0.042457368840363024 cost: 563.7947235873087\niteration: 99901 w: [ 0.2421685  0.2547055 -0.833416  -1.570997 ] b: -0.042446192506948185 cost: 563.7816551667245\niteration: 100000 w: [ 0.24217    0.254953  -0.8341585 -1.5710465] b: -0.04243509994948715 cost: 563.768717592519\n\n\n\n\n\n\nprint(\"After 100000 iterations\")\nprint(\"cost:\", cost)\n\nAfter 100000 iterations\ncost: 563.768717592519\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nplt.plot(np.arange(1,100001,1), cost_arr)\nplt.title(\"Cost vs Number of iterations\")\nplt.xlabel(\"Number of iterations\")\nplt.ylabel(\"Cost\")\nplt.show()"
  },
  {
    "objectID": "posts/mlalgos/forward_propogation_neural_networks_from_scratch.html",
    "href": "posts/mlalgos/forward_propogation_neural_networks_from_scratch.html",
    "title": "Neural Network from Scratch",
    "section": "",
    "text": "This colab contains forward propogation in a neural network written from scratch in python.\nThe aim is to create a neural network to predict whether roasting of coffee beans is done properly based its on temperature and duration.\nThe logic is the same as logistic regression.\n\n\n\nimport numpy as np\n\ndef load_coffee_data():\n    # Creates a coffee roasting data set.\n    # roasting duration: 12-15 minutes is best\n    # temperature range: 175-260C is best\n\n    rng = np.random.default_rng(2)\n    X = rng.random(400).reshape(-1,2)\n    X[:,1] = X[:,1] * 4 + 11.5          # 12-15 min is best\n    X[:,0] = X[:,0] * (285-150) + 150  # 350-500 F (175-260 C) is best\n    Y = np.zeros(len(X))\n\n    i=0\n    for t,d in X:\n        y = -3/(260-175)*t + 21\n        if (t &gt; 175 and t &lt; 260 and d &gt; 12 and d &lt; 15 and d&lt;=y ):\n            Y[i] = 1\n        else:\n            Y[i] = 0\n        i += 1\n\n    return (X, Y.reshape(-1,1))\n\n\nX,Y = load_coffee_data()\n\n\nprint(X)\n\n[[185.31763812  12.69396457]\n [259.92047498  11.86766377]\n [231.01357101  14.41424211]\n [175.3666449   11.72058651]\n [187.12086467  14.12973206]\n [225.90586448  12.10024905]\n [208.40515676  14.17718919]\n [207.07593089  14.0327376 ]\n [280.60385359  14.23225929]\n [202.86935247  12.24901028]\n [196.70468985  13.54426389]\n [270.31327028  14.60225577]\n [192.94979108  15.19686759]\n [213.57283453  14.27503537]\n [164.47298664  11.91817423]\n [177.25750542  15.03779869]\n [241.7745473   14.89694529]\n [236.99889634  13.12616959]\n [219.73805621  13.87377407]\n [266.38592796  13.25274466]\n [270.45241485  13.95486775]\n [261.96307698  13.49222422]\n [243.4899478   12.8561015 ]\n [220.58184803  12.36489356]\n [163.59498627  11.65441652]\n [244.76317931  13.32572248]\n [271.19410986  14.84073282]\n [201.98784315  15.39471508]\n [229.9283715   14.56353326]\n [204.97123839  12.28467965]\n [173.18989704  12.2248249 ]\n [231.51374483  11.95053142]\n [152.68795109  14.83198786]\n [163.42050092  13.30233814]\n [215.94730737  13.98108963]\n [218.04195512  15.24937257]\n [251.30354024  13.79786599]\n [233.33173846  13.52620597]\n [280.2428442   12.40650425]\n [243.01866352  13.72038672]\n [155.67159152  12.6846    ]\n [275.16753628  14.63825943]\n [151.73219763  12.68650532]\n [151.32372212  14.80986777]\n [164.89962496  11.72982072]\n [282.55425133  13.28347952]\n [192.98305487  11.69605356]\n [202.59536338  12.96415623]\n [220.66990639  11.52714187]\n [169.97498884  12.3395461 ]\n [209.46811003  12.70921347]\n [232.79923633  12.64173042]\n [272.80045636  15.34747983]\n [158.02370683  12.33623888]\n [226.00812974  14.58264092]\n [158.64327123  12.23921074]\n [211.65721643  14.17476303]\n [271.94927014  14.96789185]\n [257.15698108  11.71092857]\n [281.84592659  13.9585118 ]\n [161.62563505  12.5197151 ]\n [233.80180142  13.04256047]\n [210.29146919  14.71834026]\n [261.24418195  13.68714792]\n [256.98089905  13.12401972]\n [281.55504804  13.92063666]\n [280.63922766  11.67595077]\n [269.16350813  13.73750875]\n [246.34126918  12.2703799 ]\n [224.07333576  12.6571117 ]\n [164.23986438  11.51274708]\n [272.42340268  14.18361726]\n [177.67793377  12.53127471]\n [212.85523206  14.77314901]\n [165.87945639  15.37113932]\n [277.42795347  12.47864038]\n [236.50555103  12.94039013]\n [244.13865151  11.8476644 ]\n [213.44539383  13.85396321]\n [234.57422435  14.2711819 ]\n [270.33648536  12.46500084]\n [170.68123284  13.06242775]\n [226.79179994  15.34211504]\n [245.91825406  14.4537702 ]\n [281.31680226  12.57097265]\n [185.02731945  13.19016335]\n [189.881701    14.10441354]\n [278.48137931  12.11404597]\n [219.9229371   14.2103124 ]\n [216.57898499  15.15497536]\n [249.48122814  15.02870718]\n [165.0882734   12.28305401]\n [158.87007046  14.81727799]\n [279.98051934  11.55596408]\n [256.54924192  14.41132479]\n [272.60521724  12.58154189]\n [246.49491975  12.44969153]\n [160.26448201  14.48081625]\n [155.69875114  14.29837785]\n [188.26743273  13.44969358]\n [270.35697577  12.47363152]\n [213.22379155  12.92019779]\n [175.70141973  13.39460587]\n [174.52009415  14.69602997]\n [233.00092162  12.63252301]\n [281.36917436  12.88110738]\n [240.61964926  14.43289491]\n [185.80556267  11.54705521]\n [270.50314335  15.32566605]\n [172.98079126  12.11442084]\n [208.41010162  13.89027827]\n [283.51265469  15.35398447]\n [283.36013339  12.48227766]\n [230.84923224  13.24347657]\n [181.23930992  11.76150948]\n [172.77833046  12.93380692]\n [161.88293361  12.10295597]\n [156.0279505   13.99162509]\n [216.51672478  12.47421201]\n [221.05707984  13.19778711]\n [238.9856852   15.23066888]\n [197.69443437  14.08060613]\n [179.55375966  15.2595976 ]\n [233.38848487  12.13498975]\n [184.70189322  12.13660544]\n [174.18309465  12.72719543]\n [261.11450983  13.32823521]\n [187.41794561  13.17630344]\n [186.09876106  14.43472966]\n [157.93546835  12.65691424]\n [193.6382219   12.22607807]\n [249.65103076  12.22098945]\n [190.56498132  11.72590625]\n [252.00406102  12.95545207]\n [238.55033302  12.36894441]\n [152.94302629  12.78967263]\n [255.17362013  14.84978312]\n [197.09336712  14.88776311]\n [156.79710499  13.58845814]\n [184.7520262   13.25631075]\n [179.92164592  15.07426649]\n [190.79357984  15.28116771]\n [164.72717415  13.21932402]\n [209.86506596  14.33773917]\n [196.57800642  13.46997985]\n [159.51062317  12.74412583]\n [247.87288291  11.92364557]\n [212.44231569  12.44690782]\n [172.34040752  11.98526205]\n [259.8719002   14.24662808]\n [201.22657354  13.06656377]\n [248.34175919  13.9158246 ]\n [273.66206058  15.17765474]\n [215.09358488  14.13654103]\n [223.53014138  12.74114217]\n [211.22431235  14.38470346]\n [224.61209061  14.02962547]\n [215.7548952   15.31191057]\n [254.8222958   12.02314013]\n [259.90478358  15.17031031]\n [260.24885577  12.87243299]\n [199.6677145   12.47285748]\n [157.52013585  13.38824538]\n [264.8148241   14.57524634]\n [239.39685171  14.88876268]\n [238.98037311  12.39333366]\n [258.42593537  12.97008002]\n [270.15836599  12.80593012]\n [162.40676333  14.41959278]\n [164.53231154  14.98085772]\n [205.6096794   14.6204848 ]\n [157.09674149  13.67535103]\n [241.38069604  12.01802052]\n [232.13370589  12.07209046]\n [191.03853216  12.96114082]\n [233.644025    12.02047538]\n [174.9514637   14.62502635]\n [246.64321151  13.31682268]\n [188.07040705  14.26881857]\n [213.15899445  12.74607869]\n [268.08223648  12.30736127]\n [258.57818535  13.97127162]\n [237.20731698  14.22860981]\n [251.01659085  15.02378838]\n [274.27882002  12.52195619]\n [172.12463765  15.08549633]\n [177.51695425  12.38785975]\n [258.70969379  15.36444235]\n [264.01362166  13.56692157]\n [200.7060497   15.45420693]\n [249.36929914  14.01637408]\n [151.50238376  12.28076712]\n [151.82138896  15.12816166]\n [181.92285734  12.18408524]\n [228.64664273  12.31240743]\n [223.78183257  15.2991668 ]\n [266.62767329  12.48051014]\n [273.68398195  13.09756176]\n [220.61000617  12.7998907 ]\n [284.99434167  12.72829382]]\n\n\n\nprint(Y)\n\n[[1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]]\n\n\n\n\n\n\ndef Normalization_(X): # Same as tensorflow.keras.layers.Normalization(axis=-1)\n  n_samples = X.shape[0]\n  n_features = X.shape[1]\n  mean_X = np.zeros(n_features)\n  std_X = np.zeros(n_features)\n\n  for i in range(n_features):\n    for j in range(n_samples):\n      mean_X[i] += X[j][i]\n    mean_X[i]/=n_samples\n\n  for i in range(n_features):\n    for j in range(n_samples):\n      std_X[i]+=(X[j][i]-mean_X[i])**2\n    std_X[i]/=n_samples\n    std_X[i] = std_X[i]**0.5\n\n\n  for i in range(n_features):\n    for j in range(n_samples):\n      if(std_X[i]==0): std_X[i]=0.000001\n      X[j][i] = (X[j][i]-mean_X[i])/std_X[i]\n\n  return X\n\nThe normalized data looks like this:\n\nXn = Normalization_(X)\nprint(Xn)\n\n[[-0.83455487 -0.65287939]\n [ 1.03230561 -1.38514475]\n [ 0.3089396   0.87162554]\n [-1.08356841 -1.51548419]\n [-0.78943095  0.61949357]\n [ 0.18112441 -1.17902835]\n [-0.25681309  0.66154994]\n [-0.29007562  0.53353736]\n [ 1.54988623  0.7103529 ]\n [-0.39534099 -1.04719659]\n [-0.54960543  0.10065339]\n [ 1.29237479  1.0382427 ]\n [-0.64356797  1.56518596]\n [-0.12749718  0.74826093]\n [-1.35617118 -1.34038253]\n [-1.03625154  1.42421957]\n [ 0.57822214  1.2993957 ]\n [ 0.4587163  -0.26986057]\n [ 0.02678125  0.39266434]\n [ 1.19409703 -0.1576901 ]\n [ 1.29585675  0.46452933]\n [ 1.08341966  0.05453598]\n [ 0.6211483  -0.50919412]\n [ 0.04789629 -0.94450116]\n [-1.37814225 -1.57412384]\n [ 0.65300963 -0.09301737]\n [ 1.31441691  1.24958036]\n [-0.41739987  1.74051783]\n [ 0.28178358  1.00392692]\n [-0.34274342 -1.0155865 ]\n [-1.13803932 -1.0686296 ]\n [ 0.32145594 -1.31170768]\n [-1.65107977  1.2418306 ]\n [-1.38250857 -0.11374051]\n [-0.0680784   0.48776708]\n [-0.01566196  1.61171572]\n [ 0.81667554  0.32539483]\n [ 0.36694939  0.0846505 ]\n [ 1.54085233 -0.90762589]\n [ 0.60935489  0.25673292]\n [-1.57641718 -0.66117825]\n [ 1.41384789  1.07014904]\n [-1.67499653 -0.65948976]\n [-1.68521822  1.22222785]\n [-1.34549499 -1.50730086]\n [ 1.59869295 -0.13045297]\n [-0.64273558 -1.53722522]\n [-0.40219729 -0.41343633]\n [ 0.05009986 -1.68691427]\n [-1.21848914 -0.966964  ]\n [-0.23021376 -0.63936587]\n [ 0.35362406 -0.69916914]\n [ 1.35461411  1.69865809]\n [-1.51755788 -0.96989486]\n [ 0.18368349  1.02086007]\n [-1.50205391 -1.05588091]\n [-0.17543359  0.65939988]\n [ 1.33331403  1.36226833]\n [ 0.96315197 -1.52404303]\n [ 1.58096785  0.46775867]\n [-1.42742327 -0.80729876]\n [ 0.37871224 -0.34395473]\n [-0.20961003  1.14111644]\n [ 1.06543004  0.22727679]\n [ 0.9587457  -0.27176578]\n [ 1.5736889   0.43419383]\n [ 1.55077143 -1.55504025]\n [ 1.26360316  0.27190641]\n [ 0.69249973 -1.02825889]\n [ 0.13526723 -0.6855383 ]\n [-1.36200483 -1.69967088]\n [ 1.34517872  0.66724647]\n [-1.02573074 -0.79705467]\n [-0.14545446  1.18968779]\n [-1.3209757   1.71962506]\n [ 1.47041254 -0.84369906]\n [ 0.44637084 -0.43449778]\n [ 0.63738145 -1.40286813]\n [-0.13068626  0.37510802]\n [ 0.39804134  0.744846  ]\n [ 1.29295573 -0.85578638]\n [-1.2008161  -0.3263484 ]\n [ 0.20329405  1.69390383]\n [ 0.6819142   0.90665522]\n [ 1.56772704 -0.76187447]\n [-0.8418198  -0.21314948]\n [-0.72034382  0.59705637]\n [ 1.49677345 -1.16680156]\n [ 0.03140771  0.69090362]\n [-0.05227131  1.52806119]\n [ 0.77107402  1.4161627 ]\n [-1.34077425 -1.01702713]\n [-1.49637849  1.22879477]\n [ 1.53428792 -1.6613721 ]\n [ 0.94794392  0.86904023]\n [ 1.34972845 -0.75250804]\n [ 0.69634468 -0.86935345]\n [-1.46148475  0.93062335]\n [-1.57573754  0.76894698]\n [-0.76073925  0.01684545]\n [ 1.29346848 -0.8481379 ]\n [-0.13623163 -0.45239218]\n [-1.07519101 -0.03197311]\n [-1.10475248  1.12134512]\n [ 0.35867103 -0.70732872]\n [ 1.5690376  -0.48703398]\n [ 0.54932198  0.8881556 ]\n [-0.82234506 -1.66926712]\n [ 1.29712617  1.67932678]\n [-1.14327198 -1.16646935]\n [-0.25668935  0.40729032]\n [ 1.62267603  1.70442248]\n [ 1.61885934 -0.84047571]\n [ 0.30482718 -0.16590346]\n [-0.93661091 -1.47921837]\n [-1.14833836 -0.44033181]\n [-1.42098463 -1.17662949]\n [-1.56749966  0.49710358]\n [-0.05382931 -0.84762347]\n [ 0.05978848 -0.20639333]\n [ 0.50843368  1.59514056]\n [-0.52483807  0.57595831]\n [-0.9787901   1.62077711]\n [ 0.36836941 -1.14824125]\n [-0.84996327 -1.14680943]\n [-1.11318555 -0.62343031]\n [ 1.06218512 -0.09079061]\n [-0.7819968  -0.22543209]\n [-0.81500806  0.88978155]\n [-1.51976596 -0.68571329]\n [-0.62634069 -1.06751904]\n [ 0.77532316 -1.07202856]\n [-0.70324542 -1.51076985]\n [ 0.83420536 -0.42114994]\n [ 0.49753944 -0.94091131]\n [-1.64469677 -0.56806319]\n [ 0.91352037  1.25760072]\n [-0.53987917  1.29125847]\n [-1.54825235  0.1398182 ]\n [-0.84870874 -0.15452985]\n [-0.96958412  1.45653722]\n [-0.69752497  1.63989248]\n [-1.34981039 -0.18730738]\n [-0.22028034  0.80382886]\n [-0.55277555  0.03482309]\n [-1.4803493  -0.60842663]\n [ 0.73082681 -1.33553385]\n [-0.15578727 -0.87182037]\n [-1.15929693 -1.2809295 ]\n [ 1.03109008  0.72308648]\n [-0.43644987 -0.32268307]\n [ 0.74255997  0.4299294 ]\n [ 1.37617488  1.54815959]\n [-0.08944194  0.62552765]\n [ 0.12167434 -0.61107074]\n [-0.18626657  0.84544847]\n [ 0.14874903  0.53077939]\n [-0.07289332  1.66713671]\n [ 0.90472884 -1.24736205]\n [ 1.03191295  1.54165097]\n [ 1.04052301 -0.4947212 ]\n [-0.47545874 -0.84882385]\n [-1.53015923 -0.03760976]\n [ 1.15478174  1.01430703]\n [ 0.51872271  1.29214429]\n [ 0.50830076 -0.91929763]\n [ 0.99490627 -0.40818669]\n [ 1.28849847 -0.55365585]\n [-1.40787633  0.87636729]\n [-1.35468663  1.37375864]\n [-0.32676708  1.05439721]\n [-1.54075425  0.21682242]\n [ 0.56836641 -1.25189904]\n [ 0.33696984 -1.20398241]\n [-0.69139529 -0.41610858]\n [ 0.37476404 -1.24972355]\n [-1.0939579   1.05842192]\n [ 0.70005553 -0.10090436]\n [-0.76566962  0.74275162]\n [-0.13785311 -0.60669601]\n [ 1.23654543 -0.99548611]\n [ 0.99871617  0.47906639]\n [ 0.46393182  0.70711874]\n [ 0.80949492  1.41180367]\n [ 1.39160866 -0.80531272]\n [-1.16469636  1.46648908]\n [-1.02975909 -0.92414859]\n [ 1.00200704  1.71369023]\n [ 1.13473246  0.12073255]\n [-0.44947545  1.79323933]\n [ 0.76827311  0.51903606]\n [-1.68074739 -1.01905377]\n [-1.67276461  1.50429896]\n [-0.91950583 -1.10473297]\n [ 0.24970961 -0.99101422]\n [ 0.12797266  1.65584322]\n [ 1.20014646 -0.84204209]\n [ 1.37672344 -0.29521274]\n [ 0.04860092 -0.55900796]\n [ 1.65975375 -0.62245691]]\n\n\n\n\n\n\ndef sigmoid(z):\n    z = np.clip(z, -500, 500 ) # protect against overflow\n    g = 1.0/(1.0+np.exp(-z))\n    return g\n\n\n\n\n\ndef dense_(a_in, W, b, g): # This function creates a layer in our neural network\n  # W is a matrix where columns correspond to arrays w to be used\n\n  n = W.shape[1] # This is the number of neurons in the layer (as number of w arrays = number of neurons)\n  a_out = np.zeros(n) # Output/activation of this layer\n\n  for i in range(n):\n    w = W[:,i] # Extract columns from W\n    z = np.dot(w,a_in) + b[i]\n    a_out[i] = g(z)\n  return a_out\n\n\n\n\n\ndef sequential_(X, W1, b1, W2, b2):\n  a1 = dense_(X, W1, b1, sigmoid)\n  a2 = dense_(a1, W2, b2, sigmoid)\n  ans = a2\n  return ans\n\n\n\n\nInferencing a neural network is to test the predictions against pre trained values of parameters.\n\nW1_tmp = np.array( [[-8.93,  0.29, 12.9 ], [-0.1,  -7.32, 10.81]] ) # 2 X 3\nb1_tmp = np.array( [-9.82, -9.28,  0.96] )\nW2_tmp = np.array( [[-31.18], [-27.59], [-32.56]] ) # 3 X 1\nb2_tmp = np.array( [15.41] )\n\n\n\n\n\ndef predict_(X,W1_tmp,b1_tmp,W2_tmp,b2_tmp):\n  Xn = Normalization_(X)\n  n = Xn.shape[0]\n  pred = np.zeros(n)\n  for i in range(n):\n    pred[i] = sequential_(Xn[i],W1_tmp,b1_tmp,W2_tmp,b2_tmp)\n\n  yhat = (pred&gt;=0.5).astype(int)\n  return yhat\n\n\n\n\n\nX_tst = np.array([\n    [200,13.9],  # postive example\n    [200,17]])   # negative example\npredictions = predict_(X_tst, W1_tmp, b1_tmp, W2_tmp, b2_tmp)\nprint(predictions)\n\n[1 0]"
  },
  {
    "objectID": "posts/mlalgos/forward_propogation_neural_networks_from_scratch.html#generating-coffee-roasting-data",
    "href": "posts/mlalgos/forward_propogation_neural_networks_from_scratch.html#generating-coffee-roasting-data",
    "title": "Neural Network from Scratch",
    "section": "",
    "text": "import numpy as np\n\ndef load_coffee_data():\n    # Creates a coffee roasting data set.\n    # roasting duration: 12-15 minutes is best\n    # temperature range: 175-260C is best\n\n    rng = np.random.default_rng(2)\n    X = rng.random(400).reshape(-1,2)\n    X[:,1] = X[:,1] * 4 + 11.5          # 12-15 min is best\n    X[:,0] = X[:,0] * (285-150) + 150  # 350-500 F (175-260 C) is best\n    Y = np.zeros(len(X))\n\n    i=0\n    for t,d in X:\n        y = -3/(260-175)*t + 21\n        if (t &gt; 175 and t &lt; 260 and d &gt; 12 and d &lt; 15 and d&lt;=y ):\n            Y[i] = 1\n        else:\n            Y[i] = 0\n        i += 1\n\n    return (X, Y.reshape(-1,1))\n\n\nX,Y = load_coffee_data()\n\n\nprint(X)\n\n[[185.31763812  12.69396457]\n [259.92047498  11.86766377]\n [231.01357101  14.41424211]\n [175.3666449   11.72058651]\n [187.12086467  14.12973206]\n [225.90586448  12.10024905]\n [208.40515676  14.17718919]\n [207.07593089  14.0327376 ]\n [280.60385359  14.23225929]\n [202.86935247  12.24901028]\n [196.70468985  13.54426389]\n [270.31327028  14.60225577]\n [192.94979108  15.19686759]\n [213.57283453  14.27503537]\n [164.47298664  11.91817423]\n [177.25750542  15.03779869]\n [241.7745473   14.89694529]\n [236.99889634  13.12616959]\n [219.73805621  13.87377407]\n [266.38592796  13.25274466]\n [270.45241485  13.95486775]\n [261.96307698  13.49222422]\n [243.4899478   12.8561015 ]\n [220.58184803  12.36489356]\n [163.59498627  11.65441652]\n [244.76317931  13.32572248]\n [271.19410986  14.84073282]\n [201.98784315  15.39471508]\n [229.9283715   14.56353326]\n [204.97123839  12.28467965]\n [173.18989704  12.2248249 ]\n [231.51374483  11.95053142]\n [152.68795109  14.83198786]\n [163.42050092  13.30233814]\n [215.94730737  13.98108963]\n [218.04195512  15.24937257]\n [251.30354024  13.79786599]\n [233.33173846  13.52620597]\n [280.2428442   12.40650425]\n [243.01866352  13.72038672]\n [155.67159152  12.6846    ]\n [275.16753628  14.63825943]\n [151.73219763  12.68650532]\n [151.32372212  14.80986777]\n [164.89962496  11.72982072]\n [282.55425133  13.28347952]\n [192.98305487  11.69605356]\n [202.59536338  12.96415623]\n [220.66990639  11.52714187]\n [169.97498884  12.3395461 ]\n [209.46811003  12.70921347]\n [232.79923633  12.64173042]\n [272.80045636  15.34747983]\n [158.02370683  12.33623888]\n [226.00812974  14.58264092]\n [158.64327123  12.23921074]\n [211.65721643  14.17476303]\n [271.94927014  14.96789185]\n [257.15698108  11.71092857]\n [281.84592659  13.9585118 ]\n [161.62563505  12.5197151 ]\n [233.80180142  13.04256047]\n [210.29146919  14.71834026]\n [261.24418195  13.68714792]\n [256.98089905  13.12401972]\n [281.55504804  13.92063666]\n [280.63922766  11.67595077]\n [269.16350813  13.73750875]\n [246.34126918  12.2703799 ]\n [224.07333576  12.6571117 ]\n [164.23986438  11.51274708]\n [272.42340268  14.18361726]\n [177.67793377  12.53127471]\n [212.85523206  14.77314901]\n [165.87945639  15.37113932]\n [277.42795347  12.47864038]\n [236.50555103  12.94039013]\n [244.13865151  11.8476644 ]\n [213.44539383  13.85396321]\n [234.57422435  14.2711819 ]\n [270.33648536  12.46500084]\n [170.68123284  13.06242775]\n [226.79179994  15.34211504]\n [245.91825406  14.4537702 ]\n [281.31680226  12.57097265]\n [185.02731945  13.19016335]\n [189.881701    14.10441354]\n [278.48137931  12.11404597]\n [219.9229371   14.2103124 ]\n [216.57898499  15.15497536]\n [249.48122814  15.02870718]\n [165.0882734   12.28305401]\n [158.87007046  14.81727799]\n [279.98051934  11.55596408]\n [256.54924192  14.41132479]\n [272.60521724  12.58154189]\n [246.49491975  12.44969153]\n [160.26448201  14.48081625]\n [155.69875114  14.29837785]\n [188.26743273  13.44969358]\n [270.35697577  12.47363152]\n [213.22379155  12.92019779]\n [175.70141973  13.39460587]\n [174.52009415  14.69602997]\n [233.00092162  12.63252301]\n [281.36917436  12.88110738]\n [240.61964926  14.43289491]\n [185.80556267  11.54705521]\n [270.50314335  15.32566605]\n [172.98079126  12.11442084]\n [208.41010162  13.89027827]\n [283.51265469  15.35398447]\n [283.36013339  12.48227766]\n [230.84923224  13.24347657]\n [181.23930992  11.76150948]\n [172.77833046  12.93380692]\n [161.88293361  12.10295597]\n [156.0279505   13.99162509]\n [216.51672478  12.47421201]\n [221.05707984  13.19778711]\n [238.9856852   15.23066888]\n [197.69443437  14.08060613]\n [179.55375966  15.2595976 ]\n [233.38848487  12.13498975]\n [184.70189322  12.13660544]\n [174.18309465  12.72719543]\n [261.11450983  13.32823521]\n [187.41794561  13.17630344]\n [186.09876106  14.43472966]\n [157.93546835  12.65691424]\n [193.6382219   12.22607807]\n [249.65103076  12.22098945]\n [190.56498132  11.72590625]\n [252.00406102  12.95545207]\n [238.55033302  12.36894441]\n [152.94302629  12.78967263]\n [255.17362013  14.84978312]\n [197.09336712  14.88776311]\n [156.79710499  13.58845814]\n [184.7520262   13.25631075]\n [179.92164592  15.07426649]\n [190.79357984  15.28116771]\n [164.72717415  13.21932402]\n [209.86506596  14.33773917]\n [196.57800642  13.46997985]\n [159.51062317  12.74412583]\n [247.87288291  11.92364557]\n [212.44231569  12.44690782]\n [172.34040752  11.98526205]\n [259.8719002   14.24662808]\n [201.22657354  13.06656377]\n [248.34175919  13.9158246 ]\n [273.66206058  15.17765474]\n [215.09358488  14.13654103]\n [223.53014138  12.74114217]\n [211.22431235  14.38470346]\n [224.61209061  14.02962547]\n [215.7548952   15.31191057]\n [254.8222958   12.02314013]\n [259.90478358  15.17031031]\n [260.24885577  12.87243299]\n [199.6677145   12.47285748]\n [157.52013585  13.38824538]\n [264.8148241   14.57524634]\n [239.39685171  14.88876268]\n [238.98037311  12.39333366]\n [258.42593537  12.97008002]\n [270.15836599  12.80593012]\n [162.40676333  14.41959278]\n [164.53231154  14.98085772]\n [205.6096794   14.6204848 ]\n [157.09674149  13.67535103]\n [241.38069604  12.01802052]\n [232.13370589  12.07209046]\n [191.03853216  12.96114082]\n [233.644025    12.02047538]\n [174.9514637   14.62502635]\n [246.64321151  13.31682268]\n [188.07040705  14.26881857]\n [213.15899445  12.74607869]\n [268.08223648  12.30736127]\n [258.57818535  13.97127162]\n [237.20731698  14.22860981]\n [251.01659085  15.02378838]\n [274.27882002  12.52195619]\n [172.12463765  15.08549633]\n [177.51695425  12.38785975]\n [258.70969379  15.36444235]\n [264.01362166  13.56692157]\n [200.7060497   15.45420693]\n [249.36929914  14.01637408]\n [151.50238376  12.28076712]\n [151.82138896  15.12816166]\n [181.92285734  12.18408524]\n [228.64664273  12.31240743]\n [223.78183257  15.2991668 ]\n [266.62767329  12.48051014]\n [273.68398195  13.09756176]\n [220.61000617  12.7998907 ]\n [284.99434167  12.72829382]]\n\n\n\nprint(Y)\n\n[[1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]]"
  },
  {
    "objectID": "posts/mlalgos/forward_propogation_neural_networks_from_scratch.html#normalizing-the-data",
    "href": "posts/mlalgos/forward_propogation_neural_networks_from_scratch.html#normalizing-the-data",
    "title": "Neural Network from Scratch",
    "section": "",
    "text": "def Normalization_(X): # Same as tensorflow.keras.layers.Normalization(axis=-1)\n  n_samples = X.shape[0]\n  n_features = X.shape[1]\n  mean_X = np.zeros(n_features)\n  std_X = np.zeros(n_features)\n\n  for i in range(n_features):\n    for j in range(n_samples):\n      mean_X[i] += X[j][i]\n    mean_X[i]/=n_samples\n\n  for i in range(n_features):\n    for j in range(n_samples):\n      std_X[i]+=(X[j][i]-mean_X[i])**2\n    std_X[i]/=n_samples\n    std_X[i] = std_X[i]**0.5\n\n\n  for i in range(n_features):\n    for j in range(n_samples):\n      if(std_X[i]==0): std_X[i]=0.000001\n      X[j][i] = (X[j][i]-mean_X[i])/std_X[i]\n\n  return X\n\nThe normalized data looks like this:\n\nXn = Normalization_(X)\nprint(Xn)\n\n[[-0.83455487 -0.65287939]\n [ 1.03230561 -1.38514475]\n [ 0.3089396   0.87162554]\n [-1.08356841 -1.51548419]\n [-0.78943095  0.61949357]\n [ 0.18112441 -1.17902835]\n [-0.25681309  0.66154994]\n [-0.29007562  0.53353736]\n [ 1.54988623  0.7103529 ]\n [-0.39534099 -1.04719659]\n [-0.54960543  0.10065339]\n [ 1.29237479  1.0382427 ]\n [-0.64356797  1.56518596]\n [-0.12749718  0.74826093]\n [-1.35617118 -1.34038253]\n [-1.03625154  1.42421957]\n [ 0.57822214  1.2993957 ]\n [ 0.4587163  -0.26986057]\n [ 0.02678125  0.39266434]\n [ 1.19409703 -0.1576901 ]\n [ 1.29585675  0.46452933]\n [ 1.08341966  0.05453598]\n [ 0.6211483  -0.50919412]\n [ 0.04789629 -0.94450116]\n [-1.37814225 -1.57412384]\n [ 0.65300963 -0.09301737]\n [ 1.31441691  1.24958036]\n [-0.41739987  1.74051783]\n [ 0.28178358  1.00392692]\n [-0.34274342 -1.0155865 ]\n [-1.13803932 -1.0686296 ]\n [ 0.32145594 -1.31170768]\n [-1.65107977  1.2418306 ]\n [-1.38250857 -0.11374051]\n [-0.0680784   0.48776708]\n [-0.01566196  1.61171572]\n [ 0.81667554  0.32539483]\n [ 0.36694939  0.0846505 ]\n [ 1.54085233 -0.90762589]\n [ 0.60935489  0.25673292]\n [-1.57641718 -0.66117825]\n [ 1.41384789  1.07014904]\n [-1.67499653 -0.65948976]\n [-1.68521822  1.22222785]\n [-1.34549499 -1.50730086]\n [ 1.59869295 -0.13045297]\n [-0.64273558 -1.53722522]\n [-0.40219729 -0.41343633]\n [ 0.05009986 -1.68691427]\n [-1.21848914 -0.966964  ]\n [-0.23021376 -0.63936587]\n [ 0.35362406 -0.69916914]\n [ 1.35461411  1.69865809]\n [-1.51755788 -0.96989486]\n [ 0.18368349  1.02086007]\n [-1.50205391 -1.05588091]\n [-0.17543359  0.65939988]\n [ 1.33331403  1.36226833]\n [ 0.96315197 -1.52404303]\n [ 1.58096785  0.46775867]\n [-1.42742327 -0.80729876]\n [ 0.37871224 -0.34395473]\n [-0.20961003  1.14111644]\n [ 1.06543004  0.22727679]\n [ 0.9587457  -0.27176578]\n [ 1.5736889   0.43419383]\n [ 1.55077143 -1.55504025]\n [ 1.26360316  0.27190641]\n [ 0.69249973 -1.02825889]\n [ 0.13526723 -0.6855383 ]\n [-1.36200483 -1.69967088]\n [ 1.34517872  0.66724647]\n [-1.02573074 -0.79705467]\n [-0.14545446  1.18968779]\n [-1.3209757   1.71962506]\n [ 1.47041254 -0.84369906]\n [ 0.44637084 -0.43449778]\n [ 0.63738145 -1.40286813]\n [-0.13068626  0.37510802]\n [ 0.39804134  0.744846  ]\n [ 1.29295573 -0.85578638]\n [-1.2008161  -0.3263484 ]\n [ 0.20329405  1.69390383]\n [ 0.6819142   0.90665522]\n [ 1.56772704 -0.76187447]\n [-0.8418198  -0.21314948]\n [-0.72034382  0.59705637]\n [ 1.49677345 -1.16680156]\n [ 0.03140771  0.69090362]\n [-0.05227131  1.52806119]\n [ 0.77107402  1.4161627 ]\n [-1.34077425 -1.01702713]\n [-1.49637849  1.22879477]\n [ 1.53428792 -1.6613721 ]\n [ 0.94794392  0.86904023]\n [ 1.34972845 -0.75250804]\n [ 0.69634468 -0.86935345]\n [-1.46148475  0.93062335]\n [-1.57573754  0.76894698]\n [-0.76073925  0.01684545]\n [ 1.29346848 -0.8481379 ]\n [-0.13623163 -0.45239218]\n [-1.07519101 -0.03197311]\n [-1.10475248  1.12134512]\n [ 0.35867103 -0.70732872]\n [ 1.5690376  -0.48703398]\n [ 0.54932198  0.8881556 ]\n [-0.82234506 -1.66926712]\n [ 1.29712617  1.67932678]\n [-1.14327198 -1.16646935]\n [-0.25668935  0.40729032]\n [ 1.62267603  1.70442248]\n [ 1.61885934 -0.84047571]\n [ 0.30482718 -0.16590346]\n [-0.93661091 -1.47921837]\n [-1.14833836 -0.44033181]\n [-1.42098463 -1.17662949]\n [-1.56749966  0.49710358]\n [-0.05382931 -0.84762347]\n [ 0.05978848 -0.20639333]\n [ 0.50843368  1.59514056]\n [-0.52483807  0.57595831]\n [-0.9787901   1.62077711]\n [ 0.36836941 -1.14824125]\n [-0.84996327 -1.14680943]\n [-1.11318555 -0.62343031]\n [ 1.06218512 -0.09079061]\n [-0.7819968  -0.22543209]\n [-0.81500806  0.88978155]\n [-1.51976596 -0.68571329]\n [-0.62634069 -1.06751904]\n [ 0.77532316 -1.07202856]\n [-0.70324542 -1.51076985]\n [ 0.83420536 -0.42114994]\n [ 0.49753944 -0.94091131]\n [-1.64469677 -0.56806319]\n [ 0.91352037  1.25760072]\n [-0.53987917  1.29125847]\n [-1.54825235  0.1398182 ]\n [-0.84870874 -0.15452985]\n [-0.96958412  1.45653722]\n [-0.69752497  1.63989248]\n [-1.34981039 -0.18730738]\n [-0.22028034  0.80382886]\n [-0.55277555  0.03482309]\n [-1.4803493  -0.60842663]\n [ 0.73082681 -1.33553385]\n [-0.15578727 -0.87182037]\n [-1.15929693 -1.2809295 ]\n [ 1.03109008  0.72308648]\n [-0.43644987 -0.32268307]\n [ 0.74255997  0.4299294 ]\n [ 1.37617488  1.54815959]\n [-0.08944194  0.62552765]\n [ 0.12167434 -0.61107074]\n [-0.18626657  0.84544847]\n [ 0.14874903  0.53077939]\n [-0.07289332  1.66713671]\n [ 0.90472884 -1.24736205]\n [ 1.03191295  1.54165097]\n [ 1.04052301 -0.4947212 ]\n [-0.47545874 -0.84882385]\n [-1.53015923 -0.03760976]\n [ 1.15478174  1.01430703]\n [ 0.51872271  1.29214429]\n [ 0.50830076 -0.91929763]\n [ 0.99490627 -0.40818669]\n [ 1.28849847 -0.55365585]\n [-1.40787633  0.87636729]\n [-1.35468663  1.37375864]\n [-0.32676708  1.05439721]\n [-1.54075425  0.21682242]\n [ 0.56836641 -1.25189904]\n [ 0.33696984 -1.20398241]\n [-0.69139529 -0.41610858]\n [ 0.37476404 -1.24972355]\n [-1.0939579   1.05842192]\n [ 0.70005553 -0.10090436]\n [-0.76566962  0.74275162]\n [-0.13785311 -0.60669601]\n [ 1.23654543 -0.99548611]\n [ 0.99871617  0.47906639]\n [ 0.46393182  0.70711874]\n [ 0.80949492  1.41180367]\n [ 1.39160866 -0.80531272]\n [-1.16469636  1.46648908]\n [-1.02975909 -0.92414859]\n [ 1.00200704  1.71369023]\n [ 1.13473246  0.12073255]\n [-0.44947545  1.79323933]\n [ 0.76827311  0.51903606]\n [-1.68074739 -1.01905377]\n [-1.67276461  1.50429896]\n [-0.91950583 -1.10473297]\n [ 0.24970961 -0.99101422]\n [ 0.12797266  1.65584322]\n [ 1.20014646 -0.84204209]\n [ 1.37672344 -0.29521274]\n [ 0.04860092 -0.55900796]\n [ 1.65975375 -0.62245691]]"
  },
  {
    "objectID": "posts/mlalgos/forward_propogation_neural_networks_from_scratch.html#activation-function---the-sigmoid-function",
    "href": "posts/mlalgos/forward_propogation_neural_networks_from_scratch.html#activation-function---the-sigmoid-function",
    "title": "Neural Network from Scratch",
    "section": "",
    "text": "def sigmoid(z):\n    z = np.clip(z, -500, 500 ) # protect against overflow\n    g = 1.0/(1.0+np.exp(-z))\n    return g"
  },
  {
    "objectID": "posts/mlalgos/forward_propogation_neural_networks_from_scratch.html#dense-function-to-create-a-new-layer-in-our-neural-network",
    "href": "posts/mlalgos/forward_propogation_neural_networks_from_scratch.html#dense-function-to-create-a-new-layer-in-our-neural-network",
    "title": "Neural Network from Scratch",
    "section": "",
    "text": "def dense_(a_in, W, b, g): # This function creates a layer in our neural network\n  # W is a matrix where columns correspond to arrays w to be used\n\n  n = W.shape[1] # This is the number of neurons in the layer (as number of w arrays = number of neurons)\n  a_out = np.zeros(n) # Output/activation of this layer\n\n  for i in range(n):\n    w = W[:,i] # Extract columns from W\n    z = np.dot(w,a_in) + b[i]\n    a_out[i] = g(z)\n  return a_out"
  },
  {
    "objectID": "posts/mlalgos/forward_propogation_neural_networks_from_scratch.html#sequential-function-to-connect-layers-together-one-after-another",
    "href": "posts/mlalgos/forward_propogation_neural_networks_from_scratch.html#sequential-function-to-connect-layers-together-one-after-another",
    "title": "Neural Network from Scratch",
    "section": "",
    "text": "def sequential_(X, W1, b1, W2, b2):\n  a1 = dense_(X, W1, b1, sigmoid)\n  a2 = dense_(a1, W2, b2, sigmoid)\n  ans = a2\n  return ans"
  },
  {
    "objectID": "posts/mlalgos/forward_propogation_neural_networks_from_scratch.html#inferencing-our-neural-network",
    "href": "posts/mlalgos/forward_propogation_neural_networks_from_scratch.html#inferencing-our-neural-network",
    "title": "Neural Network from Scratch",
    "section": "",
    "text": "Inferencing a neural network is to test the predictions against pre trained values of parameters.\n\nW1_tmp = np.array( [[-8.93,  0.29, 12.9 ], [-0.1,  -7.32, 10.81]] ) # 2 X 3\nb1_tmp = np.array( [-9.82, -9.28,  0.96] )\nW2_tmp = np.array( [[-31.18], [-27.59], [-32.56]] ) # 3 X 1\nb2_tmp = np.array( [15.41] )"
  },
  {
    "objectID": "posts/mlalgos/forward_propogation_neural_networks_from_scratch.html#predict-function-to-predict-values-based-on-our-neural-network",
    "href": "posts/mlalgos/forward_propogation_neural_networks_from_scratch.html#predict-function-to-predict-values-based-on-our-neural-network",
    "title": "Neural Network from Scratch",
    "section": "",
    "text": "def predict_(X,W1_tmp,b1_tmp,W2_tmp,b2_tmp):\n  Xn = Normalization_(X)\n  n = Xn.shape[0]\n  pred = np.zeros(n)\n  for i in range(n):\n    pred[i] = sequential_(Xn[i],W1_tmp,b1_tmp,W2_tmp,b2_tmp)\n\n  yhat = (pred&gt;=0.5).astype(int)\n  return yhat"
  },
  {
    "objectID": "posts/mlalgos/forward_propogation_neural_networks_from_scratch.html#testing-our-neural-network",
    "href": "posts/mlalgos/forward_propogation_neural_networks_from_scratch.html#testing-our-neural-network",
    "title": "Neural Network from Scratch",
    "section": "",
    "text": "X_tst = np.array([\n    [200,13.9],  # postive example\n    [200,17]])   # negative example\npredictions = predict_(X_tst, W1_tmp, b1_tmp, W2_tmp, b2_tmp)\nprint(predictions)\n\n[1 0]"
  },
  {
    "objectID": "posts/jpeghuffman/jpeghuffman.html",
    "href": "posts/jpeghuffman/jpeghuffman.html",
    "title": "Huffman Coding in JPEG",
    "section": "",
    "text": "JPEG (Joint Photographic Experts Group) compression is a widely used method for compressing digital images. It utilizes a lossy compression technique to reduce the file size of images while attempting to preserve visual quality. Here are the general steps involved in JPEG compression:\n\nColor Space Conversion:\n\nThe first step is often to convert the image from its original color space (usually RGB for most images) to the YCbCr color space. This separates the image into luminance (Y) and chrominance (Cb and Cr) components.\n\nSubsampling:\n\nIn the YCbCr color space, chrominance information (Cb and Cr) can be subsampled. This means that some color information is reduced, which can result in significant compression without much visible loss in image quality. Common subsampling ratios are 4:4:4 (no subsampling), 4:2:2, and 4:2:0.\n\nBlock Splitting:\n\nThe image is divided into small blocks (typically 8x8 pixels). These blocks are then processed independently.\n\nDiscrete Cosine Transform (DCT):\n\nEach 8x8 block is transformed from the spatial domain to the frequency domain using DCT. This process converts the image data from its original pixel values to a set of coefficients that represent the different frequencies present in the block.\n\nQuantization:\n\nAfter the DCT, quantization is applied to the DCT coefficients. Quantization involves dividing each coefficient by a quantization matrix. This step is where significant loss of data and quality occurs. Higher quantization values result in more compression but also more loss of quality.\n\nEntropy Encoding:\n\nThe quantized DCT coefficients are then entropy-encoded. Huffman coding is often used to represent the coefficients more efficiently. Run-length encoding can also be applied to exploit the run of zeros in the quantized coefficients.\n\nHeader Information:\n\nVarious header information, such as the quantization tables, subsampling information, and image dimensions, is stored along with the compressed data to aid in decoding.\n\nWrite to File:\n\nThe compressed data, along with the header information, is written to a file. The file is typically given a “.jpg” extension.\n\nOptional Post-processing:\n\nSome post-processing may be applied to the compressed image, such as smoothing or color correction, to improve the visual quality to some extent.\n\nDecompression (Reconstruction):\n\nTo view the compressed image, it must be decompressed. The decompression process reverses the steps outlined above, starting with entropy decoding, dequantization, inverse DCT, and color space conversion.\n\n\n\n\nHuffman coding comes into play once we have the matrix of DCT coefficients.\nIn this blog, we will use the huffman library. (Refer to my blog on Huffman coding for generating huffman codes from scratch.)\n\nimport huffman\nimport numpy as np\n\n# Create a 2D array (8x8) with DCT coefficients (replace with your actual coefficients)\ndct_coefficients = np.array([\n    [20, 30, 10, 25, 30, 0, 0, 0],\n    [0, 5, 0, 0, 0, 0, 5, 0],\n    [0, 0, 15, 0, 0, 0, 20, 0],\n    [0, 0, 0, 0, 1, 0, 0, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0]\n], dtype=np.int32)\nprint(\"Initial DCT matrix\")\nprint(dct_coefficients)\n\nInitial DCT matrix\n[[20 30 10 25 30  0  0  0]\n [ 0  5  0  0  0  0  5  0]\n [ 0  0 15  0  0  0 20  0]\n [ 0  0  0  0  1  0  0  0]\n [ 0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0]]\n\n\n\n\n\nIterate through the matrix row by row.\nc is a non-zero element in the dct_coefficients matrix.\nRe calculate the number of zero elements before every non zero element (r) and the number of bits needed to represent the non-zero element (s).\nThus,\nc = Non-zero element.\nr = Number of zeros before the non-zero element.\ns = Number of bits used to represent the non-zero element.\nIn practice, we don’t iterate row by row but along diagonals in a zig-zag pattern, but the logic of generating ((r,s),c) remains the same.\n\ndef calculate_rs_pairs(dct_coefficients):\n    rs_pairs = []\n    run_length = 0\n\n    for row in dct_coefficients:\n        for element in row:\n            if element == 0:\n                run_length += 1\n            else:\n                bits_needed = len(bin(abs(element))) - 2  # Calculate the number of bits needed to represent c\n                rs_pairs.append(((run_length, bits_needed), element))\n                run_length = 0\n\n    return rs_pairs\n\n# Calculate (r, s) pairs\nrs_pairs = calculate_rs_pairs(dct_coefficients)\n\nprint(\"(r,s) pairs and c values:\")\nfor i in range(len(rs_pairs)):\n  print(rs_pairs[i])\nprint()\n\n(r,s) pairs and c values:\n((0, 5), 20)\n((0, 5), 30)\n((0, 4), 10)\n((0, 5), 25)\n((0, 5), 30)\n((4, 3), 5)\n((4, 3), 5)\n((3, 4), 15)\n((3, 5), 20)\n((5, 1), 1)\n\n\n\n\n\n\nThese (r,s) pairs repeat several times, thus, we use Huffman coding to represent them optimally.\n\n\nThis is needed to pass the pair (in the form of the symbol) to the Huffman library for Huffman coding.\n\nd = {} # maps (r,s) pair to a unique symbol (for passing in huffman library)\nd2 = {} # Inverse of d\nalph = \"abcdefghijklmnopqrstuvwxyz\"\npos=0\n\nfor (r, s), _ in rs_pairs:\n  if (r,s) in list(d.keys()):\n    continue\n  else:\n    d[(r,s)] = alph[pos]\n    d2[alph[pos]] = (r,s)\n    pos+=1\n\n\n\n\n\nfrequency_rs = {}\nfor (r,s),_ in rs_pairs:\n    if d[(r, s)] in frequency_rs:\n        frequency_rs[d[(r, s)]] += 1\n    else:\n        frequency_rs[d[(r, s)]] = 1\n\n\n\n\nThe code of a DCT matrix consists of a string of codes of every non-zero element (c), which has 2 parts:\n\nHuffman code of the (r,s) pair associated with c and\ns bits (as s is the number of bits used to represent c in binary form)\n\nSo, we know the non-zero digit c and the number of zeros before it in the dct_coefficients matrix.\n\nfor (r,s),_ in rs_pairs:\n    if d[(r, s)] in frequency_rs:\n        frequency_rs[d[(r, s)]] += 1\n    else:\n        frequency_rs[d[(r, s)]] = 1\n\n# Build Huffman codes for (r, s) pairs\nhuffman_rs = huffman.codebook([(x,frequency_rs[x]) for x in list(frequency_rs.keys())])\n\n# Create a dictionary to map (r, s) pairs to their Huffman codes\nrs_to_huffman = {pair: huffman_rs[pair] for pair in frequency_rs.keys()}\n\n\nencoded_data = \"\"\nfor (r, s), c in rs_pairs:\n    encoded_data += rs_to_huffman[d[(r, s)]] + format(c, f'0{s}b')  # Encode (r, s) and c\n    print(\"((\",r,\",\",s,\")\",c,\")   Huffman code for (r,s) pair: \",rs_to_huffman[d[(r, s)]],\" Binary representation of c: \", format(c, f'0{s}b'))\n\nprint()\nprint(\"Encoded Data:\")\nprint(encoded_data)\n\n(( 0 , 5 ) 20 )   Huffman code for (r,s) pair:  11  Binary representation of c:  10100\n(( 0 , 5 ) 30 )   Huffman code for (r,s) pair:  11  Binary representation of c:  11110\n(( 0 , 4 ) 10 )   Huffman code for (r,s) pair:  100  Binary representation of c:  1010\n(( 0 , 5 ) 25 )   Huffman code for (r,s) pair:  11  Binary representation of c:  11001\n(( 0 , 5 ) 30 )   Huffman code for (r,s) pair:  11  Binary representation of c:  11110\n(( 4 , 3 ) 5 )   Huffman code for (r,s) pair:  00  Binary representation of c:  101\n(( 4 , 3 ) 5 )   Huffman code for (r,s) pair:  00  Binary representation of c:  101\n(( 3 , 4 ) 15 )   Huffman code for (r,s) pair:  101  Binary representation of c:  1111\n(( 3 , 5 ) 20 )   Huffman code for (r,s) pair:  011  Binary representation of c:  10100\n(( 5 , 1 ) 1 )   Huffman code for (r,s) pair:  010  Binary representation of c:  1\n\nEncoded Data:\n1110100111111010010101111001111111000101001011011111011101000101\n\n\nIsn’t that amazing! Using such a small number of bits, we are able to transmit the entire DCT matrix.\n\n\n\n\n\ndecoded_data = []\ncurrent_bits = \"\"\ni=0\nwhile i&lt;len(encoded_data):\n    # print(i)\n    current_bits += encoded_data[i]\n    i+=1\n    for ch, huffman_code in rs_to_huffman.items():\n        if current_bits == huffman_code:\n            current_bits = \"\"\n            c_bits = \"\"\n            r,s = d2[ch]\n            # print(s)\n            for j in range(s):\n                c_bits += encoded_data[i+j]\n            decoded_data.append((ch,int(c_bits, 2)))\n            # print(ch,int(c_bits,2))\n            i+=s\n            # print(i)\n            c_bits = \"\"\n\nprint(\"\\nDecoded Data:\")\ndl = []\nfor ch,c in decoded_data:\n  print(d2[ch],c)\n  dl.append((d2[ch],c))\n\nnewl = []\nfor ele in dl:\n  r,s = ele[0]\n  c = ele[1]\n  for i in range(r):\n    newl.append(0)\n  newl.append(c)\n\nll = len(newl)\nfor i in range(ll+1,64+1):\n  newl.append(0)\n\nimport numpy as np\nnewl = np.array(newl)\nnewl = np.reshape(newl,(8,8))\nprint()\nprint(\"The decoded matrix is:\")\nprint(newl)\n\n\nDecoded Data:\n(0, 5) 20\n(0, 5) 30\n(0, 4) 10\n(0, 5) 25\n(0, 5) 30\n(4, 3) 5\n(4, 3) 5\n(3, 4) 15\n(3, 5) 20\n(5, 1) 1\n\nThe decoded matrix is:\n[[20 30 10 25 30  0  0  0]\n [ 0  5  0  0  0  0  5  0]\n [ 0  0 15  0  0  0 20  0]\n [ 0  0  0  0  1  0  0  0]\n [ 0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0]]\n\n\n\n\n\n\nprint(\"Initial matrix\")\nprint(dct_coefficients)\nprint()\n\nprint(\"(r,s) pairs and c values:\")\nfor i in range(len(rs_pairs)):\n  print(rs_pairs[i])\nprint()\n\nprint(\"Encoded Data:\")\nprint(encoded_data)\n\nprint(\"\\nDecoded Data:\")\nprint()\nprint(\"The decoded matrix is:\")\nprint(newl)\n\nInitial matrix\n[[20 30 10 25 30  0  0  0]\n [ 0  5  0  0  0  0  5  0]\n [ 0  0 15  0  0  0 20  0]\n [ 0  0  0  0  1  0  0  0]\n [ 0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0]]\n\n(r,s) pairs and c values:\n((0, 5), 20)\n((0, 5), 30)\n((0, 4), 10)\n((0, 5), 25)\n((0, 5), 30)\n((4, 3), 5)\n((4, 3), 5)\n((3, 4), 15)\n((3, 5), 20)\n((5, 1), 1)\n\nEncoded Data:\n1110100111111010010101111001111111000101001011011111011101000101\n\nDecoded Data:\n\nThe decoded matrix is:\n[[20 30 10 25 30  0  0  0]\n [ 0  5  0  0  0  0  5  0]\n [ 0  0 15  0  0  0 20  0]\n [ 0  0  0  0  1  0  0  0]\n [ 0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0]]"
  },
  {
    "objectID": "posts/jpeghuffman/jpeghuffman.html#huffman-coding-post-dct",
    "href": "posts/jpeghuffman/jpeghuffman.html#huffman-coding-post-dct",
    "title": "Huffman Coding in JPEG",
    "section": "",
    "text": "Huffman coding comes into play once we have the matrix of DCT coefficients.\nIn this blog, we will use the huffman library. (Refer to my blog on Huffman coding for generating huffman codes from scratch.)\n\nimport huffman\nimport numpy as np\n\n# Create a 2D array (8x8) with DCT coefficients (replace with your actual coefficients)\ndct_coefficients = np.array([\n    [20, 30, 10, 25, 30, 0, 0, 0],\n    [0, 5, 0, 0, 0, 0, 5, 0],\n    [0, 0, 15, 0, 0, 0, 20, 0],\n    [0, 0, 0, 0, 1, 0, 0, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0]\n], dtype=np.int32)\nprint(\"Initial DCT matrix\")\nprint(dct_coefficients)\n\nInitial DCT matrix\n[[20 30 10 25 30  0  0  0]\n [ 0  5  0  0  0  0  5  0]\n [ 0  0 15  0  0  0 20  0]\n [ 0  0  0  0  1  0  0  0]\n [ 0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0]]"
  },
  {
    "objectID": "posts/jpeghuffman/jpeghuffman.html#encoding-in-the-form-rs-c",
    "href": "posts/jpeghuffman/jpeghuffman.html#encoding-in-the-form-rs-c",
    "title": "Huffman Coding in JPEG",
    "section": "",
    "text": "Iterate through the matrix row by row.\nc is a non-zero element in the dct_coefficients matrix.\nRe calculate the number of zero elements before every non zero element (r) and the number of bits needed to represent the non-zero element (s).\nThus,\nc = Non-zero element.\nr = Number of zeros before the non-zero element.\ns = Number of bits used to represent the non-zero element.\nIn practice, we don’t iterate row by row but along diagonals in a zig-zag pattern, but the logic of generating ((r,s),c) remains the same.\n\ndef calculate_rs_pairs(dct_coefficients):\n    rs_pairs = []\n    run_length = 0\n\n    for row in dct_coefficients:\n        for element in row:\n            if element == 0:\n                run_length += 1\n            else:\n                bits_needed = len(bin(abs(element))) - 2  # Calculate the number of bits needed to represent c\n                rs_pairs.append(((run_length, bits_needed), element))\n                run_length = 0\n\n    return rs_pairs\n\n# Calculate (r, s) pairs\nrs_pairs = calculate_rs_pairs(dct_coefficients)\n\nprint(\"(r,s) pairs and c values:\")\nfor i in range(len(rs_pairs)):\n  print(rs_pairs[i])\nprint()\n\n(r,s) pairs and c values:\n((0, 5), 20)\n((0, 5), 30)\n((0, 4), 10)\n((0, 5), 25)\n((0, 5), 30)\n((4, 3), 5)\n((4, 3), 5)\n((3, 4), 15)\n((3, 5), 20)\n((5, 1), 1)"
  },
  {
    "objectID": "posts/jpeghuffman/jpeghuffman.html#huffman-coding-of-rs-pairs",
    "href": "posts/jpeghuffman/jpeghuffman.html#huffman-coding-of-rs-pairs",
    "title": "Huffman Coding in JPEG",
    "section": "",
    "text": "These (r,s) pairs repeat several times, thus, we use Huffman coding to represent them optimally.\n\n\nThis is needed to pass the pair (in the form of the symbol) to the Huffman library for Huffman coding.\n\nd = {} # maps (r,s) pair to a unique symbol (for passing in huffman library)\nd2 = {} # Inverse of d\nalph = \"abcdefghijklmnopqrstuvwxyz\"\npos=0\n\nfor (r, s), _ in rs_pairs:\n  if (r,s) in list(d.keys()):\n    continue\n  else:\n    d[(r,s)] = alph[pos]\n    d2[alph[pos]] = (r,s)\n    pos+=1\n\n\n\n\n\nfrequency_rs = {}\nfor (r,s),_ in rs_pairs:\n    if d[(r, s)] in frequency_rs:\n        frequency_rs[d[(r, s)]] += 1\n    else:\n        frequency_rs[d[(r, s)]] = 1\n\n\n\n\nThe code of a DCT matrix consists of a string of codes of every non-zero element (c), which has 2 parts:\n\nHuffman code of the (r,s) pair associated with c and\ns bits (as s is the number of bits used to represent c in binary form)\n\nSo, we know the non-zero digit c and the number of zeros before it in the dct_coefficients matrix.\n\nfor (r,s),_ in rs_pairs:\n    if d[(r, s)] in frequency_rs:\n        frequency_rs[d[(r, s)]] += 1\n    else:\n        frequency_rs[d[(r, s)]] = 1\n\n# Build Huffman codes for (r, s) pairs\nhuffman_rs = huffman.codebook([(x,frequency_rs[x]) for x in list(frequency_rs.keys())])\n\n# Create a dictionary to map (r, s) pairs to their Huffman codes\nrs_to_huffman = {pair: huffman_rs[pair] for pair in frequency_rs.keys()}\n\n\nencoded_data = \"\"\nfor (r, s), c in rs_pairs:\n    encoded_data += rs_to_huffman[d[(r, s)]] + format(c, f'0{s}b')  # Encode (r, s) and c\n    print(\"((\",r,\",\",s,\")\",c,\")   Huffman code for (r,s) pair: \",rs_to_huffman[d[(r, s)]],\" Binary representation of c: \", format(c, f'0{s}b'))\n\nprint()\nprint(\"Encoded Data:\")\nprint(encoded_data)\n\n(( 0 , 5 ) 20 )   Huffman code for (r,s) pair:  11  Binary representation of c:  10100\n(( 0 , 5 ) 30 )   Huffman code for (r,s) pair:  11  Binary representation of c:  11110\n(( 0 , 4 ) 10 )   Huffman code for (r,s) pair:  100  Binary representation of c:  1010\n(( 0 , 5 ) 25 )   Huffman code for (r,s) pair:  11  Binary representation of c:  11001\n(( 0 , 5 ) 30 )   Huffman code for (r,s) pair:  11  Binary representation of c:  11110\n(( 4 , 3 ) 5 )   Huffman code for (r,s) pair:  00  Binary representation of c:  101\n(( 4 , 3 ) 5 )   Huffman code for (r,s) pair:  00  Binary representation of c:  101\n(( 3 , 4 ) 15 )   Huffman code for (r,s) pair:  101  Binary representation of c:  1111\n(( 3 , 5 ) 20 )   Huffman code for (r,s) pair:  011  Binary representation of c:  10100\n(( 5 , 1 ) 1 )   Huffman code for (r,s) pair:  010  Binary representation of c:  1\n\nEncoded Data:\n1110100111111010010101111001111111000101001011011111011101000101\n\n\nIsn’t that amazing! Using such a small number of bits, we are able to transmit the entire DCT matrix."
  },
  {
    "objectID": "posts/jpeghuffman/jpeghuffman.html#decoding---finding-the-dct-matrix",
    "href": "posts/jpeghuffman/jpeghuffman.html#decoding---finding-the-dct-matrix",
    "title": "Huffman Coding in JPEG",
    "section": "",
    "text": "decoded_data = []\ncurrent_bits = \"\"\ni=0\nwhile i&lt;len(encoded_data):\n    # print(i)\n    current_bits += encoded_data[i]\n    i+=1\n    for ch, huffman_code in rs_to_huffman.items():\n        if current_bits == huffman_code:\n            current_bits = \"\"\n            c_bits = \"\"\n            r,s = d2[ch]\n            # print(s)\n            for j in range(s):\n                c_bits += encoded_data[i+j]\n            decoded_data.append((ch,int(c_bits, 2)))\n            # print(ch,int(c_bits,2))\n            i+=s\n            # print(i)\n            c_bits = \"\"\n\nprint(\"\\nDecoded Data:\")\ndl = []\nfor ch,c in decoded_data:\n  print(d2[ch],c)\n  dl.append((d2[ch],c))\n\nnewl = []\nfor ele in dl:\n  r,s = ele[0]\n  c = ele[1]\n  for i in range(r):\n    newl.append(0)\n  newl.append(c)\n\nll = len(newl)\nfor i in range(ll+1,64+1):\n  newl.append(0)\n\nimport numpy as np\nnewl = np.array(newl)\nnewl = np.reshape(newl,(8,8))\nprint()\nprint(\"The decoded matrix is:\")\nprint(newl)\n\n\nDecoded Data:\n(0, 5) 20\n(0, 5) 30\n(0, 4) 10\n(0, 5) 25\n(0, 5) 30\n(4, 3) 5\n(4, 3) 5\n(3, 4) 15\n(3, 5) 20\n(5, 1) 1\n\nThe decoded matrix is:\n[[20 30 10 25 30  0  0  0]\n [ 0  5  0  0  0  0  5  0]\n [ 0  0 15  0  0  0 20  0]\n [ 0  0  0  0  1  0  0  0]\n [ 0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0]]"
  },
  {
    "objectID": "posts/jpeghuffman/jpeghuffman.html#summary",
    "href": "posts/jpeghuffman/jpeghuffman.html#summary",
    "title": "Huffman Coding in JPEG",
    "section": "",
    "text": "print(\"Initial matrix\")\nprint(dct_coefficients)\nprint()\n\nprint(\"(r,s) pairs and c values:\")\nfor i in range(len(rs_pairs)):\n  print(rs_pairs[i])\nprint()\n\nprint(\"Encoded Data:\")\nprint(encoded_data)\n\nprint(\"\\nDecoded Data:\")\nprint()\nprint(\"The decoded matrix is:\")\nprint(newl)\n\nInitial matrix\n[[20 30 10 25 30  0  0  0]\n [ 0  5  0  0  0  0  5  0]\n [ 0  0 15  0  0  0 20  0]\n [ 0  0  0  0  1  0  0  0]\n [ 0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0]]\n\n(r,s) pairs and c values:\n((0, 5), 20)\n((0, 5), 30)\n((0, 4), 10)\n((0, 5), 25)\n((0, 5), 30)\n((4, 3), 5)\n((4, 3), 5)\n((3, 4), 15)\n((3, 5), 20)\n((5, 1), 1)\n\nEncoded Data:\n1110100111111010010101111001111111000101001011011111011101000101\n\nDecoded Data:\n\nThe decoded matrix is:\n[[20 30 10 25 30  0  0  0]\n [ 0  5  0  0  0  0  5  0]\n [ 0  0 15  0  0  0 20  0]\n [ 0  0  0  0  1  0  0  0]\n [ 0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0]]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kishan’s Blog Page",
    "section": "",
    "text": "Welcome to my Blog page! Here I’ll explain some exciting topics, based on Machine Learning, Mathematics, Coding etc., with visualisations to make it easier to grasp concepts. Every blog also contains code snippets, so you can run them locally too!\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nHuffman Coding in JPEG\n\n\n\n\n\n\n\nInformation Theory\n\n\nCoding\n\n\nJPEG\n\n\nHuffman coding\n\n\nImage compression\n\n\n\n\nHuffman coding of DCT Matrix used in JPEG image compression.\n\n\n\n\n\n\nSep 12, 2023\n\n\n\n\n\n\n  \n\n\n\n\nHuffman Coding\n\n\n\n\n\n\n\nInformation Theory\n\n\nCoding\n\n\nHuffman coding\n\n\n\n\nHuffman coding for data compression to optimize memory and transmittion time.\n\n\n\n\n\n\nAug 27, 2023\n\n\n\n\n\n\n  \n\n\n\n\nOptimizing functions\n\n\n\n\n\n\n\nML\n\n\nMath\n\n\nCoding\n\n\n\n\nUnderstanding optimization of functions using Newton’s method and L-BFGS.\n\n\n\n\n\n\nAug 20, 2023\n\n\n\n\n\n\n  \n\n\n\n\nNeural Network from Scratch\n\n\n\n\n\n\n\nML\n\n\nCoding\n\n\n\n\nThe mathematics and working of a neural netowrk is shown in this blog.\n\n\n\n\n\n\nAug 9, 2023\n\n\n\n\n\n\n  \n\n\n\n\nLinear Regression with Gradient Descent from Scratch\n\n\n\n\n\n\n\nML\n\n\nCoding\n\n\n\n\nThe mathematics and working of gradient descent algorithhm for linear regression is shown in this blog.\n\n\n\n\n\n\nAug 9, 2023\n\n\n\n\n\n\n  \n\n\n\n\nVectorized Multivariable Linear Regression from Scratch\n\n\n\n\n\n\n\nML\n\n\nCoding\n\n\n\n\nThe mathematics and working of multivariable gradient descent algorithhm using vectorization is shown in this blog.\n\n\n\n\n\n\nAug 9, 2023\n\n\n\n\n\n\n  \n\n\n\n\nHandwritten Digits Prediction using Neural Networks\n\n\n\n\n\n\n\nML\n\n\nCoding\n\n\n\n\nWorking of neural networks to predict handwritten digits is shown in this blog.\n\n\n\n\n\n\nAug 9, 2023\n\n\n\n\n\n\n  \n\n\n\n\nLogistic Regression from Scratch\n\n\n\n\n\n\n\nML\n\n\nCoding\n\n\n\n\nThe mathematics and working of logistic regression algorithhm using vectorization is shown in this blog.\n\n\n\n\n\n\nAug 9, 2023\n\n\n\n\n\n\n  \n\n\n\n\nMulticlass Segregator Neural Network\n\n\n\n\n\n\n\nML\n\n\nCoding\n\n\n\n\nWorking of Multiclass Segregator Neural Network is shown in this blog.\n\n\n\n\n\n\nAug 9, 2023\n\n\n\n\n\n\n  \n\n\n\n\nMatrix Transformations\n\n\n\n\n\n\n\nML\n\n\nMath\n\n\nCoding\n\n\n\n\nUnderstanding matrix multiplication as transformation and interpretation of low rank matrices\n\n\n\n\n\n\nJul 20, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Kishan Ved, a second year undergraduate at IIT Gandhinagar."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nIIT Gandhinagar | B.Tech | Computer Science Engineering"
  },
  {
    "objectID": "posts/huffmancoding/index.html",
    "href": "posts/huffmancoding/index.html",
    "title": "Huffman Coding",
    "section": "",
    "text": "In the world of information and data, efficiency is paramount. As we generate and consume vast amounts of digital content, the need to transmit and store this data in the most efficient manner has led to the development of various data compression techniques. One such remarkable technique is Huffman coding, which not only reduces data size but also plays a crucial role in modern data storage, transmission, and encryption. In this blog, we will delve into the mechanics of Huffman coding, its significance, and its applications across various domains.\n\n\nIn a world inundated with data, the efficiency of data storage and transmission has become paramount. Whether it’s sending files over the internet, storing data on devices with limited space, or even optimizing database structures, the ability to compress data without significant loss of information has become essential. A naive way would be to assign the same number of bits to every letter, but Huffman coding provides an optimal solution."
  },
  {
    "objectID": "posts/huffmancoding/index.html#need-for-data-compression",
    "href": "posts/huffmancoding/index.html#need-for-data-compression",
    "title": "Huffman Coding",
    "section": "",
    "text": "In a world inundated with data, the efficiency of data storage and transmission has become paramount. Whether it’s sending files over the internet, storing data on devices with limited space, or even optimizing database structures, the ability to compress data without significant loss of information has become essential. A naive way would be to assign the same number of bits to every letter, but Huffman coding provides an optimal solution."
  },
  {
    "objectID": "posts/huffmancoding/index.html#algorithm",
    "href": "posts/huffmancoding/index.html#algorithm",
    "title": "Huffman Coding",
    "section": "Algorithm",
    "text": "Algorithm\nThe Huffman coding algorithm follows a simple yet powerful process:\n\nFrequency Calculation:\nDetermine the frequency of each symbol (character) in the data to be encoded.\nConsider this long quote by Albert Einstein:\nLife is like riding a bicycle. To keep your balance, you must keep moving. The important thing is not to stop questioning. Curiosity has its own reason for existing. One cannot help but be in awe when contemplating the mysteries of eternity, of life, of the marvelous structure of reality. It is enough if one tries merely to comprehend a little of this mystery each day. The important thing is not to stop questioning; never lose a holy curiosity.\n\ns = \"Life is like riding a bicycle. To keep your balance, you must keep moving. The important thing is not to stop questioning. Curiosity has its own reason for existing. One cannot help but be in awe when contemplating the mysteries of eternity, of life, of the marvelous structure of reality. It is enough if one tries merely to comprehend a little of this mystery each day. The important thing is not to stop questioning; never lose a holy curiosity.\"\ns = s.lower()\ns_len = 0\narr = [0]*26\nd = {}\nfor i in range(len(s)):\n  if(s[i]==' ' or s[i]==',' or s[i]=='.' or s[i]==';'):\n    continue\n  s_len+=1\n  arr[ord(s[i])-ord('a')]+=1\nfor i in range(len(arr)):\n  print(chr(i+ord('a')), arr[i])\n  if arr[i]!=0:\n    d[chr(i+ord('a'))] = arr[i]\n\na 16\nb 4\nc 10\nd 3\ne 43\nf 9\ng 9\nh 14\ni 36\nj 0\nk 3\nl 14\nm 10\nn 28\no 34\np 9\nq 2\nr 19\ns 23\nt 41\nu 12\nv 3\nw 3\nx 1\ny 13\nz 0\n\n\n\n\nCreating nodes for every letter:\nThere is a node corresponding to every symbol (letter), and it contains the frequency of the letter (or the number of times it appears in the data), the huffman coding for it and also pointers to left and right nodes.\n\nclass Node:\n    def __init__(self, freq, symbol, left=None, right=None):\n        self.freq = freq\n        self.symbol = symbol\n        self.left = left\n        self.right = right\n        self.huff = ''\n\n    def __lt__(self, nxt):\n        return self.freq &lt; nxt.freq\n\n\n\nPriority Queue / Min Heap:\nCreate a priority queue (min-heap) based on the symbol frequencies. In a min heap, the value at child nodes must be greater than that at the parent node.\n\nimport heapq\n\nchars = list(d.keys())\nfreq = list(d.values())\n\nnodes = []\nfor x in range(len(chars)):\n    heapq.heappush(nodes, Node(freq[x], chars[x]))\n\n\n\nBuilding the Huffman Tree:\n\n\nMerge nodes:\nRepeatedly remove the two nodes with the lowest frequencies from the priority queue and create a new node whose frequency is the sum of the frequencies of the two nodes. Insert this new node back into the priority queue. Continue this process until there is only one node left in the priority queue. This final node represents the root of the Huffman tree.\n\n\nConstruct the Huffman tree:\nThe last node remaining in the priority queue is the root of the Huffman tree. Trace back from this root node to the leaf nodes, creating the binary code for each character as you go. Assign ‘0’ for left branches and ‘1’ for right branches. When you reach a leaf node, you have the Huffman code for that character.\n\nwhile len(nodes) &gt; 1:\n    left = heapq.heappop(nodes)\n    right = heapq.heappop(nodes)\n    left.huff = '0'\n    right.huff = '1'\n    new_symbol = left.symbol + right.symbol\n    new_freq = left.freq + right.freq\n    new_node = Node(new_freq, new_symbol, left, right)\n    heapq.heappush(nodes, new_node)\n\n\n\n\nGenerating Codes:\nTraverse the Huffman tree to assign codes to each symbol based on its position in the tree.\n\ncodes = {} \nhuffman_tree_root = nodes[0]\n\ndef huffman_codes(node, val=''):\n    newVal = val + str(node.huff)\n    if not node.left and not node.right:\n        codes[node.symbol] = newVal\n    else:\n        huffman_codes(node.left, newVal)\n        huffman_codes(node.right, newVal)\n\nhuffman_codes(huffman_tree_root)\n\nfor i in range(len(codes)):\n  print(list(codes.keys())[i],codes[list(codes.keys())[i]])\n\no 000\ni 001\nr 0100\nc 01010\nm 01011\nt 011\ne 100\ns 1010\nu 10110\nd 1011100\nv 1011101\nw 1011110\nx 10111110\nq 10111111\ny 11000\nl 11001\nn 1101\nh 11100\nk 1110100\nb 1110101\nf 111011\na 11110\ng 111110\np 111111\n\n\n\n\nVisualizing the Huffman tree\nThe huffman tree can be visualized using the graphviz python library, the following code saves an image huffman_tree.png which contains the Huffman Tree.\n\nimport graphviz\n\ndef generate_graph(node, dot=None):\n    if dot is None:\n        dot = graphviz.Digraph(format='png')\n        dot.attr(dpi='300', bgcolor='white')  # Set background color\n\n    dot.node(str(id(node)), f\"{node.symbol}:{node.freq}\", style=\"filled\", fillcolor=\"lightblue\")  # Node color\n    if node.left:\n        dot.node(str(id(node.left)), f\"{node.left.symbol}:{node.left.freq}\", style=\"filled\", fillcolor=\"lightgreen\")  # Node color\n        dot.edge(str(id(node)), str(id(node.left)), label=\"0\", color=\"blue\")  # Edge color\n        generate_graph(node.left, dot)\n    if node.right:\n        dot.node(str(id(node.right)), f\"{node.right.symbol}:{node.right.freq}\", style=\"filled\", fillcolor=\"lightgreen\")  # Node color\n        dot.edge(str(id(node)), str(id(node.right)), label=\"1\", color=\"red\")  # Edge color\n        generate_graph(node.right, dot)\n\n    return dot\n\n# Uncomment the following 2 lines before running this cell\n# graph = generate_graph(huffman_tree_root)\n# graph.render(\"huffman_tree\", cleanup=True)\n\nThe Huffman Tree obtained in our case is:"
  },
  {
    "objectID": "posts/huffmancoding/index.html#why-is-huffman-coding-better",
    "href": "posts/huffmancoding/index.html#why-is-huffman-coding-better",
    "title": "Huffman Coding",
    "section": "Why is Huffman coding better?",
    "text": "Why is Huffman coding better?\nLet’s analyze the total number of bits that we need to transmit codes. Total number of bits = Letter frequency * Number of bits used for to represent that letter\n\nprint(\"Without Huffman coding, we need 5 bits to represent every letter.\\nThis means, to transmit all data, we need to transmit a total of: \")\nl1 = len(s)*5\nprint(l1,\"bits\")\n\nl2 = 0\nfor i in range(len(codes)):\n  l2 += len(codes[list(codes.keys())[i]]) * d[list(codes.keys())[i]]\nprint(\"However, using Huffman coding, we need only:\")\nprint(l2,\"bits\")\n\nWithout Huffman coding, we need 5 bits to represent every letter.\nThis means, to transmit all data, we need to transmit a total of: \n2240 bits\nHowever, using Huffman coding, we need only:\n1485 bits"
  },
  {
    "objectID": "posts/huffmancoding/index.html#compression-ratio",
    "href": "posts/huffmancoding/index.html#compression-ratio",
    "title": "Huffman Coding",
    "section": "Compression ratio:",
    "text": "Compression ratio:\nCompression ratio stands for the number of bits needed before compression to that after compression. The higher the compression ratio, the better. The compression ratio for the above quote after Huffman coding is:\n\nprint(\"Compression ratio: \",l1,\" : \",l2,\" = \",(l1/l2))\n\nCompression ratio:  2240  :  1485  =  1.5084175084175084"
  },
  {
    "objectID": "posts/huffmancoding/index.html#entropy",
    "href": "posts/huffmancoding/index.html#entropy",
    "title": "Huffman Coding",
    "section": "Entropy",
    "text": "Entropy\nFirst, we calculate the probability of every letter in the string:\n\nprob = {}\nsum_p = 0\n\nfor i in range(len(codes)):\n  prob[list(codes.keys())[i]] = d[list(codes.keys())[i]]/s_len\n  sum_p += prob[list(codes.keys())[i]]\n\nprob\n\n{'o': 0.0947075208913649,\n 'i': 0.10027855153203342,\n 'r': 0.052924791086350974,\n 'c': 0.027855153203342618,\n 'm': 0.027855153203342618,\n 't': 0.11420612813370473,\n 'e': 0.11977715877437325,\n 's': 0.06406685236768803,\n 'u': 0.033426183844011144,\n 'd': 0.008356545961002786,\n 'v': 0.008356545961002786,\n 'w': 0.008356545961002786,\n 'x': 0.002785515320334262,\n 'q': 0.005571030640668524,\n 'y': 0.036211699164345405,\n 'l': 0.03899721448467967,\n 'n': 0.07799442896935933,\n 'h': 0.03899721448467967,\n 'k': 0.008356545961002786,\n 'b': 0.011142061281337047,\n 'f': 0.025069637883008356,\n 'a': 0.04456824512534819,\n 'g': 0.025069637883008356,\n 'p': 0.025069637883008356}\n\n\nCalculating the entropy: Entropy = -\\(\\sum_{i=1}^{n} prob[i]*log(prob[i])\\) (i denotes every character)\n\nimport numpy as np \n\nsym = list(d.keys())\nentropy = 0\n\nfor i in range(len(sym)):\n  entropy += -1*prob[sym[i]]*np.log2(prob[sym[i]])\nprint(\"Entropy of Huffman coding is:\",entropy)\n\nEntropy of Huffman coding is: 4.102837721940702"
  },
  {
    "objectID": "posts/huffmancoding/index.html#cross-entropy",
    "href": "posts/huffmancoding/index.html#cross-entropy",
    "title": "Huffman Coding",
    "section": "Cross Entropy",
    "text": "Cross Entropy\nCalculating the cross entropy: Cross entropy = -\\(\\sum_{i=1}^{n} prob[i]*log(q[i])\\) (i denotes every character) q[i] = \\(\\frac{1}{log(2^{l})}\\) where l is the length of its huffman code\nFinding q values:\n\nq = {} # Stores q values needed to find cross entropy\n\nfor i in range(len(codes)):\n  q[list(codes.keys())[i]] = np.log2(1/(2**(len(codes[list(codes.keys())[i]]))))\nq\n\n{'o': -3.0,\n 'i': -3.0,\n 'r': -4.0,\n 'c': -5.0,\n 'm': -5.0,\n 't': -3.0,\n 'e': -3.0,\n 's': -4.0,\n 'u': -5.0,\n 'd': -7.0,\n 'v': -7.0,\n 'w': -7.0,\n 'x': -8.0,\n 'q': -8.0,\n 'y': -5.0,\n 'l': -5.0,\n 'n': -4.0,\n 'h': -5.0,\n 'k': -7.0,\n 'b': -7.0,\n 'f': -6.0,\n 'a': -5.0,\n 'g': -6.0,\n 'p': -6.0}\n\n\nCalculating cross entropy:\n\ncross_entropy = 0\nsym = list(q.keys())\nlq = list(q.values())\n\nfor i in range(len(sym)):\n  cross_entropy += -1*prob[sym[i]]*q[sym[i]]\n\nprint(\"Cross entropy is:\",cross_entropy)\n\nCross entropy is: 4.13649025069638"
  },
  {
    "objectID": "posts/huffmancoding/index.html#kl-divergence",
    "href": "posts/huffmancoding/index.html#kl-divergence",
    "title": "Huffman Coding",
    "section": "KL Divergence",
    "text": "KL Divergence\nKL Divergence is the difference between cross entropy and entropy.\n\nkl_divergence = cross_entropy - entropy\nprint(\"KL Divergence is:\",kl_divergence)\n\nKL Divergence is: 0.03365252875567837"
  },
  {
    "objectID": "posts/linalg/index.html",
    "href": "posts/linalg/index.html",
    "title": "Matrix Transformations",
    "section": "",
    "text": "When a vector is multiplied by a suitable matrix, the operation essentially transforms the original vector into a new vector. This can be illustrated by the following example. The dimension of the resultant vector may also change (though it does not in this example).\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\nfrom sympy import Eq,Matrix,MatMul\n\n\n\nA = np.array([[2,1],[1,4]])\nx = np.array([1,1])\nAx = A @ x\nEq(Matrix(Ax),MatMul(Matrix(A),Matrix(x)),evaluate=False)\n\n\n\n\ndef plot_arrow(ax,v,color,label):\n    arrow = mpatches.FancyArrowPatch((0,0),(v[0],v[1]),mutation_scale=9,color=color,label=label)\n    ax.add_patch(arrow)\n    ax.legend(bbox_to_anchor=(1.6,1),borderaxespad=0)\n\n\n\ndef plot_transform(A,x):\n  Ax = A @ x\n  fig, ax = plt.subplots()\n  plot_arrow(ax,x,\"k\",f\"Original Vector x: {x}\")\n  plot_arrow(ax,Ax,\"g\",f\"Transformed vector Ax: {Ax}\")\n  plt.xlim((-5,5))\n  plt.ylim((-5,5))\n  plt.grid(alpha=0.1)\n  ax.set_aspect(\"equal\")\n\n\n\nplot_transform(np.array([[2.0,1.0],[1.0,4.0]]),np.array([1.0,1.0]))\n\n\n\n\n\n\n\ndef plot_rot(theta,v):\n  c = np.cos(theta)\n  s = np.sin(theta)\n  rot_mat = np.array([[c,-s],[s,c]])\n  w = rot_mat @ v\n  fig, ax = plt.subplots()\n  plot_arrow(ax,v,\"k\",f\"Original vector: {v}\")\n  plot_arrow(ax,w,\"g\",f\"Vector on rotation: {w}\")\n  plt.xlim((-6,6))\n  plt.ylim((-6,6))\n  plt.grid(alpha=0.4)\n  ax.set_aspect(\"equal\")\n\n\n\nplot_rot(np.pi/3,np.array([3.0,5.0]))\n\n\n\n\n\nRank of a matrix is the minimum number of linearly independent rows and columns or the number of non-zero eigenvalues in case of a square matrix.\nConsider the low rank matrix: \nDefining the function:\ndef plot_lr(v,slope):\n  A1 = np.array([1.0, 2.0])\n  A = np.vstack((A1,slope*A1)) # The low rank matrix\n  x = np.arange(-6,6,0.01)\n  y = slope*x\n  plot_transform(A,v)\n  plt.plot(x,y,lw=5,alpha=0.4,label=f\"y = {slope}x, Column Space of A\")\n  plt.legend(bbox_to_anchor=(1,1),borderaxespad=0)\nLets transform the vector [1.0 2.0] using the above transformation matrix:\nplot_lr(np.array([1.0, 2.0]),4)\nplt.tight_layout()\n\n\n\n\nA = np.array([[1.0,2.0],[4.0,8.0]])\nprint(\"The transformation matrix involved is :\")\nMatrix(A)\n\nprint(\"The rowspace of this matrix is spanned by : \")\nMatrix(np.array([1.0,2.0]))\n\nprint(\"The nullspace of a matrix is always perpendicular to the rowspace.\")\nprint(\"The nullspace of matrix A is spaned by : \")\nMatrix(np.array([-2.0,1.0]))\n\nprint(\"Any vector in the nullspace is converted to a zero matrix after transformation.\")\nplot_lr(np.array([2.0,-1.0]),4)\nplt.plot(x,-0.5*x,lw=5,alpha=0.4,label=f\"y = {-0.5}x, Nullspace of A\",color=\"g\")\nx = np.arange(-6,6,0.01)\nplt.legend(bbox_to_anchor=(1,1), borderaxespad=0)\nplt.tight_layout()\n\nConsider the vector v = [1.0, 1.0] and the same low rank transformation matrix as above.\nThe matrix obtained on transformation is: \nThe projection of v along the rowspace of A is:\nStep 1: Finding the projection matrix:\nr = np.array([1.0, 2.0])\nproj = np.outer(r,r)\nproj = proj / np.inner(r,r)\nMatrix(proj)\nThe projection matrix is: \nStep 2: Finding the projection of v along the rowspace of A\nb = np.array([1.0,1.0])\nv = proj @ b\nMatrix(v)\n\nThe matrix obtained on transformation of this projection vector is:\nAv = A @ v\nEq(MatMul(Matrix(A),Matrix(v)),Matrix(Av),evaluate=False)\n\nWe notice that this is the same as the matrix obtained before.\nLearnings:\n\nAny vector can be written as the vector sum of the projection on the rowspace and projection perpendicular to the rowspace (ie; in the nullspace). Only the component along the rowspace gets transformed to a non-zero matrix, the transformation of the component in the nullspace is always a zero matrix.\nAnalogy to PCA: As all vectors after transformation lie in the column space of A, this can be thought of as dimensionality reduction where there is a change in dimension from the initial vector space to the dimension of the column space of the transformation matrix"
  },
  {
    "objectID": "posts/linalg/index.html#matrix-multiplication-as-transformation-and-interpretation-of-low-rank-matrices",
    "href": "posts/linalg/index.html#matrix-multiplication-as-transformation-and-interpretation-of-low-rank-matrices",
    "title": "Matrix Transformations",
    "section": "",
    "text": "When a vector is multiplied by a suitable matrix, the operation essentially transforms the original vector into a new vector. This can be illustrated by the following example. The dimension of the resultant vector may also change (though it does not in this example).\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\nfrom sympy import Eq,Matrix,MatMul\n\n\n\nA = np.array([[2,1],[1,4]])\nx = np.array([1,1])\nAx = A @ x\nEq(Matrix(Ax),MatMul(Matrix(A),Matrix(x)),evaluate=False)\n\n\n\n\ndef plot_arrow(ax,v,color,label):\n    arrow = mpatches.FancyArrowPatch((0,0),(v[0],v[1]),mutation_scale=9,color=color,label=label)\n    ax.add_patch(arrow)\n    ax.legend(bbox_to_anchor=(1.6,1),borderaxespad=0)\n\n\n\ndef plot_transform(A,x):\n  Ax = A @ x\n  fig, ax = plt.subplots()\n  plot_arrow(ax,x,\"k\",f\"Original Vector x: {x}\")\n  plot_arrow(ax,Ax,\"g\",f\"Transformed vector Ax: {Ax}\")\n  plt.xlim((-5,5))\n  plt.ylim((-5,5))\n  plt.grid(alpha=0.1)\n  ax.set_aspect(\"equal\")\n\n\n\nplot_transform(np.array([[2.0,1.0],[1.0,4.0]]),np.array([1.0,1.0]))\n\n\n\n\n\n\n\ndef plot_rot(theta,v):\n  c = np.cos(theta)\n  s = np.sin(theta)\n  rot_mat = np.array([[c,-s],[s,c]])\n  w = rot_mat @ v\n  fig, ax = plt.subplots()\n  plot_arrow(ax,v,\"k\",f\"Original vector: {v}\")\n  plot_arrow(ax,w,\"g\",f\"Vector on rotation: {w}\")\n  plt.xlim((-6,6))\n  plt.ylim((-6,6))\n  plt.grid(alpha=0.4)\n  ax.set_aspect(\"equal\")\n\n\n\nplot_rot(np.pi/3,np.array([3.0,5.0]))\n\n\n\n\n\nRank of a matrix is the minimum number of linearly independent rows and columns or the number of non-zero eigenvalues in case of a square matrix.\nConsider the low rank matrix: \nDefining the function:\ndef plot_lr(v,slope):\n  A1 = np.array([1.0, 2.0])\n  A = np.vstack((A1,slope*A1)) # The low rank matrix\n  x = np.arange(-6,6,0.01)\n  y = slope*x\n  plot_transform(A,v)\n  plt.plot(x,y,lw=5,alpha=0.4,label=f\"y = {slope}x, Column Space of A\")\n  plt.legend(bbox_to_anchor=(1,1),borderaxespad=0)\nLets transform the vector [1.0 2.0] using the above transformation matrix:\nplot_lr(np.array([1.0, 2.0]),4)\nplt.tight_layout()\n\n\n\n\nA = np.array([[1.0,2.0],[4.0,8.0]])\nprint(\"The transformation matrix involved is :\")\nMatrix(A)\n\nprint(\"The rowspace of this matrix is spanned by : \")\nMatrix(np.array([1.0,2.0]))\n\nprint(\"The nullspace of a matrix is always perpendicular to the rowspace.\")\nprint(\"The nullspace of matrix A is spaned by : \")\nMatrix(np.array([-2.0,1.0]))\n\nprint(\"Any vector in the nullspace is converted to a zero matrix after transformation.\")\nplot_lr(np.array([2.0,-1.0]),4)\nplt.plot(x,-0.5*x,lw=5,alpha=0.4,label=f\"y = {-0.5}x, Nullspace of A\",color=\"g\")\nx = np.arange(-6,6,0.01)\nplt.legend(bbox_to_anchor=(1,1), borderaxespad=0)\nplt.tight_layout()\n\nConsider the vector v = [1.0, 1.0] and the same low rank transformation matrix as above.\nThe matrix obtained on transformation is: \nThe projection of v along the rowspace of A is:\nStep 1: Finding the projection matrix:\nr = np.array([1.0, 2.0])\nproj = np.outer(r,r)\nproj = proj / np.inner(r,r)\nMatrix(proj)\nThe projection matrix is: \nStep 2: Finding the projection of v along the rowspace of A\nb = np.array([1.0,1.0])\nv = proj @ b\nMatrix(v)\n\nThe matrix obtained on transformation of this projection vector is:\nAv = A @ v\nEq(MatMul(Matrix(A),Matrix(v)),Matrix(Av),evaluate=False)\n\nWe notice that this is the same as the matrix obtained before.\nLearnings:\n\nAny vector can be written as the vector sum of the projection on the rowspace and projection perpendicular to the rowspace (ie; in the nullspace). Only the component along the rowspace gets transformed to a non-zero matrix, the transformation of the component in the nullspace is always a zero matrix.\nAnalogy to PCA: As all vectors after transformation lie in the column space of A, this can be thought of as dimensionality reduction where there is a change in dimension from the initial vector space to the dimension of the column space of the transformation matrix"
  },
  {
    "objectID": "posts/mlalgos/gradient_descent_from_scratch.html",
    "href": "posts/mlalgos/gradient_descent_from_scratch.html",
    "title": "Linear Regression with Gradient Descent from Scratch",
    "section": "",
    "text": "The following describes in detail the algorithm of gradient descent for one single feature x.\n\n\n\ndef cost_func(x,y,w,b):\n  cost = 0\n  n = y.shape[0]\n  for i in range(n):\n    f = w*x[i]+b\n    cost += (f -y[i])**2\n  cost = cost/(2*n)\n  return cost\n\n\n\n\n\ndef derivative_func(x,y,w,b):\n  dw = 0\n  db = 0\n  n = y.shape[0]\n  for i in range(n):\n    f = w*x[i] + b\n    dw += (f - y[i])*x[i]\n    db += (f - y[i])\n  dw = dw/n\n  db = db/n\n  return dw, db\n\n\n\n\n\ndef grad_desc(x,y,w,b,a,n,derivative_func, cost_func):\n  cost_arr = []\n  for i in range(n):\n    dw,db = derivative_func(x,y,w,b)\n    w = w - a * dw\n    b = b - a * db\n    cost = cost_func(x,y,w,b)\n    cost_arr.append(cost)\n    if(i%100==0 or i==n-1):\n      print(\"iteration:\",i+1,\"w:\", w,\"b:\", b, \"cost:\", cost)\n  return w, b, cost, cost_arr\n\n\n\n\nThe learning rate has been refined ahead, this is just a starting value.\n\nimport numpy as np\nx_train = np.array([3,5])\ny_train = np.array([30,50])\nw = 1\nb = 1\na = 0.01\nn = 10000\n\n\nw,b,cost,cost_arr = grad_desc(x_train, y_train, w, b, a, n, derivative_func, cost_func)\n\niteration: 1 w: 2.49 b: 1.35 cost: 439.75810000000007\niteration: 101 w: 9.339565362904866 b: 2.7976459389680706 cost: 0.23024051215739977\niteration: 201 w: 9.375373101618957 b: 2.645962002116952 cost: 0.2059507823972854\niteration: 301 w: 9.409239384745288 b: 2.5025021246485672 cost: 0.18422355115792033\niteration: 401 w: 9.441269491530562 b: 2.366820414979559 cost: 0.16478848201589685\niteration: 501 w: 9.471562976553681 b: 2.2384951531462525 cost: 0.14740375828400765\niteration: 601 w: 9.500213996701634 b: 2.1171274841748158 cost: 0.1318530742588818\niteration: 701 w: 9.527311603823819 b: 2.002340178377654 cost: 0.11794294388356959\niteration: 801 w: 9.552940021519124 b: 1.8937764588645787 cost: 0.10550029333871105\niteration: 901 w: 9.577178906916032 b: 1.7910988926244504 cost: 0.09437030760858088\niteration: 1001 w: 9.600103598259423 b: 1.6939883417306432 cost: 0.08441450422840094\niteration: 1101 w: 9.621785349073628 b: 1.6021429714104642 cost: 0.07550901024591822\niteration: 1201 w: 9.642291549629508 b: 1.5152773118955152 cost: 0.06754302095871141\niteration: 1301 w: 9.661685936403957 b: 1.433121371137018 cost: 0.0604174212504073\niteration: 1401 w: 9.680028790182844 b: 1.355419795628318 cost: 0.05404355237205757\niteration: 1501 w: 9.6973771234231 b: 1.2819310767262733 cost: 0.04834210882464059\niteration: 1601 w: 9.713784857456362 b: 1.2124268000046385 cost: 0.043242151617364485\niteration: 1701 w: 9.729302990084895 b: 1.1466909353063202 cost: 0.03868022562445457\niteration: 1801 w: 9.74397975409073 b: 1.0845191652878774 cost: 0.034599570058348496\niteration: 1901 w: 9.757860767150701 b: 1.0257182503692823 cost: 0.030949412235737803\niteration: 2001 w: 9.7709891736233 b: 0.970105428115095 cost: 0.027684335849326697\niteration: 2101 w: 9.78340577864811 b: 0.9175078451802461 cost: 0.024763715885153796\niteration: 2201 w: 9.795149174974528 b: 0.8677620200548155 cost: 0.022151213154551198\niteration: 2301 w: 9.806255862914089 b: 0.8207133349379552 cost: 0.019814322151569225\niteration: 2401 w: 9.81676036378913 b: 0.7762155551615757 cost: 0.01772396661017591\niteration: 2501 w: 9.826695327230455 b: 0.7341303746701543 cost: 0.01585413772904501\niteration: 2601 w: 9.836091632657489 b: 0.694326986143886 cost: 0.014181570562607746\niteration: 2701 w: 9.844978485256348 b: 0.6566816744290933 cost: 0.012685454551954486\niteration: 2801 w: 9.853383506754124 b: 0.621077432012172 cost: 0.011347174593905366\niteration: 2901 w: 9.861332821271517 b: 0.5874035953419712 cost: 0.010150079426577727\niteration: 3001 w: 9.868851136520698 b: 0.5555555008701584 cost: 0.009079274449621191\niteration: 3101 w: 9.875961820600743 b: 0.5254341597405601 cost: 0.008121436401343444\niteration: 3201 w: 9.88268697462933 b: 0.49694595011631976 cost: 0.007264647586881216\niteration: 3301 w: 9.889047501436464 b: 0.47000232618859034 cost: 0.00649824759482768\niteration: 3401 w: 9.895063170533751 b: 0.44451954296232915 cost: 0.00581270065735045\niteration: 3501 w: 9.900752679561133 b: 0.42041839596374814 cost: 0.00519947700343959\niteration: 3601 w: 9.906133712402054 b: 0.3976239750604395 cost: 0.004650946729058455\niteration: 3701 w: 9.911222994147764 b: 0.37606543162896877 cost: 0.004160284863694492\niteration: 3801 w: 9.916036343081492 b: 0.3556757583462743 cost: 0.0037213864521273275\niteration: 3901 w: 9.920588719844154 b: 0.33639158092044336 cost: 0.0033287905948298775\niteration: 4001 w: 9.924894273934369 b: 0.3181529611134936 cost: 0.0029776125018920884\niteration: 4101 w: 9.928966387687296 b: 0.30090321044397317 cost: 0.00266348271507243\niteration: 4201 w: 9.932817717869048 b: 0.2845887139902829 cost: 0.0023824927417454853\niteration: 4301 w: 9.93646023501587 b: 0.2691587637471333 cost: 0.002131146424322084\niteration: 4401 w: 9.939905260640463 b: 0.2545654010171292 cost: 0.001906316440054949\niteration: 4501 w: 9.943163502421001 b: 0.2407632673476406 cost: 0.0017052053899958256\niteration: 4601 w: 9.946245087482273 b: 0.2277094635496474 cost: 0.0015253109929568177\niteration: 4701 w: 9.949159593872366 b: 0.21536341636035097 cost: 0.001364394951414451\niteration: 4801 w: 9.951916080332746 b: 0.2036867523351264 cost: 0.001220455101970358\niteration: 4901 w: 9.95452311445423 b: 0.19264317857686616 cost: 0.0010917005038612597\niteration: 5001 w: 9.956988799306398 b: 0.1821983699319763 cost: 0.0009765291555642971\niteration: 5101 w: 9.959320798623219 b: 0.17231986230243798 cost: 0.0008735080622334296\niteration: 5201 w: 9.961526360623111 b: 0.16297695174230992 cost: 0.0007813554059691848\niteration: 5301 w: 9.963612340537546 b: 0.1541405990250685 cost: 0.0006989245970740047\niteration: 5401 w: 9.965585221918182 b: 0.14578333938515137 cost: 0.0006251900078543284\niteration: 5501 w: 9.967451136788755 b: 0.1378791971531778 cost: 0.0005592342114691611\niteration: 5601 w: 9.969215884704363 b: 0.13040360501950188 cost: 0.0005002365670410818\niteration: 5701 w: 9.970884950777405 b: 0.12333332767517048 cost: 0.00044746300900952603\niteration: 5801 w: 9.97246352272614 b: 0.11664638959295495 cost: 0.00040025691367620076\niteration: 5901 w: 9.97395650699895 b: 0.11032200672398479 cost: 0.0003580309293056527\niteration: 6001 w: 9.975368544024345 b: 0.10434052189766362 cost: 0.00032025966812710625\niteration: 6101 w: 9.976704022634133 b: 0.09868334372411054 cost: 0.00028647316930910654\niteration: 6201 w: 9.977967093704596 b: 0.09333288880922198 cost: 0.00025625105157298534\niteration: 6301 w: 9.979161683058063 b: 0.08827252710273104 cost: 0.0002292172826886905\niteration: 6401 w: 9.980291503664992 b: 0.08348653020940747 cost: 0.00020503550077424578\niteration: 6501 w: 9.981360067184436 b: 0.07896002250274999 cost: 0.0001834048291849217\niteration: 6601 w: 9.982370694878863 b: 0.07467893488921702 cost: 0.00016405613292008877\niteration: 6701 w: 9.983326527937127 b: 0.07062996107927426 cost: 0.00014674866996850262\niteration: 6801 w: 9.984230537237789 b: 0.06680051622937841 cost: 0.00013126709592753912\niteration: 6901 w: 9.985085532583073 b: 0.06317869782630874 cost: 0.00011741878462651558\niteration: 7001 w: 9.985894171432166 b: 0.0597532486922987 cost: 0.00010503143141656318\niteration: 7101 w: 9.986658967161024 b: 0.05651352199596749 cost: 9.395090930719517e-05\niteration: 7201 w: 9.987382296874317 b: 0.05344944816030203 cost: 8.403935127422673e-05\niteration: 7301 w: 9.988066408793873 b: 0.05055150356484316 cost: 7.517343487865789e-05\niteration: 7401 w: 9.98871342924648 b: 0.04781068094477002 cost: 6.724284785364107e-05\niteration: 7501 w: 9.989325369272846 b: 0.04521846139493253 cost: 6.014891556793781e-05\niteration: 7601 w: 9.989904130878234 b: 0.0427667878917476 cost: 5.380337328775985e-05\niteration: 7701 w: 9.990451512944183 b: 0.040448040250715044 cost: 4.812726796165485e-05\niteration: 7801 w: 9.990969216819725 b: 0.03825501144170068 cost: 4.3049975864271824e-05\niteration: 7901 w: 9.991458851609437 b: 0.03618088518834209 cost: 3.850832387553253e-05\niteration: 8001 w: 9.991921939174778 b: 0.034219214782015284 cost: 3.4445803462908096e-05\niteration: 8101 w: 9.992359918864297 b: 0.03236390304444657 cost: 3.081186758585819e-05\niteration: 8201 w: 9.992774151987335 b: 0.030609183376727626 cost: 2.756130177513599e-05\niteration: 8301 w: 9.993165926045192 b: 0.02894960183582901 cost: 2.4653661561485263e-05\niteration: 8401 w: 9.993536458732882 b: 0.027380000182893945 cost: 2.205276925405976e-05\niteration: 8501 w: 9.993886901723917 b: 0.025895499850621943 cost: 1.97262638071178e-05\niteration: 8601 w: 9.994218344249902 b: 0.024491486779919497 cost: 1.7645198174645112e-05\niteration: 8701 w: 9.994531816486013 b: 0.02316359707868738 cost: 1.5783679143059506e-05\niteration: 8801 w: 9.994828292752969 b: 0.021907703458153932 cost: 1.4118545160273562e-05\niteration: 8901 w: 9.995108694545348 b: 0.020719902404621453 cost: 1.262907815320633e-05\niteration: 9001 w: 9.995373893395755 b: 0.019596502046739055 cost: 1.1296745747482045e-05\niteration: 9101 w: 9.995624713583677 b: 0.018534010680580106 cost: 1.0104970682320256e-05\niteration: 9201 w: 9.99586193469746 b: 0.01752912591688735 cost: 9.038924551674657e-06\niteration: 9301 w: 9.996086294057417 b: 0.016578724416733255 cost: 8.085343304708596e-06\niteration: 9401 w: 9.996298489007502 b: 0.015679852183681973 cost: 7.23236221095301e-06\niteration: 9501 w: 9.996499179082797 b: 0.014829715382324277 cost: 6.469368235706646e-06\niteration: 9601 w: 9.996688988059427 b: 0.014025671654583972 cost: 5.786867989809793e-06\niteration: 9701 w: 9.996868505893358 b: 0.013265221906866779 cost: 5.176369610027965e-06\niteration: 9801 w: 9.997038290554087 b: 0.012546002542482825 cost: 4.630277100996098e-06\niteration: 9901 w: 9.997198869758881 b: 0.011865778115217215 cost: 4.141795823558715e-06\niteration: 10000 w: 9.997349265409245 b: 0.011228691916751294 cost: 3.7089806903241235e-06\n\n\n###Result of the model\n\nprint(\"After 10000 iterations\")\nprint(\"w:\", w,\"b:\", b, \"cost:\", cost)\n\nAfter 10000 iterations\nw: 9.997349265409245 b: 0.011228691916751294 cost: 3.7089806903241235e-06\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nplt.plot(np.arange(1,10001,1), cost_arr)\nplt.xlim([0,100])\nplt.title(\"Cost vs Number of iterations\")\nplt.xlabel(\"Number of iterations\")\nplt.ylabel(\"Cost\")\nplt.show()"
  },
  {
    "objectID": "posts/mlalgos/gradient_descent_from_scratch.html#gradient-descent-for-one-feature",
    "href": "posts/mlalgos/gradient_descent_from_scratch.html#gradient-descent-for-one-feature",
    "title": "Linear Regression with Gradient Descent from Scratch",
    "section": "",
    "text": "The following describes in detail the algorithm of gradient descent for one single feature x.\n\n\n\ndef cost_func(x,y,w,b):\n  cost = 0\n  n = y.shape[0]\n  for i in range(n):\n    f = w*x[i]+b\n    cost += (f -y[i])**2\n  cost = cost/(2*n)\n  return cost\n\n\n\n\n\ndef derivative_func(x,y,w,b):\n  dw = 0\n  db = 0\n  n = y.shape[0]\n  for i in range(n):\n    f = w*x[i] + b\n    dw += (f - y[i])*x[i]\n    db += (f - y[i])\n  dw = dw/n\n  db = db/n\n  return dw, db\n\n\n\n\n\ndef grad_desc(x,y,w,b,a,n,derivative_func, cost_func):\n  cost_arr = []\n  for i in range(n):\n    dw,db = derivative_func(x,y,w,b)\n    w = w - a * dw\n    b = b - a * db\n    cost = cost_func(x,y,w,b)\n    cost_arr.append(cost)\n    if(i%100==0 or i==n-1):\n      print(\"iteration:\",i+1,\"w:\", w,\"b:\", b, \"cost:\", cost)\n  return w, b, cost, cost_arr\n\n\n\n\nThe learning rate has been refined ahead, this is just a starting value.\n\nimport numpy as np\nx_train = np.array([3,5])\ny_train = np.array([30,50])\nw = 1\nb = 1\na = 0.01\nn = 10000\n\n\nw,b,cost,cost_arr = grad_desc(x_train, y_train, w, b, a, n, derivative_func, cost_func)\n\niteration: 1 w: 2.49 b: 1.35 cost: 439.75810000000007\niteration: 101 w: 9.339565362904866 b: 2.7976459389680706 cost: 0.23024051215739977\niteration: 201 w: 9.375373101618957 b: 2.645962002116952 cost: 0.2059507823972854\niteration: 301 w: 9.409239384745288 b: 2.5025021246485672 cost: 0.18422355115792033\niteration: 401 w: 9.441269491530562 b: 2.366820414979559 cost: 0.16478848201589685\niteration: 501 w: 9.471562976553681 b: 2.2384951531462525 cost: 0.14740375828400765\niteration: 601 w: 9.500213996701634 b: 2.1171274841748158 cost: 0.1318530742588818\niteration: 701 w: 9.527311603823819 b: 2.002340178377654 cost: 0.11794294388356959\niteration: 801 w: 9.552940021519124 b: 1.8937764588645787 cost: 0.10550029333871105\niteration: 901 w: 9.577178906916032 b: 1.7910988926244504 cost: 0.09437030760858088\niteration: 1001 w: 9.600103598259423 b: 1.6939883417306432 cost: 0.08441450422840094\niteration: 1101 w: 9.621785349073628 b: 1.6021429714104642 cost: 0.07550901024591822\niteration: 1201 w: 9.642291549629508 b: 1.5152773118955152 cost: 0.06754302095871141\niteration: 1301 w: 9.661685936403957 b: 1.433121371137018 cost: 0.0604174212504073\niteration: 1401 w: 9.680028790182844 b: 1.355419795628318 cost: 0.05404355237205757\niteration: 1501 w: 9.6973771234231 b: 1.2819310767262733 cost: 0.04834210882464059\niteration: 1601 w: 9.713784857456362 b: 1.2124268000046385 cost: 0.043242151617364485\niteration: 1701 w: 9.729302990084895 b: 1.1466909353063202 cost: 0.03868022562445457\niteration: 1801 w: 9.74397975409073 b: 1.0845191652878774 cost: 0.034599570058348496\niteration: 1901 w: 9.757860767150701 b: 1.0257182503692823 cost: 0.030949412235737803\niteration: 2001 w: 9.7709891736233 b: 0.970105428115095 cost: 0.027684335849326697\niteration: 2101 w: 9.78340577864811 b: 0.9175078451802461 cost: 0.024763715885153796\niteration: 2201 w: 9.795149174974528 b: 0.8677620200548155 cost: 0.022151213154551198\niteration: 2301 w: 9.806255862914089 b: 0.8207133349379552 cost: 0.019814322151569225\niteration: 2401 w: 9.81676036378913 b: 0.7762155551615757 cost: 0.01772396661017591\niteration: 2501 w: 9.826695327230455 b: 0.7341303746701543 cost: 0.01585413772904501\niteration: 2601 w: 9.836091632657489 b: 0.694326986143886 cost: 0.014181570562607746\niteration: 2701 w: 9.844978485256348 b: 0.6566816744290933 cost: 0.012685454551954486\niteration: 2801 w: 9.853383506754124 b: 0.621077432012172 cost: 0.011347174593905366\niteration: 2901 w: 9.861332821271517 b: 0.5874035953419712 cost: 0.010150079426577727\niteration: 3001 w: 9.868851136520698 b: 0.5555555008701584 cost: 0.009079274449621191\niteration: 3101 w: 9.875961820600743 b: 0.5254341597405601 cost: 0.008121436401343444\niteration: 3201 w: 9.88268697462933 b: 0.49694595011631976 cost: 0.007264647586881216\niteration: 3301 w: 9.889047501436464 b: 0.47000232618859034 cost: 0.00649824759482768\niteration: 3401 w: 9.895063170533751 b: 0.44451954296232915 cost: 0.00581270065735045\niteration: 3501 w: 9.900752679561133 b: 0.42041839596374814 cost: 0.00519947700343959\niteration: 3601 w: 9.906133712402054 b: 0.3976239750604395 cost: 0.004650946729058455\niteration: 3701 w: 9.911222994147764 b: 0.37606543162896877 cost: 0.004160284863694492\niteration: 3801 w: 9.916036343081492 b: 0.3556757583462743 cost: 0.0037213864521273275\niteration: 3901 w: 9.920588719844154 b: 0.33639158092044336 cost: 0.0033287905948298775\niteration: 4001 w: 9.924894273934369 b: 0.3181529611134936 cost: 0.0029776125018920884\niteration: 4101 w: 9.928966387687296 b: 0.30090321044397317 cost: 0.00266348271507243\niteration: 4201 w: 9.932817717869048 b: 0.2845887139902829 cost: 0.0023824927417454853\niteration: 4301 w: 9.93646023501587 b: 0.2691587637471333 cost: 0.002131146424322084\niteration: 4401 w: 9.939905260640463 b: 0.2545654010171292 cost: 0.001906316440054949\niteration: 4501 w: 9.943163502421001 b: 0.2407632673476406 cost: 0.0017052053899958256\niteration: 4601 w: 9.946245087482273 b: 0.2277094635496474 cost: 0.0015253109929568177\niteration: 4701 w: 9.949159593872366 b: 0.21536341636035097 cost: 0.001364394951414451\niteration: 4801 w: 9.951916080332746 b: 0.2036867523351264 cost: 0.001220455101970358\niteration: 4901 w: 9.95452311445423 b: 0.19264317857686616 cost: 0.0010917005038612597\niteration: 5001 w: 9.956988799306398 b: 0.1821983699319763 cost: 0.0009765291555642971\niteration: 5101 w: 9.959320798623219 b: 0.17231986230243798 cost: 0.0008735080622334296\niteration: 5201 w: 9.961526360623111 b: 0.16297695174230992 cost: 0.0007813554059691848\niteration: 5301 w: 9.963612340537546 b: 0.1541405990250685 cost: 0.0006989245970740047\niteration: 5401 w: 9.965585221918182 b: 0.14578333938515137 cost: 0.0006251900078543284\niteration: 5501 w: 9.967451136788755 b: 0.1378791971531778 cost: 0.0005592342114691611\niteration: 5601 w: 9.969215884704363 b: 0.13040360501950188 cost: 0.0005002365670410818\niteration: 5701 w: 9.970884950777405 b: 0.12333332767517048 cost: 0.00044746300900952603\niteration: 5801 w: 9.97246352272614 b: 0.11664638959295495 cost: 0.00040025691367620076\niteration: 5901 w: 9.97395650699895 b: 0.11032200672398479 cost: 0.0003580309293056527\niteration: 6001 w: 9.975368544024345 b: 0.10434052189766362 cost: 0.00032025966812710625\niteration: 6101 w: 9.976704022634133 b: 0.09868334372411054 cost: 0.00028647316930910654\niteration: 6201 w: 9.977967093704596 b: 0.09333288880922198 cost: 0.00025625105157298534\niteration: 6301 w: 9.979161683058063 b: 0.08827252710273104 cost: 0.0002292172826886905\niteration: 6401 w: 9.980291503664992 b: 0.08348653020940747 cost: 0.00020503550077424578\niteration: 6501 w: 9.981360067184436 b: 0.07896002250274999 cost: 0.0001834048291849217\niteration: 6601 w: 9.982370694878863 b: 0.07467893488921702 cost: 0.00016405613292008877\niteration: 6701 w: 9.983326527937127 b: 0.07062996107927426 cost: 0.00014674866996850262\niteration: 6801 w: 9.984230537237789 b: 0.06680051622937841 cost: 0.00013126709592753912\niteration: 6901 w: 9.985085532583073 b: 0.06317869782630874 cost: 0.00011741878462651558\niteration: 7001 w: 9.985894171432166 b: 0.0597532486922987 cost: 0.00010503143141656318\niteration: 7101 w: 9.986658967161024 b: 0.05651352199596749 cost: 9.395090930719517e-05\niteration: 7201 w: 9.987382296874317 b: 0.05344944816030203 cost: 8.403935127422673e-05\niteration: 7301 w: 9.988066408793873 b: 0.05055150356484316 cost: 7.517343487865789e-05\niteration: 7401 w: 9.98871342924648 b: 0.04781068094477002 cost: 6.724284785364107e-05\niteration: 7501 w: 9.989325369272846 b: 0.04521846139493253 cost: 6.014891556793781e-05\niteration: 7601 w: 9.989904130878234 b: 0.0427667878917476 cost: 5.380337328775985e-05\niteration: 7701 w: 9.990451512944183 b: 0.040448040250715044 cost: 4.812726796165485e-05\niteration: 7801 w: 9.990969216819725 b: 0.03825501144170068 cost: 4.3049975864271824e-05\niteration: 7901 w: 9.991458851609437 b: 0.03618088518834209 cost: 3.850832387553253e-05\niteration: 8001 w: 9.991921939174778 b: 0.034219214782015284 cost: 3.4445803462908096e-05\niteration: 8101 w: 9.992359918864297 b: 0.03236390304444657 cost: 3.081186758585819e-05\niteration: 8201 w: 9.992774151987335 b: 0.030609183376727626 cost: 2.756130177513599e-05\niteration: 8301 w: 9.993165926045192 b: 0.02894960183582901 cost: 2.4653661561485263e-05\niteration: 8401 w: 9.993536458732882 b: 0.027380000182893945 cost: 2.205276925405976e-05\niteration: 8501 w: 9.993886901723917 b: 0.025895499850621943 cost: 1.97262638071178e-05\niteration: 8601 w: 9.994218344249902 b: 0.024491486779919497 cost: 1.7645198174645112e-05\niteration: 8701 w: 9.994531816486013 b: 0.02316359707868738 cost: 1.5783679143059506e-05\niteration: 8801 w: 9.994828292752969 b: 0.021907703458153932 cost: 1.4118545160273562e-05\niteration: 8901 w: 9.995108694545348 b: 0.020719902404621453 cost: 1.262907815320633e-05\niteration: 9001 w: 9.995373893395755 b: 0.019596502046739055 cost: 1.1296745747482045e-05\niteration: 9101 w: 9.995624713583677 b: 0.018534010680580106 cost: 1.0104970682320256e-05\niteration: 9201 w: 9.99586193469746 b: 0.01752912591688735 cost: 9.038924551674657e-06\niteration: 9301 w: 9.996086294057417 b: 0.016578724416733255 cost: 8.085343304708596e-06\niteration: 9401 w: 9.996298489007502 b: 0.015679852183681973 cost: 7.23236221095301e-06\niteration: 9501 w: 9.996499179082797 b: 0.014829715382324277 cost: 6.469368235706646e-06\niteration: 9601 w: 9.996688988059427 b: 0.014025671654583972 cost: 5.786867989809793e-06\niteration: 9701 w: 9.996868505893358 b: 0.013265221906866779 cost: 5.176369610027965e-06\niteration: 9801 w: 9.997038290554087 b: 0.012546002542482825 cost: 4.630277100996098e-06\niteration: 9901 w: 9.997198869758881 b: 0.011865778115217215 cost: 4.141795823558715e-06\niteration: 10000 w: 9.997349265409245 b: 0.011228691916751294 cost: 3.7089806903241235e-06\n\n\n###Result of the model\n\nprint(\"After 10000 iterations\")\nprint(\"w:\", w,\"b:\", b, \"cost:\", cost)\n\nAfter 10000 iterations\nw: 9.997349265409245 b: 0.011228691916751294 cost: 3.7089806903241235e-06\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nplt.plot(np.arange(1,10001,1), cost_arr)\nplt.xlim([0,100])\nplt.title(\"Cost vs Number of iterations\")\nplt.xlabel(\"Number of iterations\")\nplt.ylabel(\"Cost\")\nplt.show()"
  },
  {
    "objectID": "posts/mlalgos/gradient_descent_from_scratch.html#finding-an-optimal-learning-rate",
    "href": "posts/mlalgos/gradient_descent_from_scratch.html#finding-an-optimal-learning-rate",
    "title": "Linear Regression with Gradient Descent from Scratch",
    "section": "Finding an optimal learning rate",
    "text": "Finding an optimal learning rate\nLet’s plot the cost for a variety of learning rates.\n\nw = 1\nb = 1\nn = 40\na = [0.001,0.003,0.01,0.03,0.1]\ny = []\nfor i in range(len(a)):\n  w,b,cost,cost_arr = grad_desc(x_train, y_train, w, b, a[i], n, derivative_func, cost_func)\n  y.append(cost_arr)\n\nfor i in range(len(a)):\n  plt.plot(np.arange(0,40,1),y[i],label=a[i])\nplt.legend()\nplt.show()\n\niteration: 1 w: 1.149 b: 1.035 cost: 629.784181\niteration: 40 w: 5.279480251821813 b: 2.003290665567403 cost: 153.58840104275913\niteration: 1 w: 5.496187270992092 b: 2.0539270305488393 cost: 137.52409463762862\niteration: 40 w: 8.867665170298114 b: 2.8295651380794244 cost: 2.0857072160129806\niteration: 1 w: 8.946979485824258 b: 2.8465628798867058 cost: 1.4867474127639555\niteration: 40 w: 9.32262651466889 b: 2.8686787258659976 cost: 0.24208731712957882\niteration: 1 w: 9.323845545083836 b: 2.863903182329751 cost: 0.2412783368311639\niteration: 40 w: 9.366629881783739 b: 2.6829988756811596 cost: 0.21175672971498352\niteration: 1 w: 9.370159532478919 b: 2.6680470353995482 cost: 0.20940314645599492\niteration: 40 w: 9.493502617986875 b: 2.145557340633284 cost: 0.1354180298957514\n\n\n\n\n\nIt is clear that 0.0001 is very slow and 0.1 is large but not too large that the cost increases with number of iterations."
  },
  {
    "objectID": "posts/mlalgos/gradient_descent_from_scratch.html#visualizing-fast-verses-slow-learning-rates",
    "href": "posts/mlalgos/gradient_descent_from_scratch.html#visualizing-fast-verses-slow-learning-rates",
    "title": "Linear Regression with Gradient Descent from Scratch",
    "section": "Visualizing fast verses slow learning rates",
    "text": "Visualizing fast verses slow learning rates\n\nw = 1\nb = 1\nn = 100\na = [0.001,0.1]\ny = []\nfor i in range(len(a)):\n  w,b,cost,cost_arr = grad_desc(x_train, y_train, w, b, a[i], n, derivative_func, cost_func)\n  y.append(cost_arr)\n\nfor i in range(len(a)):\n  plt.plot(np.arange(0,100,1),y[i],label=a[i])\nplt.legend()\nplt.show()\n\niteration: 1 w: 1.149 b: 1.035 cost: 629.784181\niteration: 100 w: 7.9476347185687555 b: 2.6227491584391602 cost: 17.71177692691945\niteration: 1 w: 10.387556033626208 b: 3.181420355167742 cost: 11.269329627924463\niteration: 100 w: 9.602662229726635 b: 1.6831498042914486 cost: 0.08333775207969249\n\n\n\n\n\nConclusions:\n0.001 - Slow learning rate\n0.01 - Fast learning rate"
  },
  {
    "objectID": "posts/mlalgos/gradient_descent_from_scratch.html#checking-if-we-can-do-better",
    "href": "posts/mlalgos/gradient_descent_from_scratch.html#checking-if-we-can-do-better",
    "title": "Linear Regression with Gradient Descent from Scratch",
    "section": "Checking if we can do better",
    "text": "Checking if we can do better\nIncrease the learning rate and check if the cost function still continues to decrease.\n\nw = 1\nb = 1\nn = 100\na = [0.001,0.1,0.3]\ny = []\nfor i in range(len(a)):\n  w,b,cost,cost_arr = grad_desc(x_train, y_train, w, b, a[i], n, derivative_func, cost_func)\n  y.append(cost_arr)\n\nfor i in range(len(a)):\n  plt.plot(np.arange(0,100,1),y[i],label=a[i])\nplt.legend()\nplt.show()\n\niteration: 1 w: 1.149 b: 1.035 cost: 629.784181\niteration: 100 w: 7.9476347185687555 b: 2.6227491584391602 cost: 17.71177692691945\niteration: 1 w: 10.387556033626208 b: 3.181420355167742 cost: 11.269329627924463\niteration: 100 w: 9.602662229726635 b: 1.6831498042914486 cost: 0.08333775207969249\niteration: 1 w: 9.609305092971061 b: 1.6550101873320526 cost: 0.08057449321585036\niteration: 100 w: -2.0801856610004073e+54 b: -4.910652218164293e+53 cost: 4.098756512416235e+109"
  },
  {
    "objectID": "posts/mlalgos/gradient_descent_from_scratch.html#conclusion-set-the-learning-rate-to-0.1",
    "href": "posts/mlalgos/gradient_descent_from_scratch.html#conclusion-set-the-learning-rate-to-0.1",
    "title": "Linear Regression with Gradient Descent from Scratch",
    "section": "Conclusion: Set the learning rate to 0.1",
    "text": "Conclusion: Set the learning rate to 0.1\n\nimport numpy as np\nx_train = np.array([3,5])\ny_train = np.array([30,50])\nw = 1\nb = 1\na = 0.1\nn = 10000\n\n\nw,b,cost,cost_arr = grad_desc(x_train, y_train, w, b, a, n, derivative_func, cost_func)\n\niteration: 1 w: 15.9 b: 4.5 cost: 412.21000000000004\niteration: 101 w: 9.60266836170258 b: 1.6831238324254831 cost: 0.08333518019207553\niteration: 201 w: 9.772777027894861 b: 0.962531955886906 cost: 0.02725376794504336\niteration: 301 w: 9.870057468399038 b: 0.5504453970300839 cost: 0.008913016873429894\niteration: 401 w: 9.925689461050384 b: 0.3147844944352242 cost: 0.002914894921914694\niteration: 501 w: 9.957503858583115 b: 0.18001654382337315 cost: 0.0009532813105214929\niteration: 601 w: 9.975697632383632 b: 0.10294648123711944 cost: 0.0003117591821775748\niteration: 701 w: 9.9861021482876 b: 0.05887224459493621 cost: 0.0001019570892655447\niteration: 801 w: 9.99205220309112 b: 0.03366740797738293 cost: 3.334383923801124e-05\niteration: 901 w: 9.995454874824398 b: 0.01925345921009804 cost: 1.0904701410566107e-05\niteration: 1001 w: 9.997400768653412 b: 0.011010520673404262 cost: 3.566251384688875e-06\niteration: 1101 w: 9.998513571500878 b: 0.006296612165977895 cost: 1.1662996041765428e-06\niteration: 1201 w: 9.999149952663545 b: 0.003600858301325476 cost: 3.814242519604661e-07\niteration: 1301 w: 9.999513881444926 b: 0.002059231244427411 cost: 1.247402120866741e-07\niteration: 1401 w: 9.999722002246868 b: 0.0011776173798518413 cost: 4.079478541682378e-08\niteration: 1501 w: 9.999841020775817 b: 0.0006734468006357401 cost: 1.334144370438836e-08\niteration: 1601 w: 9.999909084179867 b: 0.00038512559431133533 cost: 4.363158631288135e-09\niteration: 1701 w: 9.999948007757663 b: 0.00022024267285022806 cost: 1.4269185302952782e-09\niteration: 1801 w: 9.999970267074978 b: 0.0001259506915682373 cost: 4.66656535884604e-10\niteration: 1901 w: 9.999982996562743 b: 7.20277160697712e-05 cost: 1.5261440495607602e-10\niteration: 2001 w: 9.999990276204635 b: 4.119065816756014e-05 cost: 4.9910704804512534e-11\niteration: 2101 w: 9.999994439230438 b: 2.355579786254431e-05 cost: 1.6322695470293246e-11\niteration: 2201 w: 9.99999681994973 b: 1.3470909124573112e-05 cost: 5.338141149193196e-12\niteration: 2301 w: 9.999998181417226 b: 7.703640252503084e-06 cost: 1.7457748318741034e-12\niteration: 2401 w: 9.999998960002822 b: 4.405498737407422e-06 cost: 5.709346551062598e-13\niteration: 2501 w: 9.999999405254385 b: 2.519382849046461e-06 cost: 1.867173088141536e-13\niteration: 2601 w: 9.999999659881437 b: 1.4407653511503998e-06 cost: 6.106364823594468e-14\niteration: 2701 w: 9.999999805495602 b: 8.239338454550408e-07 cost: 1.9970131332789973e-14\niteration: 2801 w: 9.999999888768317 b: 4.711849721437697e-07 cost: 6.530991282823491e-15\niteration: 2901 w: 9.999999936389676 b: 2.694576496529729e-07 cost: 2.1358820037435938e-15\niteration: 3001 w: 9.999999963623017 b: 1.540953754307092e-07 cost: 6.985144677984442e-16\niteration: 3101 w: 9.999999979197007 b: 8.812288271748043e-08 cost: 2.284407747483156e-16\niteration: 3201 w: 9.999999988103344 b: 5.039503911269594e-08 cost: 7.470882133172757e-17\niteration: 3301 w: 9.99999999319663 b: 2.881952742434168e-08 cost: 2.4432604956797844e-17\niteration: 3401 w: 9.999999996109342 b: 1.6481090130251214e-08 cost: 7.990396382009646e-18\niteration: 3501 w: 9.999999997775038 b: 9.425080487015262e-09 cost: 2.6131649171968987e-18\niteration: 3601 w: 9.999999998727608 b: 5.389941863777911e-09 cost: 8.54600922971168e-19\niteration: 3701 w: 9.999999999272351 b: 3.082358228491976e-09 cost: 2.7948981881301694e-19\niteration: 3801 w: 9.99999999958388 b: 1.7627151669994074e-09 cost: 9.14036778361001e-20\niteration: 3901 w: 9.99999999976203 b: 1.008047016805869e-09 cost: 2.989179777548941e-20\niteration: 4001 w: 9.999999999863913 b: 5.764763265101116e-10 cost: 9.775802261103237e-21\niteration: 4101 w: 9.999999999922176 b: 3.2967285995751955e-10 cost: 3.1974078561318533e-21\niteration: 4201 w: 9.999999999955492 b: 1.885294074763455e-10 cost: 1.0455682344656884e-21\niteration: 4301 w: 9.999999999974548 b: 1.0781495013630904e-10 cost: 3.4190433540753846e-22\niteration: 4401 w: 9.999999999985445 b: 6.1654718943338e-11 cost: 1.1180010956842834e-22\niteration: 4501 w: 9.999999999991676 b: 3.525858921690209e-11 cost: 3.657881437649431e-23\niteration: 4601 w: 9.999999999995238 b: 2.0162220617259075e-11 cost: 1.1943622317753894e-23\niteration: 4701 w: 9.999999999997277 b: 1.1530192191877509e-11 cost: 3.9014567571770745e-24\niteration: 4801 w: 9.999999999998444 b: 6.594051806552098e-12 cost: 1.278964408416722e-24\niteration: 4901 w: 9.999999999999112 b: 3.771776060112977e-12 cost: 4.186895031678625e-25\niteration: 5001 w: 9.999999999999492 b: 2.1570676930981497e-12 cost: 1.3678217007808168e-25\niteration: 5101 w: 9.999999999999709 b: 1.2340726793457789e-12 cost: 4.4318205655316443e-26\niteration: 5201 w: 9.999999999999833 b: 7.047183412045043e-13 cost: 1.4808496912808834e-26\niteration: 5301 w: 9.999999999999904 b: 4.0184950008676143e-13 cost: 4.758408980293143e-27\niteration: 5401 w: 9.999999999999945 b: 2.3043106508463737e-13 cost: 1.7575820968324143e-27\niteration: 5501 w: 9.99999999999997 b: 1.3237616754974336e-13 cost: 4.322957760611145e-28\niteration: 5601 w: 9.99999999999998 b: 7.446693458529524e-14 cost: 2.808344822586802e-28\niteration: 5701 w: 9.999999999999991 b: 4.373596126367095e-14 cost: 5.048709793414476e-29\niteration: 5801 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 5901 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 6001 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 6101 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 6201 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 6301 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 6401 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 6501 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 6601 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 6701 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 6801 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 6901 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 7001 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 7101 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 7201 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 7301 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 7401 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 7501 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 7601 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 7701 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 7801 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 7901 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 8001 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 8101 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 8201 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 8301 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 8401 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 8501 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 8601 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 8701 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 8801 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 8901 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 9001 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 9101 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 9201 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 9301 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 9401 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 9501 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 9601 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 9701 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 9801 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 9901 w: 9.999999999999996 b: 3.005801360028904e-14 cost: 1.2937318845624594e-28\niteration: 10000 w: 9.99999999999999 b: 2.845929244482882e-14 cost: 2.0510383535746307e-28\n\n\n\nprint(\"After 10000 iterations\")\nprint(\"w:\", w,\"b:\", b, \"cost:\", cost)\n\nAfter 10000 iterations\nw: 9.99999999999999 b: 2.845929244482882e-14 cost: 2.0510383535746307e-28"
  },
  {
    "objectID": "posts/mlalgos/gradient_descent_from_scratch.html#comparing-costs",
    "href": "posts/mlalgos/gradient_descent_from_scratch.html#comparing-costs",
    "title": "Linear Regression with Gradient Descent from Scratch",
    "section": "Comparing costs",
    "text": "Comparing costs\n0.001 - cost: 3.7089806903241235e-06\n0.1 - cost: 2.0510383535746307e-28\nThe cost defers by several orders of magnitude!"
  },
  {
    "objectID": "posts/mlalgos/gradient_descent_from_scratch.html#final-model",
    "href": "posts/mlalgos/gradient_descent_from_scratch.html#final-model",
    "title": "Linear Regression with Gradient Descent from Scratch",
    "section": "Final model",
    "text": "Final model\n\nprint(\"w:\", w,\"b:\", b, \"cost:\", cost)\n\nw: 9.99999999999999 b: 2.845929244482882e-14 cost: 2.0510383535746307e-28\n\n\ny = 10x is the line that fits the data best."
  },
  {
    "objectID": "posts/mlalgos/Handwritten_digit_prediction_Neural_Network.html",
    "href": "posts/mlalgos/Handwritten_digit_prediction_Neural_Network.html",
    "title": "Handwritten Digits Prediction using Neural Networks",
    "section": "",
    "text": "#Neural Network for predicting handwritten digits"
  },
  {
    "objectID": "posts/mlalgos/Handwritten_digit_prediction_Neural_Network.html#generating-data",
    "href": "posts/mlalgos/Handwritten_digit_prediction_Neural_Network.html#generating-data",
    "title": "Handwritten Digits Prediction using Neural Networks",
    "section": "Generating data",
    "text": "Generating data\n\nfrom keras.datasets import mnist\n\n(X_train, y_train), (X_test, y_test)= digits = mnist.load_data()\n\n\nprint(X_train.shape)\nprint(X_test.shape)\n\n(60000, 28, 28)\n(10000, 28, 28)"
  },
  {
    "objectID": "posts/mlalgos/Handwritten_digit_prediction_Neural_Network.html#changing-3d-arrays-to-2d-arrays",
    "href": "posts/mlalgos/Handwritten_digit_prediction_Neural_Network.html#changing-3d-arrays-to-2d-arrays",
    "title": "Handwritten Digits Prediction using Neural Networks",
    "section": "Changing 3D arrays to 2D arrays",
    "text": "Changing 3D arrays to 2D arrays\n\nimport numpy as np\nX_train = np.array([X_train[i].ravel() for i in range(X_train.shape[0])])\n\n\nX_train.shape\n\n(60000, 784)\n\n\n\nX_test = np.array([X_test[i].ravel() for i in range(X_test.shape[0])])\nX_test.shape\n\n(10000, 784)"
  },
  {
    "objectID": "posts/mlalgos/Handwritten_digit_prediction_Neural_Network.html#defining-the-model",
    "href": "posts/mlalgos/Handwritten_digit_prediction_Neural_Network.html#defining-the-model",
    "title": "Handwritten Digits Prediction using Neural Networks",
    "section": "Defining the model",
    "text": "Defining the model\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\n\n\nmodel = Sequential([\n    Dense(units=25,activation='relu'),\n    Dense(units=15,activation='relu'),\n    Dense(units=10,activation='linear')\n])\n\n\nmodel.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              optimizer = tf.optimizers.Adam(0.00001),)"
  },
  {
    "objectID": "posts/mlalgos/Handwritten_digit_prediction_Neural_Network.html#fitting-data-into-our-model",
    "href": "posts/mlalgos/Handwritten_digit_prediction_Neural_Network.html#fitting-data-into-our-model",
    "title": "Handwritten Digits Prediction using Neural Networks",
    "section": "Fitting data into our model",
    "text": "Fitting data into our model\n\nmodel.fit(X_train,y_train,epochs=50)\n\nEpoch 1/50\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.0796\nEpoch 2/50\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0722\nEpoch 3/50\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0697\nEpoch 4/50\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.0682\nEpoch 5/50\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0671\nEpoch 6/50\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.0662\nEpoch 7/50\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.0654\nEpoch 8/50\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0648\nEpoch 9/50\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.0642\nEpoch 10/50\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.0637\nEpoch 11/50\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0632\nEpoch 12/50\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.0628\nEpoch 13/50\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0625\nEpoch 14/50\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0621\nEpoch 15/50\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.0618\nEpoch 16/50\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0615\nEpoch 17/50\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0613\nEpoch 18/50\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.0610\nEpoch 19/50\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0607\nEpoch 20/50\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0605\nEpoch 21/50\n1875/1875 [==============================] - 8s 4ms/step - loss: 0.0603\nEpoch 22/50\n1875/1875 [==============================] - 7s 3ms/step - loss: 0.0601\nEpoch 23/50\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.0598\nEpoch 24/50\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0597\nEpoch 25/50\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0595\nEpoch 26/50\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.0592\nEpoch 27/50\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0591\nEpoch 28/50\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.0589\nEpoch 29/50\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0588\nEpoch 30/50\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0586\nEpoch 31/50\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.0584\nEpoch 32/50\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0582\nEpoch 33/50\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0581\nEpoch 34/50\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.0580\nEpoch 35/50\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0578\nEpoch 36/50\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0577\nEpoch 37/50\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.0576\nEpoch 38/50\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0574\nEpoch 39/50\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0572\nEpoch 40/50\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.0571\nEpoch 41/50\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0570\nEpoch 42/50\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0569\nEpoch 43/50\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.0567\nEpoch 44/50\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0565\nEpoch 45/50\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0564\nEpoch 46/50\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.0563\nEpoch 47/50\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0562\nEpoch 48/50\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0561\nEpoch 49/50\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.0560\nEpoch 50/50\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0558\n\n\n&lt;keras.callbacks.History at 0x7da45a7ac430&gt;"
  },
  {
    "objectID": "posts/mlalgos/Handwritten_digit_prediction_Neural_Network.html#models-predictions",
    "href": "posts/mlalgos/Handwritten_digit_prediction_Neural_Network.html#models-predictions",
    "title": "Handwritten Digits Prediction using Neural Networks",
    "section": "Model’s predictions",
    "text": "Model’s predictions\n\nz = model.predict(X_test)\nz\n\n313/313 [==============================] - 1s 1ms/step\n\n\narray([[  72.16857  ,  141.73775  ,   96.48515  , ...,  154.52151  ,\n         121.25679  ,  132.14868  ],\n       [  -0.627826 ,   -3.3008962,    5.777237 , ...,   -2.4945285,\n           0.6228762,    0.7712919],\n       [-230.27592  ,  258.76544  ,   64.82611  , ..., -130.3381   ,\n         -27.08746  , -234.17287  ],\n       ...,\n       [  34.202644 ,   43.17494  ,   33.014576 , ...,   53.978813 ,\n          40.810585 ,   55.017437 ],\n       [ 138.34541  ,  144.58145  ,   81.53359  , ...,  156.82513  ,\n         158.51685  ,  143.86653  ],\n       [ 116.74089  ,  120.48703  ,   86.29796  , ...,   48.32769  ,\n         111.52056  ,   98.84621  ]], dtype=float32)"
  },
  {
    "objectID": "posts/mlalgos/Handwritten_digit_prediction_Neural_Network.html#converting-predicted-values-to-digits",
    "href": "posts/mlalgos/Handwritten_digit_prediction_Neural_Network.html#converting-predicted-values-to-digits",
    "title": "Handwritten Digits Prediction using Neural Networks",
    "section": "Converting predicted values to digits",
    "text": "Converting predicted values to digits\n\npred = np.array([np.argmax(z[i]) for i in range(len(z))])\npred\n\narray([7, 2, 1, ..., 4, 5, 6])\n\n\n\nprint(pred.shape)\nprint(y_test.shape)\n\n(10000,)\n(10000,)"
  },
  {
    "objectID": "posts/mlalgos/Handwritten_digit_prediction_Neural_Network.html#accuracy-of-our-neural-network",
    "href": "posts/mlalgos/Handwritten_digit_prediction_Neural_Network.html#accuracy-of-our-neural-network",
    "title": "Handwritten Digits Prediction using Neural Networks",
    "section": "Accuracy of our Neural Network",
    "text": "Accuracy of our Neural Network\n\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test,pred)*100\n\n95.38\n\n\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\ncf = confusion_matrix(y_test,pred)\nsns.heatmap(cf,annot=True)\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "posts/mlalgos/multi_class_neural_network.html",
    "href": "posts/mlalgos/multi_class_neural_network.html",
    "title": "Multiclass Segregator Neural Network",
    "section": "",
    "text": "import tensorflow as tf\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential"
  },
  {
    "objectID": "posts/mlalgos/multi_class_neural_network.html#generating-data",
    "href": "posts/mlalgos/multi_class_neural_network.html#generating-data",
    "title": "Multiclass Segregator Neural Network",
    "section": "Generating data",
    "text": "Generating data\n\nfrom sklearn.datasets import make_blobs\ncenters = [[-5, 2], [-2, -2], [1, 2], [5, -2]]\nX,y= make_blobs(n_samples=2000, centers=centers, cluster_std=1.0,random_state=30)\n\nfrom sklearn.model_selection import train_test_split\nXtrain, Xtest, ytrain, ytest = train_test_split(X,y)"
  },
  {
    "objectID": "posts/mlalgos/multi_class_neural_network.html#creating-the-neural-network",
    "href": "posts/mlalgos/multi_class_neural_network.html#creating-the-neural-network",
    "title": "Multiclass Segregator Neural Network",
    "section": "Creating the neural network",
    "text": "Creating the neural network\n\nmodel = Sequential([\n    Dense(units=25,activation='relu'),\n    Dense(units=15,activation='relu'),\n    Dense(units=4,activation='linear')\n])\n\n\nmodel.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              optimizer=tf.optimizers.Adam(0.001),)\n\n\nmodel.fit(Xtrain,ytrain,epochs=10)\n\nEpoch 1/10\n47/47 [==============================] - 1s 2ms/step - loss: 0.9830\nEpoch 2/10\n47/47 [==============================] - 0s 2ms/step - loss: 0.6116\nEpoch 3/10\n47/47 [==============================] - 0s 2ms/step - loss: 0.3040\nEpoch 4/10\n47/47 [==============================] - 0s 3ms/step - loss: 0.1548\nEpoch 5/10\n47/47 [==============================] - 0s 4ms/step - loss: 0.0988\nEpoch 6/10\n47/47 [==============================] - 0s 4ms/step - loss: 0.0727\nEpoch 7/10\n47/47 [==============================] - 0s 4ms/step - loss: 0.0612\nEpoch 8/10\n47/47 [==============================] - 0s 4ms/step - loss: 0.0549\nEpoch 9/10\n47/47 [==============================] - 0s 4ms/step - loss: 0.0507\nEpoch 10/10\n47/47 [==============================] - 0s 3ms/step - loss: 0.0474\n\n\n&lt;keras.callbacks.History at 0x7da779c31a50&gt;"
  },
  {
    "objectID": "posts/mlalgos/multi_class_neural_network.html#predictions-of-our-neural-network",
    "href": "posts/mlalgos/multi_class_neural_network.html#predictions-of-our-neural-network",
    "title": "Multiclass Segregator Neural Network",
    "section": "Predictions of our neural network",
    "text": "Predictions of our neural network\n\nz_pred = model.predict(Xtest)\n\n16/16 [==============================] - 0s 1ms/step\n\n\n\nz_pred\n\narray([[-3.993621  , -3.7787347 , -0.25490975, -0.07915557],\n       [ 3.1840923 , -1.6633781 , -3.830906  , -7.8026066 ],\n       [-2.4112265 , -6.316679  ,  4.8291907 , -4.6041875 ],\n       ...,\n       [-4.908304  ,  2.9423723 , -3.8089552 , -3.944756  ],\n       [-3.3867204 , -5.7413936 ,  3.1279325 , -2.993482  ],\n       [ 3.0492527 , -2.0576117 , -3.2582407 , -6.9038754 ]],\n      dtype=float32)"
  },
  {
    "objectID": "posts/mlalgos/multi_class_neural_network.html#array-of-arrays-containing-probability-of-category-index-i",
    "href": "posts/mlalgos/multi_class_neural_network.html#array-of-arrays-containing-probability-of-category-index-i",
    "title": "Multiclass Segregator Neural Network",
    "section": "Array of arrays containing probability of category = index i",
    "text": "Array of arrays containing probability of category = index i\n\nprob_pred = tf.nn.softmax(z_pred).numpy()\nprob_pred\n\narray([[1.05925733e-02, 1.31318374e-02, 4.45351750e-01, 5.30923843e-01],\n       [9.91312504e-01, 7.78002478e-03, 8.90503230e-04, 1.67782946e-05],\n       [7.16431008e-04, 1.44230735e-05, 9.99189198e-01, 7.99435875e-05],\n       ...,\n       [3.88486253e-04, 9.97427046e-01, 1.16631761e-03, 1.01821462e-03],\n       [1.47593522e-03, 1.40102551e-04, 9.96196985e-01, 2.18699919e-03],\n       [9.92137134e-01, 6.00742921e-03, 1.80826557e-03, 4.72044776e-05]],\n      dtype=float32)"
  },
  {
    "objectID": "posts/mlalgos/multi_class_neural_network.html#predicting-the-category",
    "href": "posts/mlalgos/multi_class_neural_network.html#predicting-the-category",
    "title": "Multiclass Segregator Neural Network",
    "section": "Predicting the category",
    "text": "Predicting the category\nWe dont need to use probabilities here, we can simply take the index of the maximum z value in the z_pred array as the values of z_pred and prob_pred increase or decrease together.\n\nimport numpy as np\npred = np.argmax(z_pred,axis=1) # Max of z_pred is the category\n\n\npred\n\narray([3, 0, 2, 0, 0, 3, 2, 3, 1, 3, 1, 2, 1, 2, 3, 2, 3, 2, 3, 3, 1, 0,\n       1, 0, 1, 3, 2, 3, 2, 1, 0, 1, 0, 2, 2, 2, 0, 2, 2, 1, 3, 1, 0, 1,\n       0, 1, 2, 3, 3, 0, 0, 1, 2, 3, 2, 2, 3, 0, 0, 3, 1, 1, 1, 3, 3, 2,\n       0, 3, 3, 1, 0, 1, 3, 2, 1, 3, 1, 2, 2, 2, 0, 3, 1, 1, 0, 0, 2, 2,\n       2, 2, 3, 0, 0, 1, 2, 0, 0, 2, 1, 3, 2, 3, 1, 2, 2, 1, 2, 1, 0, 0,\n       1, 3, 1, 0, 2, 3, 0, 1, 2, 1, 1, 0, 0, 0, 1, 0, 1, 3, 1, 2, 3, 2,\n       2, 0, 3, 0, 1, 3, 2, 0, 0, 0, 2, 3, 2, 1, 2, 2, 1, 2, 3, 2, 3, 0,\n       1, 0, 1, 2, 2, 0, 3, 1, 3, 2, 3, 1, 1, 0, 2, 3, 3, 0, 2, 0, 0, 1,\n       3, 2, 0, 1, 2, 0, 0, 1, 2, 0, 0, 2, 0, 1, 3, 0, 2, 1, 1, 3, 0, 2,\n       3, 1, 0, 2, 0, 0, 2, 3, 3, 1, 3, 0, 0, 1, 3, 3, 3, 2, 1, 0, 2, 0,\n       0, 2, 0, 1, 1, 2, 2, 1, 1, 3, 0, 2, 3, 1, 0, 0, 2, 2, 2, 1, 2, 0,\n       0, 1, 3, 3, 2, 1, 1, 3, 1, 0, 3, 0, 3, 1, 0, 3, 3, 1, 3, 0, 3, 2,\n       3, 3, 0, 2, 1, 3, 3, 0, 1, 3, 0, 3, 0, 3, 3, 2, 2, 3, 3, 1, 2, 0,\n       1, 1, 1, 3, 2, 1, 1, 2, 1, 3, 2, 1, 1, 1, 2, 0, 1, 2, 1, 1, 1, 3,\n       1, 0, 3, 2, 2, 1, 1, 0, 0, 2, 3, 0, 2, 1, 0, 3, 3, 1, 1, 3, 2, 3,\n       2, 1, 1, 1, 0, 1, 1, 3, 1, 2, 1, 3, 0, 3, 3, 2, 2, 0, 3, 2, 3, 3,\n       1, 0, 2, 1, 0, 2, 0, 0, 2, 1, 0, 1, 3, 3, 1, 1, 0, 2, 0, 0, 0, 0,\n       1, 2, 0, 2, 1, 2, 1, 1, 0, 1, 0, 3, 3, 2, 2, 2, 0, 1, 3, 0, 2, 2,\n       2, 1, 0, 3, 1, 0, 3, 0, 2, 3, 1, 2, 1, 0, 1, 0, 1, 0, 3, 0, 2, 1,\n       1, 1, 0, 1, 1, 0, 3, 1, 0, 2, 0, 0, 0, 0, 3, 1, 2, 2, 2, 1, 0, 3,\n       3, 1, 2, 3, 0, 0, 3, 3, 1, 1, 1, 3, 3, 3, 2, 2, 3, 2, 0, 2, 3, 1,\n       3, 3, 2, 2, 0, 2, 2, 1, 2, 1, 1, 1, 2, 0, 1, 2, 0, 3, 0, 3, 2, 0,\n       1, 0, 2, 1, 1, 2, 1, 3, 0, 0, 2, 0, 0, 1, 2, 0])"
  },
  {
    "objectID": "posts/mlalgos/multi_class_neural_network.html#evaluating-our-models-accuracy",
    "href": "posts/mlalgos/multi_class_neural_network.html#evaluating-our-models-accuracy",
    "title": "Multiclass Segregator Neural Network",
    "section": "Evaluating our model’s accuracy",
    "text": "Evaluating our model’s accuracy\n\nfrom sklearn.metrics import accuracy_score\naccuracy_score(pred,ytest) * 100\n\n98.8"
  }
]