{"title":"Optimizing bivariate functions","markdown":{"yaml":{"title":"Optimizing bivariate functions","authors":"Kishan Ved and Vannsh Jani","description":"Understanding optimization of bivariate functions using Newton's method and L-BFGS.","date":"08/20/2023","draft":false,"categories":["ML","Math","Coding"]},"headingText":"Hessian Matrix","containsRefs":false,"markdown":"\n\n\nHessian matrix is a square matrix of second order partial derivatives of a scalar valued function. It can be used to describe the local curvature of the function at a point and it is denoted by **H**.\n\n$$\nH(f) = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \\cdots& \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\frac{\\partial^2 f}{\\partial x_2\\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} \\cdots& \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\ \\vdots &\\quad \\vdots \\quad \\ddots & \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2}  \\cdots& \\frac{\\partial^2 f}{\\partial x_n^2} \\end{bmatrix}\n$$\n\nFor example, let's take a bivariate function(n=2),\n\n$$\nf(x,y) = xy^2\n$$\n\n$$\nH(f)= \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\ \\frac{\\partial^2 f}{\\partial y \\partial x} & \\frac{\\partial^2 f}{\\partial y^2} \\end{bmatrix}\n$$\n\nHere, $\\frac{\\partial f}{\\partial x}=y^2$, $\\frac{\\partial f}{\\partial y}=2xy$, $\\frac{\\partial^2 f}{\\partial x^2}=0,\\frac{\\partial^2 f}{\\partial y^2}=2x,\\frac{\\partial^2 f}{\\partial x \\partial y}=\\frac{\\partial^2 f}{\\partial y \\partial x}=2y$\n\n$$\nH(f)=\\begin{bmatrix} 0 & 2y \\\\ 2y & 2x \\end{bmatrix}\n$$\n\nUsing this matrix, we can find out the nature of the curvature at any point $(x_1,y_1)$, by substituting this point in the Hessian.\n\nIf the Hessian matrix is positive definite(all eigenvalues are positive) at a point, it indicates that the function is locally convex(has a local minimum) around that point. If it is negative definite, the function is locally concave. If the eigenvalues have both positive and negative values, then this point has a mixture of concave and convex behaviour in different directions and such a point is called a saddle point.\n\n```{python}\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define a sample function\ndef f(x):\n    return x[0]*x[1]**2 \n\n# point where you want to compute Hessian matrix\n# requires_grad=True tells pytorch to keep track of x0 which form a computation graph to compute gradients easily.\nx0 = torch.tensor([2.0, 1.0], requires_grad=True)\n# create_graph=True is used to compute higher order derivatives in the computation graph\ngrads = torch.autograd.grad(f(x0), x0, create_graph=True)[0]\nHessian = torch.zeros((len(x0), len(x0)))\nfor i in range(len(x0)):\n    Hessian[i] = torch.autograd.grad(grads[i], x0, retain_graph=True)[0]\n\nHessian = Hessian.detach().numpy()\nplt.imshow(Hessian, cmap='coolwarm')\nplt.xticks(np.arange(len(x0)))\nplt.yticks(np.arange(len(x0)))\nplt.xlabel('Hessian Row Index')\nplt.ylabel('Hessian Column Index')\nplt.colorbar()\nplt.title('Visualization of the Hessian Matrix')\nplt.show()\n```\n\n## Newton's method for optimizing bivariate functions using Hessian.\n\nFollowing are the steps to find minimum or maximum of a function:\n\n1.  Make an intial guess.\n2.  At the initial guess, we find out how steep the slope of the curve is and how quickly the slope is changing. Hence, we calculate the first derivative and the second derivative at this point.\n3.  We can approximate a quadratic function(parabolic bowl) at that point using the taylor series.\n4.  Newton's method then moves to the minimum of the parabolic bowl which is the new guess for the optimal point of the original function.\n5.  This process repeats and with each iteration you edge closer to the optimal value of the original function and finally newton's method converges.\n\nAt any iteration, the value of $x$ can be updated as,\n\n$$\nx_{i+1} = x_i-H^{-1}\\nabla f(x_i) \\quad -(*)\n$$\n\nwhere $H^{-1}$ is the inverse of the hessian(which is initially assumed to be the identity matrix) and $\\nabla f(x_i)$ is an array/vector containing the partial derivatives of $f$ with respect to all the variables.\n\nFollowing is the code for optimizing $f(x,y)=-sin(x)-cos(y)$.\n\nLet's first plot $f(x,y)$.\n\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-5, 5, 100)\ny = np.linspace(-5, 5, 100)\nX, Y = np.meshgrid(x, y)\n\ndef f(X,Y):\n    return -np.sin(X) - np.cos(Y)\nZ = f(X,Y)\n\nfig = plt.figure(figsize=(8,6))\nax1= fig.add_subplot(111, projection='3d')\nax1.plot_surface(X, Y, Z, cmap='viridis')\nax1.set_xlabel('X')\nax1.set_ylabel('Y')\nax1.set_zlabel('Z')\nax1.set_title('f(x) = -sin(x) - cos(y)')\nplt.show()\n```\n\n```{python}\nimport torch\n\ndef f(x):\n    return -torch.sin(x[0])-torch.cos(x[1])\n\niterations = 10\n\n\ndef newton(guess,f,iterations):\n  guesses=[]\n  guesses.append(guess)\n  for i in range(iterations):\n      f_value = f(guess)\n      gradient = torch.autograd.grad(f_value, guess, create_graph=True)[0]\n      hessian = torch.zeros((len(guess), len(guess)))\n      for j in range(len(guess)):\n          hessian_row = torch.autograd.grad(gradient[j], guess, retain_graph=True)[0]\n          hessian[j] = hessian_row\n      step = -torch.linalg.solve(hessian, gradient)\n      guess = guess + step\n      guesses.append(guess)\n  return guesses\n      \n\nguess = torch.tensor([2.0, 1.0], requires_grad=True)\nguesses=newton(guess,f,iterations)\nfor i in range(len(guesses)):\n  print(f\"Iteration {i}: guess = {guesses[i]}\")\n```\n\nSo after updating our guess using $(*)$ for a sufficient number of iterations, we get our final guess as $x=1.5708 \\quad and \\quad y=0.0$.\n\nLet's plot the contour plot of the above function to verify our results.\n\n```{python}\nimport time \n\nx = np.linspace(-6, 6, 100)\ny = np.linspace(-6, 6, 100)\nX, Y = np.meshgrid(x, y)\n\ndef f1(X,Y):\n    return -np.sin(X) - np.cos(Y)\nZ = f1(X,Y)\n\ndef plot_contour(guesses):\n  fig=plt.figure(figsize=(10,6))\n  ax = fig.add_subplot(111)\n  contour = ax.contourf(X,Y,Z)\n  plt.colorbar(contour)\n  ax.set_xlabel(\"X\")\n  ax.set_ylabel(\"Y\")\n  ax.set_title(\"Contour plot of f(x,y)\")\n  marker=\"\"\n  for i in range(2):\n    if i==0:\n      marker=\"o\"\n      color=\"cyan\"\n      ax.scatter(guesses[0][0].detach().numpy(), guesses[0][1].detach().numpy(), color=color, alpha=1,marker=marker)\n    else:\n      marker=\"x\"\n      color=\"red\"\n      ax.scatter(guesses[-1][0].detach().numpy(), guesses[-1][1].detach().numpy(), color=color, alpha=1,marker=marker)\n  plt.show()\n  \nplot_contour(guesses)\n```\n\n<https://www.flexclip.com/share/367853402cf50de52d9ad00687ce49806e7a8e5.html>\n\nThrough the contour plot we can understand that even though our initial guess was the point $(2,1)$ we finally reached the minima of the function. In the above contour plot, the bluish circle is the initial guess and the red cross is the final guess.\n\nDepending upon different initial guesses, the final guess could land onto different minimas or possibly even a saddle point.\n\nLet's say we take another point $(1,-2)$.\n\n```{python}\nguess = torch.tensor([1.0, -2.0], requires_grad=True)\niterations=10\nguesses = newton(guess,f,iterations)\nplot_contour(guesses)\n```\n\nIn the case above we got a saddle point, this is one of the drawbacks of the newton method.\n\nAlthough the newton's method for optimization converges faster than the gradient descent algorithm and one doesn't have to also face the difficulty in deciding the learning rate as is faced in gradient descent, the computation of the Hessian and it's inverse is computationally very expensive(having computational complexity of $O(n^3)$ for functions with n variables.\n\nIn order to use this method for optimization, the hessian needs to be positive definite which may not always be possible.\n\nHence to overcome these scenarios, Quasi-newton optimization algorithms can be used like the BFGS, and the LBFGS, where we try to approximate the hessian instead of calculating it.\n\n## L-BFGS for optimizing functions:\n\nThe BFGS algorithm constructs an approximation of the inverse Hessian matrix using a sequence of rank-two updates. This approximation captures information about the curvature of the objective function's landscape and guides the optimization process. BFGS has good convergence properties and doesn't require the explicit computation of the Hessian matrix, making it suitable for problems with a large number of variables.\n\nL-BFGS is a variant of the BFGS algorithm that addresses the memory and computational requirements associated with the Hessian matrix. In high-dimensional optimization problems, storing and manipulating the full Hessian matrix can be expensive. L-BFGS overcomes this limitation by maintaining a limited-memory approximation of the Hessian, using only a small number of vectors.\n\nL-BFGS uses a recursive formula to update and approximate the inverse Hessian matrix. Instead of storing the full Hessian matrix explicitly, L-BFGS maintains a limited number of vector pairs to approximate the Hessian. This makes L-BFGS well-suited for large-scale optimization problems and enables it to operate efficiently in high-dimensional spaces.\n\nThe following code implements LBFGS of the function -sin(x)-cos(y)\n\n```{python}\nimport torch\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\n\ndef f(x):\n    return -torch.sin(x[0])-torch.cos(x[1])\n\n# L-BFGS\ndef closure():\n    lbfgs.zero_grad()\n    objective = f(x_lbfgs)\n    objective.backward()\n    return objective\n\nx_lbfgs = torch.ones(2, 1)\nx_lbfgs.requires_grad = True\n\nlbfgs = optim.LBFGS([x_lbfgs],\n                    history_size=10, \n                    max_iter=4, \n                    line_search_fn=\"strong_wolfe\")\n                    \nhistory_lbfgs = []\nfor i in range(100):\n    history_lbfgs.append(f(x_lbfgs).item())\n    lbfgs.step(closure)\n```\n\nLet us also perform gradient descent on this, with learning rate of 10^-5.\n\n```{python}\nx_gd = torch.ones(2, 1)\nx_gd.requires_grad = True\ngd = optim.SGD([x_gd], lr=1e-5)\n\nhistory_gd = []\nfor i in range(100):\n    gd.zero_grad()\n    objective = f(x_gd)\n    objective.backward()\n    gd.step()\n    history_gd.append(objective.item())\n```\n\nNow, to visualize the results, we use a contour plot:\n\n```{python}\nx_range = np.linspace(-5, 5, 400)\ny_range = np.linspace(-5, 5, 400)\nX, Y = np.meshgrid(x_range, y_range)\n\nZ = f(torch.tensor([X, Y])).detach().numpy()\n\nfig=plt.figure(figsize=(10,6))\nplt.contourf(X, Y, Z, levels=20, cmap=\"viridis\")\n\ncoordinates = np.array([2.0, 1.0])\nplt.plot(coordinates[0], coordinates[1], marker=\"o\", color=\"cyan\", label=\"Initial Coordinates\")\n\ncoordinates = x_lbfgs.detach().numpy()\nplt.plot(coordinates[0], coordinates[1], marker=\"o\", color=\"red\", label=\"LBFGS\")\n\ncoordinates = x_gd.detach().numpy()\nplt.plot(coordinates[0], coordinates[1], marker=\"o\", color=\"orange\", label=\"Grad Desc (lr=1e-5)\")\n\nplt.colorbar(label=\"Objective Value\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.title(\"Contour Plot of -sin(X)-cos(Y)\")\nplt.legend()\nplt.show()\n```\n\nWe observe that in a 100 iterations, the gradient descent algorithm does not converge to the minima, but remains somewhere in between. Changing the learning rate might lead to the optimal value. For L-BFGS, the convergence is at the minima. \n\n## Remarks[^1]\n\n[^1]: Liu, D.C. and Nocedal, J. (no date) On the limited memory BFGS method for large scale optimization - mathematical programming, SpringerLink. Available at: https://link.springer.com/article/10.1007/BF01589116 (Accessed: 20 August 2023). \n\nThe LBFGS method is appealing for several reasons it is very simple to implement it requires only function and gradient values and no other information on the problem # and it can be faster than the partitioned quasi Newton method on problems where the element functions depend on more than or variables\t \nIn addition the LBFGS method appears to be preferable to PQN for large problems in which the Hessian matrix is not very sparse or for problems in which\nthe information on the separablity of the ob jective function is difficult to obtain.\n","srcMarkdownNoYaml":"\n\n# Hessian Matrix\n\nHessian matrix is a square matrix of second order partial derivatives of a scalar valued function. It can be used to describe the local curvature of the function at a point and it is denoted by **H**.\n\n$$\nH(f) = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \\cdots& \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\frac{\\partial^2 f}{\\partial x_2\\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} \\cdots& \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\ \\vdots &\\quad \\vdots \\quad \\ddots & \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2}  \\cdots& \\frac{\\partial^2 f}{\\partial x_n^2} \\end{bmatrix}\n$$\n\nFor example, let's take a bivariate function(n=2),\n\n$$\nf(x,y) = xy^2\n$$\n\n$$\nH(f)= \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\ \\frac{\\partial^2 f}{\\partial y \\partial x} & \\frac{\\partial^2 f}{\\partial y^2} \\end{bmatrix}\n$$\n\nHere, $\\frac{\\partial f}{\\partial x}=y^2$, $\\frac{\\partial f}{\\partial y}=2xy$, $\\frac{\\partial^2 f}{\\partial x^2}=0,\\frac{\\partial^2 f}{\\partial y^2}=2x,\\frac{\\partial^2 f}{\\partial x \\partial y}=\\frac{\\partial^2 f}{\\partial y \\partial x}=2y$\n\n$$\nH(f)=\\begin{bmatrix} 0 & 2y \\\\ 2y & 2x \\end{bmatrix}\n$$\n\nUsing this matrix, we can find out the nature of the curvature at any point $(x_1,y_1)$, by substituting this point in the Hessian.\n\nIf the Hessian matrix is positive definite(all eigenvalues are positive) at a point, it indicates that the function is locally convex(has a local minimum) around that point. If it is negative definite, the function is locally concave. If the eigenvalues have both positive and negative values, then this point has a mixture of concave and convex behaviour in different directions and such a point is called a saddle point.\n\n```{python}\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define a sample function\ndef f(x):\n    return x[0]*x[1]**2 \n\n# point where you want to compute Hessian matrix\n# requires_grad=True tells pytorch to keep track of x0 which form a computation graph to compute gradients easily.\nx0 = torch.tensor([2.0, 1.0], requires_grad=True)\n# create_graph=True is used to compute higher order derivatives in the computation graph\ngrads = torch.autograd.grad(f(x0), x0, create_graph=True)[0]\nHessian = torch.zeros((len(x0), len(x0)))\nfor i in range(len(x0)):\n    Hessian[i] = torch.autograd.grad(grads[i], x0, retain_graph=True)[0]\n\nHessian = Hessian.detach().numpy()\nplt.imshow(Hessian, cmap='coolwarm')\nplt.xticks(np.arange(len(x0)))\nplt.yticks(np.arange(len(x0)))\nplt.xlabel('Hessian Row Index')\nplt.ylabel('Hessian Column Index')\nplt.colorbar()\nplt.title('Visualization of the Hessian Matrix')\nplt.show()\n```\n\n## Newton's method for optimizing bivariate functions using Hessian.\n\nFollowing are the steps to find minimum or maximum of a function:\n\n1.  Make an intial guess.\n2.  At the initial guess, we find out how steep the slope of the curve is and how quickly the slope is changing. Hence, we calculate the first derivative and the second derivative at this point.\n3.  We can approximate a quadratic function(parabolic bowl) at that point using the taylor series.\n4.  Newton's method then moves to the minimum of the parabolic bowl which is the new guess for the optimal point of the original function.\n5.  This process repeats and with each iteration you edge closer to the optimal value of the original function and finally newton's method converges.\n\nAt any iteration, the value of $x$ can be updated as,\n\n$$\nx_{i+1} = x_i-H^{-1}\\nabla f(x_i) \\quad -(*)\n$$\n\nwhere $H^{-1}$ is the inverse of the hessian(which is initially assumed to be the identity matrix) and $\\nabla f(x_i)$ is an array/vector containing the partial derivatives of $f$ with respect to all the variables.\n\nFollowing is the code for optimizing $f(x,y)=-sin(x)-cos(y)$.\n\nLet's first plot $f(x,y)$.\n\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-5, 5, 100)\ny = np.linspace(-5, 5, 100)\nX, Y = np.meshgrid(x, y)\n\ndef f(X,Y):\n    return -np.sin(X) - np.cos(Y)\nZ = f(X,Y)\n\nfig = plt.figure(figsize=(8,6))\nax1= fig.add_subplot(111, projection='3d')\nax1.plot_surface(X, Y, Z, cmap='viridis')\nax1.set_xlabel('X')\nax1.set_ylabel('Y')\nax1.set_zlabel('Z')\nax1.set_title('f(x) = -sin(x) - cos(y)')\nplt.show()\n```\n\n```{python}\nimport torch\n\ndef f(x):\n    return -torch.sin(x[0])-torch.cos(x[1])\n\niterations = 10\n\n\ndef newton(guess,f,iterations):\n  guesses=[]\n  guesses.append(guess)\n  for i in range(iterations):\n      f_value = f(guess)\n      gradient = torch.autograd.grad(f_value, guess, create_graph=True)[0]\n      hessian = torch.zeros((len(guess), len(guess)))\n      for j in range(len(guess)):\n          hessian_row = torch.autograd.grad(gradient[j], guess, retain_graph=True)[0]\n          hessian[j] = hessian_row\n      step = -torch.linalg.solve(hessian, gradient)\n      guess = guess + step\n      guesses.append(guess)\n  return guesses\n      \n\nguess = torch.tensor([2.0, 1.0], requires_grad=True)\nguesses=newton(guess,f,iterations)\nfor i in range(len(guesses)):\n  print(f\"Iteration {i}: guess = {guesses[i]}\")\n```\n\nSo after updating our guess using $(*)$ for a sufficient number of iterations, we get our final guess as $x=1.5708 \\quad and \\quad y=0.0$.\n\nLet's plot the contour plot of the above function to verify our results.\n\n```{python}\nimport time \n\nx = np.linspace(-6, 6, 100)\ny = np.linspace(-6, 6, 100)\nX, Y = np.meshgrid(x, y)\n\ndef f1(X,Y):\n    return -np.sin(X) - np.cos(Y)\nZ = f1(X,Y)\n\ndef plot_contour(guesses):\n  fig=plt.figure(figsize=(10,6))\n  ax = fig.add_subplot(111)\n  contour = ax.contourf(X,Y,Z)\n  plt.colorbar(contour)\n  ax.set_xlabel(\"X\")\n  ax.set_ylabel(\"Y\")\n  ax.set_title(\"Contour plot of f(x,y)\")\n  marker=\"\"\n  for i in range(2):\n    if i==0:\n      marker=\"o\"\n      color=\"cyan\"\n      ax.scatter(guesses[0][0].detach().numpy(), guesses[0][1].detach().numpy(), color=color, alpha=1,marker=marker)\n    else:\n      marker=\"x\"\n      color=\"red\"\n      ax.scatter(guesses[-1][0].detach().numpy(), guesses[-1][1].detach().numpy(), color=color, alpha=1,marker=marker)\n  plt.show()\n  \nplot_contour(guesses)\n```\n\n<https://www.flexclip.com/share/367853402cf50de52d9ad00687ce49806e7a8e5.html>\n\nThrough the contour plot we can understand that even though our initial guess was the point $(2,1)$ we finally reached the minima of the function. In the above contour plot, the bluish circle is the initial guess and the red cross is the final guess.\n\nDepending upon different initial guesses, the final guess could land onto different minimas or possibly even a saddle point.\n\nLet's say we take another point $(1,-2)$.\n\n```{python}\nguess = torch.tensor([1.0, -2.0], requires_grad=True)\niterations=10\nguesses = newton(guess,f,iterations)\nplot_contour(guesses)\n```\n\nIn the case above we got a saddle point, this is one of the drawbacks of the newton method.\n\nAlthough the newton's method for optimization converges faster than the gradient descent algorithm and one doesn't have to also face the difficulty in deciding the learning rate as is faced in gradient descent, the computation of the Hessian and it's inverse is computationally very expensive(having computational complexity of $O(n^3)$ for functions with n variables.\n\nIn order to use this method for optimization, the hessian needs to be positive definite which may not always be possible.\n\nHence to overcome these scenarios, Quasi-newton optimization algorithms can be used like the BFGS, and the LBFGS, where we try to approximate the hessian instead of calculating it.\n\n## L-BFGS for optimizing functions:\n\nThe BFGS algorithm constructs an approximation of the inverse Hessian matrix using a sequence of rank-two updates. This approximation captures information about the curvature of the objective function's landscape and guides the optimization process. BFGS has good convergence properties and doesn't require the explicit computation of the Hessian matrix, making it suitable for problems with a large number of variables.\n\nL-BFGS is a variant of the BFGS algorithm that addresses the memory and computational requirements associated with the Hessian matrix. In high-dimensional optimization problems, storing and manipulating the full Hessian matrix can be expensive. L-BFGS overcomes this limitation by maintaining a limited-memory approximation of the Hessian, using only a small number of vectors.\n\nL-BFGS uses a recursive formula to update and approximate the inverse Hessian matrix. Instead of storing the full Hessian matrix explicitly, L-BFGS maintains a limited number of vector pairs to approximate the Hessian. This makes L-BFGS well-suited for large-scale optimization problems and enables it to operate efficiently in high-dimensional spaces.\n\nThe following code implements LBFGS of the function -sin(x)-cos(y)\n\n```{python}\nimport torch\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\n\ndef f(x):\n    return -torch.sin(x[0])-torch.cos(x[1])\n\n# L-BFGS\ndef closure():\n    lbfgs.zero_grad()\n    objective = f(x_lbfgs)\n    objective.backward()\n    return objective\n\nx_lbfgs = torch.ones(2, 1)\nx_lbfgs.requires_grad = True\n\nlbfgs = optim.LBFGS([x_lbfgs],\n                    history_size=10, \n                    max_iter=4, \n                    line_search_fn=\"strong_wolfe\")\n                    \nhistory_lbfgs = []\nfor i in range(100):\n    history_lbfgs.append(f(x_lbfgs).item())\n    lbfgs.step(closure)\n```\n\nLet us also perform gradient descent on this, with learning rate of 10^-5.\n\n```{python}\nx_gd = torch.ones(2, 1)\nx_gd.requires_grad = True\ngd = optim.SGD([x_gd], lr=1e-5)\n\nhistory_gd = []\nfor i in range(100):\n    gd.zero_grad()\n    objective = f(x_gd)\n    objective.backward()\n    gd.step()\n    history_gd.append(objective.item())\n```\n\nNow, to visualize the results, we use a contour plot:\n\n```{python}\nx_range = np.linspace(-5, 5, 400)\ny_range = np.linspace(-5, 5, 400)\nX, Y = np.meshgrid(x_range, y_range)\n\nZ = f(torch.tensor([X, Y])).detach().numpy()\n\nfig=plt.figure(figsize=(10,6))\nplt.contourf(X, Y, Z, levels=20, cmap=\"viridis\")\n\ncoordinates = np.array([2.0, 1.0])\nplt.plot(coordinates[0], coordinates[1], marker=\"o\", color=\"cyan\", label=\"Initial Coordinates\")\n\ncoordinates = x_lbfgs.detach().numpy()\nplt.plot(coordinates[0], coordinates[1], marker=\"o\", color=\"red\", label=\"LBFGS\")\n\ncoordinates = x_gd.detach().numpy()\nplt.plot(coordinates[0], coordinates[1], marker=\"o\", color=\"orange\", label=\"Grad Desc (lr=1e-5)\")\n\nplt.colorbar(label=\"Objective Value\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.title(\"Contour Plot of -sin(X)-cos(Y)\")\nplt.legend()\nplt.show()\n```\n\nWe observe that in a 100 iterations, the gradient descent algorithm does not converge to the minima, but remains somewhere in between. Changing the learning rate might lead to the optimal value. For L-BFGS, the convergence is at the minima. \n\n## Remarks[^1]\n\n[^1]: Liu, D.C. and Nocedal, J. (no date) On the limited memory BFGS method for large scale optimization - mathematical programming, SpringerLink. Available at: https://link.springer.com/article/10.1007/BF01589116 (Accessed: 20 August 2023). \n\nThe LBFGS method is appealing for several reasons it is very simple to implement it requires only function and gradient values and no other information on the problem # and it can be faster than the partitioned quasi Newton method on problems where the element functions depend on more than or variables\t \nIn addition the LBFGS method appears to be preferable to PQN for large problems in which the Hessian matrix is not very sparse or for problems in which\nthe information on the separablity of the ob jective function is difficult to obtain.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"cosmo","title-block-banner":true,"title":"Optimizing bivariate functions","authors":"Kishan Ved and Vannsh Jani","description":"Understanding optimization of bivariate functions using Newton's method and L-BFGS.","date":"08/20/2023","draft":false,"categories":["ML","Math","Coding"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}